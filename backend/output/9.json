{
    "paper_id": "9",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-01-13T17:58:42.071607Z"
    },
    "title": "Heidelberg colorectal data set for surgical data science in the sensor operating room",
    "authors": [
        {
            "first": "Lena",
            "middle": [],
            "last": "Maier-Hein",
            "suffix": "",
            "affiliation": {},
            "email": "l.maier-hein@dkfz.de"
        },
        {
            "first": "Martin",
            "middle": [],
            "last": "Wagner",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Tobias",
            "middle": [],
            "last": "Ross",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Annika",
            "middle": [],
            "last": "Reinke",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Sebastian",
            "middle": [],
            "last": "Bodenstedt",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Peter",
            "middle": [
                "M"
            ],
            "last": "Full",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Hellena",
            "middle": [],
            "last": "Hempe",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Diana",
            "middle": [],
            "last": "Mindroc-Filimon",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Patrick",
            "middle": [],
            "last": "Scholz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Calabria",
                "location": {
                    "addrLine": "Im Neuenheimer Feld 223, Via Pietro Bucci",
                    "postCode": "69120, 87036",
                    "settlement": "Heidelberg, Arcavacata, Rende",
                    "region": "CS",
                    "country": "Germany"
                }
            },
            "email": ""
        },
        {
            "first": "Thuy",
            "middle": [],
            "last": "Nuong Tran",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Pierangela",
            "middle": [],
            "last": "Bruno",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Anna",
            "middle": [],
            "last": "Kisilenko",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Benjamin",
            "middle": [],
            "last": "M\u00fcller",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Tornike",
            "middle": [],
            "last": "Davitashvili",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Manuela",
            "middle": [],
            "last": "Capek",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Minu",
            "middle": [
                "D"
            ],
            "last": "Tizabi",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Matthias",
            "middle": [],
            "last": "Eisenmann",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Tim",
            "middle": [
                "J"
            ],
            "last": "Adler",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Janek",
            "middle": [],
            "last": "Gr\u00f6hl",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Melanie",
            "middle": [],
            "last": "Schellenberg",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Silvia",
            "middle": [],
            "last": "Seidlitz",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Calabria",
                "location": {
                    "addrLine": "Im Neuenheimer Feld 223, Via Pietro Bucci",
                    "postCode": "69120, 87036",
                    "settlement": "Heidelberg, Arcavacata, Rende",
                    "region": "CS",
                    "country": "Germany"
                }
            },
            "email": ""
        },
        {
            "first": "T",
            "middle": [
                "Y Emmy"
            ],
            "last": "Lai",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "B\u00fcnyamin",
            "middle": [],
            "last": "Pekdemir",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Veith",
            "middle": [],
            "last": "Roethlingshoefer",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Fabian",
            "middle": [],
            "last": "Both",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Sebastian",
            "middle": [],
            "last": "Bittel",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "BMW Group",
                "location": {
                    "addrLine": "Heidemannstra\u00dfe 164",
                    "postCode": "80939",
                    "settlement": "Munich",
                    "country": "Germany"
                }
            },
            "email": ""
        },
        {
            "first": "Marc",
            "middle": [],
            "last": "Mengler",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Lars",
            "middle": [],
            "last": "M\u00fcndermann",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Corporate Research & Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Martin",
            "middle": [],
            "last": "Apitz",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Annette",
            "middle": [],
            "last": "Kopp- Schneider",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Stefanie",
            "middle": [],
            "last": "Speidel",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Felix",
            "middle": [],
            "last": "Nickel",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Pascal",
            "middle": [],
            "last": "Probst",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Hannes",
            "middle": [
                "G"
            ],
            "last": "Kenngott",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "P",
            "middle": [],
            "last": "M\u00fcller-Stich",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set-the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019. Background & Summary Surgical data science was recently defined as an interdisciplinary research field which aims \"to improve the quality of interventional healthcare and its value through the capture, organization, analysis and modelling of data\" 1. The vision is to derive data science-based methodology to provide physicians with the right assistance at the right time. One active field of research consists in analyzing laparoscopic video data to provide context-aware Division of computer Assisted Medical interventions (cAMi), German cancer Research center (DKfZ), im neuenheimer Feld 223, 69120, Heidelberg, Germany.",
    "pdf_parse": {
        "paper_id": "9",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set-the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019. Background & Summary Surgical data science was recently defined as an interdisciplinary research field which aims \"to improve the quality of interventional healthcare and its value through the capture, organization, analysis and modelling of data\" 1. The vision is to derive data science-based methodology to provide physicians with the right assistance at the right time. One active field of research consists in analyzing laparoscopic video data to provide context-aware Division of computer Assisted Medical interventions (cAMi), German cancer Research center (DKfZ), im neuenheimer Feld 223, 69120, Heidelberg, Germany.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "intraoperative assistance to the surgical team during minimally-invasive surgery. Accurate tracking of surgical instruments is a fundamental prerequisite for many assistance tasks ranging from surgical navigation 2 to skill analysis 3 and complication prediction. While encouraging results for detecting, segmenting and tracking medical devices in relatively controlled settings have been achieved , the proposed methods still tend to fail when applied to challenging images (e.g. in the presence of blood, smoke or motion artifacts) and do not generalize well (e.g. to other interventions or hospitals) . As of now, no large (with respect to the number of images), diverse (with respect to different procedures and levels of image quality), and extensively annotated data set (sensor data, surgical phase data, segmentations) has been made publicly available, which impedes the development of robust methodology.",
                "cite_spans": [
                    {
                        "start": 233,
                        "end": 234,
                        "text": "3",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This paper introduces a new annotated laparoscopic data set to address this bottleneck. This data set comprises 30 surgical procedures from three different types of surgery, namely from proctocolectomy (surgery to remove the entire colon and rectum), rectal resection (surgery to remove all or a part of the rectum), and sigmoid resection (surgery to remove the sigmoid colon). Annotations include surgical phase information and information on the status of medical devices for all frames as well as detailed segmentation maps for the surgical instruments in more than 10,000 frames ( Fig. 1 ). As illustrated in Figs. 1 and 2 , the data set is well-suited to both developing methods for instrument detection and binary or multi-instance segmentation. It features various levels of difficulty including motion artifacts, occlusion, inhomogeneous lighting, small or crossing instruments and smoke or blood in the field of view (see Fig. 3 for some challenging examples).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 585,
                        "end": 591,
                        "text": "Fig. 1",
                        "ref_id": null
                    },
                    {
                        "start": 613,
                        "end": 626,
                        "text": "Figs. 1 and 2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 931,
                        "end": 937,
                        "text": "Fig. 3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In this paper, we shall use the terminology for biomedical image analysis challenges that was introduced in a recent international guideline paper 6 . We define a biomedical image analysis challenge as an open competition on a specific scientific problem in the field of biomedical image analysis. A challenge may encompass multiple competitions related to multiple tasks, for which separate results and rankings (if any) are generated. The data set presented in this paper served as a basis for the Robust Medical Instrument Segmentation (ROBUST-MIS) challenge 7 organized as part of the Endoscopic Vision (EndoVis) challenge (https://endovis.grand-challenge.org/) at the International Conference on Medical Image Computing and Computer Assisted Interventions (MICCAI) 2019. ROBUST-MIS comprised three tasks, each requiring participating algorithms to annotate endoscopic image frames (Fig. 2) . For the binary segmentation task, participants had to provide precise contours of instruments, represented by binary masks, with '1' indicating the presence of a surgical instrument in a given pixel and '0' representing the absence thereof. Analogously, for the multi-instance segmentation task, participants had to provide image masks with numbers '1' , '2' , etc. which represented different instances of medical instruments. In contrast, Instruments per frame: 0-5",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 148,
                        "text": "6",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 562,
                        "end": 563,
                        "text": "7",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 886,
                        "end": 894,
                        "text": "(Fig. 2)",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Number of videos: 10",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sigmoid resection",
                "sec_num": null
            },
            {
                "text": "Median duration: 2h 32'",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sigmoid resection",
                "sec_num": null
            },
            {
                "text": "Instruments per frame: 0-7",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Sigmoid resection",
                "sec_num": null
            },
            {
                "text": "Number of videos: 10",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rectal resection",
                "sec_num": null
            },
            {
                "text": "Median duration: 3h 36'",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rectal resection",
                "sec_num": null
            },
            {
                "text": "Instruments per frame: 0-5",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rectal resection",
                "sec_num": null
            },
            {
                "text": "Overview of the Heidelberg Colorectal (HeiCo) data set. Raw data comprises anonymized, downsampled laparoscopic video data from three different types of colorectal surgery along with corresponding streams from medical devices in the operating room. Annotations include surgical phase information for the entire video sequences as well as information on instrument presence and corresponding instance-wise segmentation masks of medical instruments (if any) for more than 10,000 frames.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fig. 1",
                "sec_num": null
            },
            {
                "text": "the multi-instance detection task merely required participants to detect and roughly locate instrument instances in video frames. The location could be represented by arbitrary forms, such as bounding boxes. Information on the activity of medical devices and the surgical phase was also provided as context information for each frame in the 30 videos. This information was obtained from the annotations generated as part of the MICCAI EndoVis Surgical workflow analysis in the sensor operating room 2017 challenge (https://endoviss-ub2017-workflow.grand-challenge.org/). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fig. 1",
                "sec_num": null
            },
            {
                "text": "The data set was generated using the following multi-stage process: recording of surgical data. Data acquisition took place during daily routine procedures in the integrated operating room KARL STORZ OR1 FUSION \u00ae (KARL STORZ SE & Co KG, Tuttlingen, Germany) at Heidelberg University Hospital, Department of Surgery, a certified center of excellence for minimally invasive surgery. Videos from 30 surgical procedures in three different types of surgery served as a basis for this data set: 10 proctocolectomy procedures, 10 rectal resection procedures, and 10 sigmoid resection procedures. While previous research on surgical skill and workflow analysis and corresponding publicly released data have focused on ex vivo training scenarios 8 and comparatively simple procedures, such as cholecystectomy 9 , we placed emphasis on colorectal surgery. As these procedures are more complex, more variations occur in surgical strategy (e.g. length or order of phases) and phases may occur repeatedly.",
                "cite_spans": [
                    {
                        "start": 737,
                        "end": 738,
                        "text": "8",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 800,
                        "end": 801,
                        "text": "9",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "I",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "All video data were recorded with a laparoscopic camera from KARL STORZ Image 1, with a forward-oblique telescope 30\u00b0. The KARL STORZ Xenon 300 was used as a cold light source. To comply with ethical standards and the general data protection regulation of the European Union, data were anonymized. To this end, frames corresponding to parts of the surgery performed outside of the abdomen were manually identified and subsequently replaced by blue images. Image resolution was scaled down from 1920 \u00d7 pixels (high definition (HD)) in the primary video to 960 \u00d7 in our data set. In addition, KARL STORZ OR1 FUSION \u00ae was used to record additional data streams from medical devices in the room, namely Insufflator Thermoflator, OR lights, cold light fountain Xenon 300, and Camera Image 1. A complete list of all parameters and the corresponding descriptions can be found in Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 872,
                        "end": 879,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "It is worth noting that all three surgery types contained in this work included an extra-abdominal phase (bowel anastomosis; the connection of two parts of bowel) that was executed extra-abdominally without use of www.nature.com/scientificdata www.nature.com/scientificdata/ the laparoscope. As all three types of surgical procedure take place in the same anatomical region, many phases occur in two or all three of the procedures, as shown in Online-only Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 456,
                        "end": 463,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Methods",
                "sec_num": null
            },
            {
                "text": "We use the following terminology throughout the remainder of this manuscript based on the definitions provided by . Phases represent the highest level of hierarchy in surgical workflow analysis and consist of several steps. Steps are composed of surgical activities that aim to reach a specific goal. Activities represent the lowest level of hierarchy as \"a physical task\" or \"well-defined surgical motion unit\" such as dissecting, dividing or suturing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation of videos.",
                "sec_num": null
            },
            {
                "text": "In our data set, phases were modelled by surgical experts by first dividing the surgical procedure by dominant surgical activity, namely orientation (in the abdomen), mobilization (of colon), division (of vessels), (retroperitoneal) dissection (of rectum), and reconstruction with anastomosis. Subsequently, these parts were subdivided into phases by anatomical region. For example, the mobilization of colon was divided into phases for the sigmoid and descending colon, transverse colon, ascending colon and splenic flexure. Each phase received a unique ID, as shown in Online-only Table 1 . Furthermore, during the annotation process, aberrations from the defined standard phase definitions occurred that had not been modelled beforehand. Examples include an additional cholecystectomy or a bladder injury. These phases were subsumed as \"exceptional phases\" (ID 13; see Online-only Table 1 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 583,
                        "end": 590,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 884,
                        "end": 891,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Annotation of videos.",
                "sec_num": null
            },
            {
                "text": "The annotator (surgical resident) had access to the endoscopic video sequence of the surgical procedure. The result of the annotation was a list of predefined phases for each video (represented by the IDs provided in Online-only Table 1 ) with corresponding timestamps denoting their starting points. The labeling was performed according to the following protocol:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 229,
                        "end": 236,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Annotation of videos.",
                "sec_num": null
            },
            {
                "text": "1. Definition of the start of a phase a. A phase starts when the instrument related to the first activity relevant for this phase enters the screen. Example: a grasper providing tissue tension for dissection of the sigmoid mesocolon in order to identify and dissect the inferior mesenteric artery. b. If a change of the anatomical region (such as change from mobilization of ascending colon to mobilization of transverse colon) results in the transition to a new phase, the camera movement towards the new region marks the start of the phase. c. If the camera leaves the body or is pulled back into the trocar between two phases, the new phase starts with the first frame that does not show the trocar in which the camera is located. 2. Definition of the end of a phase: a. A phase is defined by its starting point. The end of a phase thus occurs when the next phase starts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation of videos.",
                "sec_num": null
            },
            {
                "text": "This implies that idle time is assigned to the preceding phase. Note that while phases in other surgeries, such as cholecystectomy, follow a rigid process, this is not the case for more complex surgeries, such as the ones subject to this data set. In other words, each phase can occur multiple times. Moreover, colorectal surgery comes with possible technical variations between centers, surgeons and procedures. For example, in sigmoid resection, some surgeons may choose a tubular resection of the mesentery over a central dissection of vessels and lymph nodes en bloc for benign disease, which results in completely omitting the respective phase.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Annotation of videos.",
                "sec_num": null
            },
            {
                "text": "From the 30 surgical procedures described above, a total of 10,040 frames were extracted for instrument segmentation. In the first step, a video frame was extracted every 60 seconds. In this www.nature.com/scientificdata www.nature.com/scientificdata/ process, blue frames included due to video anonymization (see Recording of data) were ignored. This resulted in a total of 4,456 frames (corresponding to the extracted IDs) for annotation. To reach the goal of annotating more than 10,000 video frames in total, it was decided to place a particular focus on interesting snippets of the video. Surgical workflow analysis is currently a very active field of research. For an accurate segmentation of a video into surgical phases, it requires the detection of the transition from one surgical phase to the next. For this reason, frames corresponding to surgical phase transitions were obtained in seven of the 30 videos (three from rectal resection, two from the other two types of surgery). More specifically, frames within 25 seconds of the phase transition (before and after) were sampled every second (again, excluding blue frames). This led to a doubling of the number of annotated frames. Statistics of the number of frames selected for the different procedures are provided in Table 2 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 1282,
                        "end": 1289,
                        "text": "Table 2",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Selection of frames.",
                "sec_num": null
            },
            {
                "text": "An initial segmentation of the instruments in the selected frames was performed by the company understand.ai (https://understand.ai/). To this end, a U-Net style neural network architecture 11 was trained on a small manually labeled subset of the data set. This network was then used to label the rest of the data set in a semi-automated way; based on pixel-wise segmentation proposals, a manual refinement was performed, following previous data annotation policies 4 . Based on this initial segmentation, a comprehensive quality and consistency analysis was performed and a detailed annotation protocol was developed, which is provided in the Supplementary Methods. Based on this protocol, the initial annotations were refined/completed by an annotation team of four medical students and 14 engineers. In ambiguous or unclear cases, a team of two engineers and one medical student generated a consensus annotation. For quality control, two medical experts went through all of the segmentation masks and reported potential errors which were then corrected by members of the annotation team. Final agreement on each label was generated by a team comprising a medical expert and an engineer. Examples of annotated frames are provided in Figs. 2 and 3 .",
                "cite_spans": [
                    {
                        "start": 190,
                        "end": 192,
                        "text": "11",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 466,
                        "end": 467,
                        "text": "4",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1235,
                        "end": 1248,
                        "text": "Figs. 2 and 3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Annotation of frames.",
                "sec_num": null
            },
            {
                "text": "Typically, a challenge has a training phase. At the beginning of the training phase, the challenge organizers release training cases with the relevant reference annotations 6 . These help the participating teams in developing their method (e.g. by training a machine learning algorithm). In the test phase, the participating teams either upload their algorithm, or they receive access to the test cases without the reference annotations and submit the results that their algorithm has achieved for the test cases to the challenge organizers. To enable comparative benchmarking to be executed for this challenge paradigm, our data set was split into a training set and a test set. Our data set was arranged such that it allows for validation of detection/ binary segmentation/multi-instance segmentation algorithms in three test stages:",
                "cite_spans": [
                    {
                        "start": 173,
                        "end": 174,
                        "text": "6",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation of challenge data set.",
                "sec_num": null
            },
            {
                "text": "\u2022 Stage 1: The test data are taken from the procedures (patients) from which the training data were extracted.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generation of challenge data set.",
                "sec_num": null
            },
            {
                "text": "The test data are taken from the exact same type of surgery as the training data but from procedures (patients) that were not included in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Stage 2:",
                "sec_num": null
            },
            {
                "text": "The test data are taken from a different but similar type of surgery (and different patients) compared to the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Stage 3:",
                "sec_num": null
            },
            {
                "text": "Following this concept, the data set was split into training and test data as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Stage 3:",
                "sec_num": null
            },
            {
                "text": "\u2022 The data from all 10 sigmoid resection surgery procedures were reserved for testing in stage 3. We picked sigmoid resection for stage 3 as it comprised the lowest number of annotated frames and we aimed to come as close as possible to the recommended 80%/20% split of training and test data. \u2022 Of the 20 remaining videos corresponding to proctocolectomy and rectal resection procedures, 80% were reserved for training and 20% (i.e. two procedures of each type) were reserved for testing in stage 2. More specifically, the two patients with the lowest number of annotated frames were taken as test data for stage 2 (for both rectal resection and proctocolectomy). Again, the reason for this choice was to increase the size of the training data set compared to the test set. \u2022 For stage 1, every 10th annotated frame from the remaining 2*(10-2) = 16 procedures was used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Stage 3:",
                "sec_num": null
            },
            {
                "text": "In summary, this amounted to a total of 10,040 frames, distributed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 Stage 3:",
                "sec_num": null
            },
            {
                "text": "\u2022 Training data: 5,983 frames in total (2,943 frames from proctocolectomy surgery and 3,040 frames from rectal resection surgery) \u2022 Test data (4,057 frames in total): As suggested in 6 , we use the term case to refer to a data set for which the algorithm(s) participating in a specific challenge task produce one result (e.g. a segmentation map). To enable instrument detection/segmentation algorithms to take temporal context into account, we define a case as a 10 second video snippet comprising 250 endoscopic image frames (not annotated) and an annotation mask for the last frame (Fig. 4) . In the mask, a '0' indicates the absence of a medical instrument and numbers '1' , '2' , \u2026 represent different instances of medical instruments.",
                "cite_spans": [
                    {
                        "start": 183,
                        "end": 184,
                        "text": "6",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 584,
                        "end": 592,
                        "text": "(Fig. 4)",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "\u2022 Stage 3:",
                "sec_num": null
            },
            {
                "text": "The primary data set of this paper corresponds to the ROBUST-MIS 2019 challenge and features 30 surgical videos along with frame-based instrument annotations for more than 10,000 frames. The annotations for this data set underwent a rigorous multi-stage quality control process. This data set is complemented by surgical phase annotations for the 30 videos which were used in the Surgical Workflow Analysis in the sensorOR challenge organized in 2017.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Records",
                "sec_num": null
            },
            {
                "text": "The data can be accessed in two primary ways: (1) As a complete data set that contains videos and medical device data along with corresponding annotations (surgical workflow and instrument segmentations) following the folder structure shown in Fig. 4 or (2) as ROBUST-MIS challenge data sets that represent the split of the data into training and test sets as used in the ROBUST-MIS challenge 2019 (Fig. 5) .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 244,
                        "end": 257,
                        "text": "Fig. 4 or (2)",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 398,
                        "end": 406,
                        "text": "(Fig. 5)",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Data Records",
                "sec_num": null
            },
            {
                "text": "Complete data set. To access the complete data sets (without a split in training and test data), users are www.nature.com/scientificdata www.nature.com/scientificdata/ rOBUST-MIS challenge data set. The segmentation data is additionally provided in the way it was avail-",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Records",
                "sec_num": null
            },
            {
                "text": "Validation of segmentations. The verification of the annotations was part of the data annotation procedure, as detailed above. To estimate the inter-rater reliability, five of the annotators that had curated the data set segmented (the same) 20 randomly selected images from the data set, where each image was drawn from a different surgery and contained up to three instrument instances. As we were interested in the inter-rater reliability for instrument instances rather than for whole images, we evaluated all 34 visible instrument instances of these 20 images individually. The S\u00f8rensen Dice Similarity Coefficient (DSC) 15 and the Haussdorf distance (HD) 16 were used as metric for contour agreement as they are the most widely used metrics for assessing segmentation quality 17 . For each instrument instance, we determined the DSC/HD for all combinations of two different raters. This yielded a median DSC of 0.96 (mean: 0.88, 25-quantile: 0.91, 75-quantile: 0.98) and a median HD of 12.8 (mean: 89.3, 25-quantile: 7.6, 25-quantile: 7.6, 75-quantile: 36.1) determined over all tuples of annotations and instrument instances. Manual analysis showed that outliers mainly occurred primarily if one or multiple of the raters did not detect a specific instance. It should be noted that an agreement of around 0.95 is extremely high given previous studies on inter-rater variability 18 .",
                "cite_spans": [
                    {
                        "start": 626,
                        "end": 628,
                        "text": "15",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 661,
                        "end": 663,
                        "text": "16",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 782,
                        "end": 784,
                        "text": "17",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 1385,
                        "end": 1387,
                        "text": "18",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "The recorded data and the corresponding segmentations/workflow annotations were used as basis for the ROBUST-MIS challenge 2019 (https://phabricator.mitk.org/source/rmis2019/). According to the challenge results 7 , the performance of algorithms decreases as the domain gap between training and test data increases. In fact, the performance dropped by 3% and 5% for the binary and multi-instance segmentation respectively (comparison of stage 1 with stage 3). This confirms our initial hypothesis that splitting the data set as suggested is useful for developing and validating algorithms with a specific focus on their generalization capabilities.",
                "cite_spans": [
                    {
                        "start": 212,
                        "end": 213,
                        "text": "7",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "Validation of surgical phase annotations. The phase annotations primarily serve as context information, which is why we did not put a focus on their validation. However, the following study was conducted to approximate intra-rater and inter-rater agreement for phases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "To assess the quality of the phase annotations, we randomly selected 10 time points in each of the procedures resulting in n = 300 video frames. Then, we extracted a video snippet comprising 30 seconds before and 30 seconds after the respective frame from the video. The frames were independently categorized by five expert surgeons with at least 6 years of surgical experience, including the surgeon who performed the phase definition www.nature.com/scientificdata www.nature.com/scientificdata/ for our dataset in the first place, into the corresponding surgical phases according to our definition in Online-only Table 1 . If the video snippet did not provide enough context to determine the phase, the surgeons reviewed the whole video.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 615,
                        "end": 622,
                        "text": "Table 1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "For the statistical analysis, we compared the original annotation to the new annotation of the original rater (intra-rater agreement) and to the new annotation of the other surgeons (inter-rater agreement). Agreement between ground truth and raters was calculated as Cohen's kappa which quantifies agreement between two raters adjusted for agreement expected by chance alone. Calculation of unweighted kappa (for nominal ratings) with a 95% confidence interval (CI) was made by SAS Version 9.4 (SAS Inc., Cary, North Carolina, USA). To assess agreement between all five raters Fleiss' kappa for nominal ratings was performed with the function confIntKappa from the R package biostatUZH with 1,000 bootstraps (R Version 4.0.2, https://www.R-project.org). Values of kappa between 0.81 and 1.00 can be considered almost perfect. Intra-rater agreement between ground truth and new annotation by the original rater was 0.834 (CI 0.789-0.879) and inter-rater agreement between reference and each of the four other raters ranged from 0.682 (CI 0.626-0.739) to 0.793 (0.744-0.842). Accordingly, inter-rater agreement between reference and raters was at least substantial. Intra-rater agreement was almost perfect. Fleiss' kappa for agreement of all 5 raters was 0.712 (bootstrap 95% CI 0.673-0.747).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "The recorded data and the corresponding segmentations/workflow annotations were used as the basis for the Surgical Workflow Analysis in the sensorOR 2017 challenge (https://endovissub2017-workflow.grand-challenge.org/).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "The following data set characteristics have been computed based on the video and frame annotations. Eight of the 13 phases occurred in all three surgical procedures. The median (min;max) number of surgical phase transitions for proctocolectomy, rectal resection and sigmoid resection was 19 (15;25), 20 (10;31) and 14 (9;30) respectively. The median duration of the phases is summarized in Table 3 . As shown in Table 4 , the number of instruments per frame ranges from 0-7, thus reflecting the wide range of scenarios that can occur in clinical practice. Most frames (>70% for all three procedures) contain only one or two instruments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 390,
                        "end": 397,
                        "text": "Table 3",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 412,
                        "end": 419,
                        "text": "Table 4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Technical Validation",
                "sec_num": null
            },
            {
                "text": "A limitation of our data set could be seen in the fact that the phase annotations were performed by only a single expert surgeon. It should be noted, however, that the phase annotations merely served as context information while the segmentations, which were generated with a highly quality-controlled process, are in the focus of this work. Furthermore, we acquired data from only one hospital. This implies limited variability with respect to the acquisition conditions as only one specific endoscope and light source were used. Still, to our knowledge, the HeiCo data set is the only publicly available data set (1) based on multiple different surgeries and (2) comprising not only annotated video data but also sensor data from medical devices in the operating room.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of the data set.",
                "sec_num": null
            },
            {
                "text": "The data set was published under a Creative Commons Attribution-NonCommercial-ShareAlike (CC BY-NC-SA) license, which means that it will be publicly available for non-commercial usage. Should you wish to use or refer to this data set, you must cite this paper. The licensing of new creations must use the exact same terms as in the current version of the data set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Usage Notes",
                "sec_num": null
            },
            {
                "text": "For benchmarking instrument segmentation algorithms, we recommend using the scripts provided for the ROBUST-MIS challenge (https://phabricator.mitk.org/source/rmis2019/). They include Python files for downloading the data from the Synapse platform and evaluation scripts for the performance measures used in the challenge. For benchmarking surgical workflow analysis algorithms, we recommend using the script provided on Synapse 19 for the surgical workflow challenges. ",
                "cite_spans": [
                    {
                        "start": 429,
                        "end": 431,
                        "text": "19",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Usage Notes",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "This project has been funded by the Surgical Oncology Program of the National Center for Tumor Diseases (NCT) Heidelberg and the project \"OP4.1, \" funded by the German Federal Ministry of Economic Affairs and Energy (grant number BMWI 01MT17001C). In addition, the project has been funded by \"InnOPlan\", funded by the German Federal Ministry of Economic Affairs and Energy (grant number BMWI 01MD15002E). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            },
            {
                "text": "The data set can be used without any further code. As stated in the usage notes, we recommend using the scripts provided for the ROBUST-MIS and surgical workflow challenges (https://phabricator.mitk.org/source/ rmis2019/ and 19 ) as well as the challengeR package (https://github.com/wiesenfa/challengeR) for comparative benchmarking of algorithms. ",
                "cite_spans": [
                    {
                        "start": 225,
                        "end": 227,
                        "text": "19",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Code availability",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Surgical data science for next-generation interventions",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Maier-Hein",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Nat. Biomed. Eng",
                "volume": "1",
                "issue": "",
                "pages": "691--696",
                "other_ids": {
                    "DOI": [
                        "10.1038/s41551-017-0132-7"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maier-Hein, L. et al. Surgical data science for next-generation interventions. Nat. Biomed. Eng. 1, 691-696, https://doi.org/10.1038/ s41551-017-0132-7 (2017).",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Learning where to look while tracking instruments in robot-assisted surgery",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Islam",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Image Comput. Comput. Assist. Interv",
                "volume": "",
                "issue": "",
                "pages": "412--420",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Islam, M., Li, Y. & Ren, H. Learning where to look while tracking instruments in robot-assisted surgery. in Med. Image Comput. Comput. Assist. Interv., 412-420 (Springer, 2019).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Video-based surgical skill assessment using 3D convolutional neural networks",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Funke",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "T"
                        ],
                        "last": "Mees",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Weitz",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Speidel",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Int. J. Comput. Assist. Radiol. Surg",
                "volume": "14",
                "issue": "",
                "pages": "1217--1225",
                "other_ids": {
                    "DOI": [
                        "10.1007/s11548-019-01995-1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Funke, I., Mees, S. T., Weitz, J. & Speidel, S. Video-based surgical skill assessment using 3D convolutional neural networks. Int. J. Comput. Assist. Radiol. Surg. 14, 1217-1225, https://doi.org/10.1007/s11548-019-01995-1 (2019).",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Robotic instrument segmentation challenge",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Allan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Allan, M. et al. 2017 Robotic instrument segmentation challenge. Preprint at https://arxiv.org/abs/1902.06426 (2019).",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Exploiting the potential of unlabeled endoscopic video data with self-supervised learning",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Int. J. Comput. Assist. Radiol. Surg",
                "volume": "13",
                "issue": "",
                "pages": "925--933",
                "other_ids": {
                    "DOI": [
                        "10.1007/s11548-018-1772-0"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ross, T. et al. Exploiting the potential of unlabeled endoscopic video data with self-supervised learning. Int. J. Comput. Assist. Radiol. Surg. 13, 925-933, https://doi.org/10.1007/s11548-018-1772-0 (2018).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "BIAS: Transparent reporting of biomedical image analysis challenges",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Maier-Hein",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Med. Image Anal",
                "volume": "66",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.media.2020.101796"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maier-Hein, L. et al. BIAS: Transparent reporting of biomedical image analysis challenges. Med. Image Anal. 66, 101796, https://doi. org/10.1016/j.media.2020.101796 (2020).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Comparative validation of multi-instance instrument segmentation in endoscopy: results of the ROBUST-MIS 2019 challenge",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Med. Image Anal",
                "volume": "101920",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.media.2020.101920"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ross, T. et al. Comparative validation of multi-instance instrument segmentation in endoscopy: results of the ROBUST-MIS 2019 challenge. Med. Image Anal. 101920, https://doi.org/10.1016/j.media.2020.101920 (2020).",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Ahmidi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IEEE Trans. Biomed. Eng",
                "volume": "64",
                "issue": "",
                "pages": "2025--2041",
                "other_ids": {
                    "DOI": [
                        "10.1109/TBME.2016.2647680"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ahmidi, N. et al. A dataset and benchmarks for segmentation and recognition of gestures in robotic surgery. IEEE Trans. Biomed. Eng. 64, 2025-2041, https://doi.org/10.1109/TBME.2016.2647680 (2017).",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "EndoNet: a deep architecture for recognition tasks on laparoscopic videos",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "P"
                        ],
                        "last": "Twinanda",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IEEE Trans. Med. Imaging",
                "volume": "36",
                "issue": "",
                "pages": "86--97",
                "other_ids": {
                    "DOI": [
                        "10.1109/TMI.2016.2593957"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Twinanda, A. P. et al. EndoNet: a deep architecture for recognition tasks on laparoscopic videos. IEEE Trans. Med. Imaging 36, 86-97, https://doi.org/10.1109/TMI.2016.2593957 (2017).",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Surgical process modelling: a review",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Lalys",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Jannin",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Int. J. CARS",
                "volume": "9",
                "issue": "",
                "pages": "495--511",
                "other_ids": {
                    "DOI": [
                        "10.1007/s11548-013-0940-5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lalys, F. & Jannin, P. Surgical process modelling: a review. Int. J. CARS 9, 495-511, https://doi.org/10.1007/s11548-013-0940-5 (2014).",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "U-net: convolutional networks for biomedical image segmentation",
                "authors": [
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Ronneberger",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Fischer",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Brox",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Med",
                "volume": "",
                "issue": "",
                "pages": "234--241",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronneberger, O., Fischer, P. & Brox, T. U-net: convolutional networks for biomedical image segmentation. In Med. Image Comput. Comput. Assist. Interv., 234-241 (Springer, 2015).",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Deep learning",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Heidelberg Colorectal (HeiCo) Data Set for Surgical Data Science in the Sensor Operating Room",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7303/syn21903917"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ross, T. et al. Heidelberg Colorectal (HeiCo) Data Set for Surgical Data Science in the Sensor Operating Room. Synapse https://doi. org/10.7303/syn21903917 (2020).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Endoscopic Vision Challenge, Sub-Challenge -Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7303/syn18779624"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ross, T. et al. Endoscopic Vision Challenge, Sub-Challenge -Robust Medical Instrument Segmentation (ROBUST-MIS) Challenge 2019. Synapse https://doi.org/10.7303/syn18779624 (2019).",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Measures of the amount of ecologic association between species",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "R"
                        ],
                        "last": "Dice",
                        "suffix": ""
                    }
                ],
                "year": 1945,
                "venue": "Ecology",
                "volume": "26",
                "issue": "",
                "pages": "297--302",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dice, L. R. Measures of the amount of ecologic association between species. Ecology 26, 297-302 (1945).",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Comparing images using the Hausdorff distance",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "P"
                        ],
                        "last": "Huttenlocher",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "A"
                        ],
                        "last": "Klanderman",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "J"
                        ],
                        "last": "Rucklidge",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
                "volume": "15",
                "issue": "",
                "pages": "850--863",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huttenlocher, D. P., Klanderman, G. A. & Rucklidge, W. J. Comparing images using the Hausdorff distance. IEEE Trans. Pattern Anal. Mach. Intell. 15, 850-863 (1993).",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Why rankings of biomedical image analysis competitions should be interpreted with care",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Maier-Hein",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Nat. Commun",
                "volume": "9",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1038/s41467-018-07619-7"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Maier-Hein, L. et al. Why rankings of biomedical image analysis competitions should be interpreted with care. Nat. Commun. 9, 5217, https://doi.org/10.1038/s41467-018-07619-7 (2018).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Inter-observer variability of manual contour delineation of structures in CT",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Joskowicz",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cohen",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Caplan",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Sosna",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Eur. Radiol",
                "volume": "29",
                "issue": "",
                "pages": "1391--1399",
                "other_ids": {
                    "DOI": [
                        "10.1007/s00330-018-5695-5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Joskowicz, L., Cohen, D., Caplan, N. & Sosna, J. Inter-observer variability of manual contour delineation of structures in CT. Eur. Radiol. 29, 1391-1399, https://doi.org/10.1007/s00330-018-5695-5 (2019).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Heidelberg Colorectal (HeiCo) Data Set",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bodenstedt",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.7303/syn21898456"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bodenstedt, S. Heidelberg Colorectal (HeiCo) Data Set. Synapse https://doi.org/10.7303/syn21898456 (2020).",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Methods and open-source toolkit for analyzing and visualizing challenge results",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wiesenfarth",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Sci. Rep",
                "volume": "11",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1038/s41598-021-82017-6"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wiesenfarth, M. et al. Methods and open-source toolkit for analyzing and visualizing challenge results. Sci. Rep. 11, 2369, https:// doi.org/10.1038/s41598-021-82017-6 (2021).",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "segmentations as a medical expert. L.M. preprocessed, structured and analyzed the medical device data. A.K.-S. coordinated and implemented the statistical analyses. M.T. analyzed the data and contributed to the writing and proofreading of the manuscript. S.Sp. and S.B. co-organized the surgical workflow analysis in the sensorOR challenge, developed the concept for the phase annotations, analyzed the surgical phase annotations and the medical device data, and contributed to the writing and proofreading of the manuscript",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1038/s41597-021-00882-2"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "8:101 | https://doi.org/10.1038/s41597-021-00882-2 www.nature.com/scientificdata www.nature.com/scientificdata/ instrument segmentations as a medical expert. L.M. preprocessed, structured and analyzed the medical device data. A.K.-S. coordinated and implemented the statistical analyses. M.T. analyzed the data and contributed to the writing and proofreading of the manuscript. S.Sp. and S.B. co-organized the surgical workflow analysis in the sensorOR challenge, developed the concept for the phase annotations, analyzed the surgical phase annotations and the medical device data, and contributed to the writing and proofreading of the manuscript. F.N., H.G.K. and P.P. performed the phase validation. H.G.K. and B.M. acquired the data, developed the concept for the phase annotations, (co-) organized the surgical workflow analysis in the sensorOR challenge and the ROBUST-MIS challenge, and proofread the manuscript.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "fig_num": "2",
                "type_str": "figure",
                "num": null,
                "text": "Laparoscopic images representing various levels of difficulty for the tasks of medical instrument detection, binary segmentation and multi-instance segmentation. Raw input frames (a) and corresponding reference segmentation masks (b) computed from the reference contours.",
                "uris": null
            },
            "FIGREF2": {
                "fig_num": null,
                "type_str": "figure",
                "num": null,
                "text": ". Recording of surgical data II. Annotation of videos (surgical phases) III. Selection of frames for surgical instrument segmentation IV. Annotation of frames (surgical instruments) A. Generation of protocol for instrument segmentation B. Segmentation of instruments C. Verification of annotations V. Generation of challenge data set Details are provided in the following paragraphs.",
                "uris": null
            },
            "FIGREF3": {
                "fig_num": "3",
                "type_str": "figure",
                "num": null,
                "text": "Examples of challenging frames overlaid with reference multi-instance segmentations created by surgical data science experts.",
                "uris": null
            },
            "FIGREF5": {
                "fig_num": "4",
                "type_str": "figure",
                "num": null,
                "text": "Folder structure for the complete data set. It comprises five levels corresponding to (1) surgery type, (2) procedure number, (3) procedural data (video and device data along with phase annotations), (4) frame number and (5) frame-based data. (2021) 8:101 | https://doi.org/10.1038/s41597-021-00882-2",
                "uris": null
            },
            "FIGREF6": {
                "fig_num": "5",
                "type_str": "figure",
                "num": null,
                "text": "Folder structure for the ROBUST-MIS challenge data set. It comprises five levels corresponding to (1) data type (training/test), (2) surgery type, (3) procedure number, (4) frame number and (5) case data. (2021) 8:101 | https://doi.org/10.1038/s41597-021-00882-2",
                "uris": null
            },
            "TABREF1": {
                "type_str": "table",
                "content": "<table><tr><td/><td/><td>Flow Actual</td></tr><tr><td/><td/><td>Flow Target</td></tr><tr><td/><td/><td>Pressure Actual</td></tr><tr><td/><td/><td>Pressure Target</td></tr><tr><td/><td/><td>Gas Volume</td></tr><tr><td/><td/><td>Supply Pressure</td></tr><tr><td/><td/><td>Light1 On</td></tr><tr><td>OR lights LED2 (Dr. Mach GmBH &amp; Co KG,</td><td>Light mounted onto movable arms on the ceiling. Used to</td><td>Light1 Intensity Actual</td></tr><tr><td>Germany)</td><td>illuminate the patient's abdomen during open surgery.</td><td>Light2 On</td></tr><tr><td/><td/><td>Light2 Intensity Actual</td></tr><tr><td>Coldlight fountain Xenon 300 (KARL</td><td>Light source that illuminates the abdomen via a light cable</td><td>Intensity Actual</td></tr><tr><td>STORZ SE &amp; Co KG, Tuttlingen, Germany)</td><td>mounted onto the laparoscopic camera.</td><td>Standby</td></tr><tr><td/><td/><td>White Balance</td></tr><tr><td>Camera Image 1 (KARL STORZ SE &amp; Co.</td><td>Endoscopic camera control unit for use with both single and</td><td>Shutter Speed</td></tr><tr><td>KG, Tuttlingen, Germany)</td><td>three-chip camera heads.</td><td>Brightness</td></tr><tr><td/><td/><td>Enhancement</td></tr></table>",
                "html": null,
                "num": null,
                "text": "Insufflator Thermoflator (KARL STORZ SE & Co. KG, Tuttlingen, Germany) Device used to insufflate the abdomen with carbon dioxide in order to create space for the minimally invasive surgery. Medical devices of the operating room and corresponding sensor streams provided by KARL STORZ OR1 FUSION \u00ae (KARL STORZ SE & Co KG, Tuttlingen, Germany)."
            },
            "TABREF2": {
                "type_str": "table",
                "content": "<table><tr><td>www.nature.com/scientificdata</td></tr></table>",
                "html": null,
                "num": null,
                "text": "Number of frames selected from the different procedures."
            },
            "TABREF5": {
                "type_str": "table",
                "content": "<table><tr><td/><td colspan=\"3\">Number of frames with n instruments</td><td/><td/><td/><td/><td/></tr><tr><td>Surgery type</td><td>n = 0</td><td>n = 1</td><td>n = 2</td><td>n = 3</td><td>n = 4</td><td>n = 5</td><td>n = 6</td><td>n = 7</td></tr><tr><td>Proctocolectomy</td><td>(12.9%)</td><td>1,697 (48.6%)</td><td>1,063 (30.4%)</td><td>227 (6.5%)</td><td>54 (1.5%)</td><td>2 (0.1%)</td><td>0 (0.0%)</td><td>0 (0.0%)</td></tr><tr><td>Rectal surgery</td><td>(19.5%)</td><td>1,850 (50.4%)</td><td>917 (25.0%)</td><td>158 (4.3%)</td><td>21 (0.6%)</td><td>7 (0.2%)</td><td>0 (0.0%)</td><td>0 (0.0%)</td></tr><tr><td>Sigmoid surgery</td><td>(22.6%)</td><td>1,198 (41.6%)</td><td>827 (28.7%)</td><td>178 (6.2%)</td><td>24 (0.8%)</td><td>2 (0.1%)</td><td>0 (0.0%)</td><td>1 (0.0%)</td></tr></table>",
                "html": null,
                "num": null,
                "text": "Median duration of phases[min]. IDs are introduced in Online-onlyTable 1. *Phase 4 (mobilization of sigmoid colon and descending colon) is shorter for proctocolectomy because no oncological but tubular resection is performed. **Phase 8 (dissection and resection of the rectum) is shorter for sigmoid resection because only the proximal part of the rectum, but not the middle and distal part of the rectum are subject to removal."
            },
            "TABREF6": {
                "type_str": "table",
                "content": "<table/>",
                "html": null,
                "num": null,
                "text": "Number of instruments in annotated frames. Most frames (>70% for all three procedures) contain one or two instruments."
            }
        }
    }
}