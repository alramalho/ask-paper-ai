#### Abstract
Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. 1 

####  Introduction
Chest radiography is the most common imaging examination globally, critical for screening, diagnosis, and management of many life threatening diseases. Automated chest radiograph interpretation at the level of practicing radiologists could provide substantial benefit in many medical settings, from improved workflow prioritization and clinical decision support to large-scale screening and global population health initiatives. For progress, there is a need for labeled datasets that (1) are large, (2) have strong reference standards, and (3) provide expert human performance metrics for comparison.
Figure 1 : The CheXpert task is to predict the probability of different observations from multi-view chest radiographs.
In this work, we present CheXpert (Chest eXpert), a large dataset for chest radiograph interpretation. The dataset consists of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 common chest radiographic observations. We design a labeler that can extract observations from free-text radiology reports and capture uncertainties present in the reports by using an uncertainty label.
The CheXpert task is to predict the probability of 14 different observations from multi-view chest radiographs (see Figure 1 ). We pay particular attention to uncertainty labels in the dataset, and investigate different approaches towards incorporating those labels into the training process. We as- Table 1 : The CheXpert dataset consists of 14 labeled observations. We report the number of studies which contain these observations in the training set.
sess the performance of these uncertainty approaches on a validation set of 200 labeled studies, where ground truth is set by a consensus of 3 radiologists who annotated the set using the radiographs. We evaluate the approaches on 5 observations selected based on their clinical significance and prevalence in the dataset, and find that different uncertainty approaches are useful for different observations. We compare the performance of our final model to 3 additional board certified radiologists on a test set of 500 studies on which the consensus of 5 separate board-certified radiologists serves as ground truth. We find that on 4 out of 5 pathologies, the model ROC and PR curves lie above at least 2 of 3 radiologist operating points. We make our dataset publicly available to encourage further development of models.

####  Labeler Results
We evaluate the performance of the labeler and compare it to the performance of another automated radiology report labeler on a report evaluation set.

####  Validation Results
We compare the performance of the different uncertainty approaches on a validation set on which the consensus of radi- Atelectasis (>0 rads)
Figure 3 : We compare the performance of 3 radiologists to the model against the test set ground truth in both the ROC and the PR space. We examine whether the radiologist operating points lie below the curves to determine if the model is superior to the radiologists. We also compute the lower (LabelL) and upper bounds (LabelU) of the performance of the labels extracted automatically from the radiology report using our labeling system against the test set ground truth.
ologist annotations serves as ground truth.

####  Results
The validation AUCs achieved by the different approaches to using the uncertainty labels are shown in Table 3. There are a few significant differences between the performance of the uncertainty approaches. On Atelectasis, the U-Ones model (AUC=0.858) significantly outperforms (p = 0.03) the U-Zeros model (AUC=0.811). On Cardiomegaly, we observe that the U-MultiClass model (AUC=0.854) performs significantly better (p < 0.01) than the U-Ignore model (AUC=0.828). On Consolidation, Edema and Pleural Effusion, we do not find the best models to be significantly better than the worst.
Analysis We find that ignoring the uncertainty label is not an effective approach to handling uncertainty in the dataset, and is particularly ineffective on Cardiomegaly. Most of the uncertain Cardiomegaly cases are borderline cases such as "minimal cardiac enlargement", which if ignored, would likely cause the model to perform poorly on cases which are difficult to distinguish. However, explicitly supervising the model to distinguish between borderline and non-borderline cases (as in the U-MultiClass approach) could enable the model to better disambiguate the borderline cases. Moreover, assignment of the Cardiomegaly label when the heart is mentioned in the impression are difficult to categorize in many cases, particularly for common mentions such as "un- changed appearance of the heart" or "stable cardiac contours" either of which could be used in both enlarged and non-enlarged cases. These cases were classified as uncertain by the labeler, and therefore the binary assignment of 0s and 1s in this setting fails to achieve optimal performance as there is insufficient information conveyed by these modifications.
In the detection of Atelectasis, the U-Ones approach performs the best, hinting that the uncertainty label for this observation is effectively utilized when treated as positive. We expect that phrases such as "possible atelectasis" or "may be atelectasis," were meant to describe the most likely findings in the image, rather than convey uncertainty, which supports the good performance of U-Ones on this pathology. We suspect a similar explanation for the high performance of U-Ones on Edema, where uncertain phrases like "possible mild pulmonary edema" in fact convey likely findings. In contrast, the U-Ones approach performs worst on the Consolidation label, whereas the U-Zeros approach performs the best. We also note that Atelectasis and Consolidation are often mentioned together in radiology reports. For example, the phrase "findings may represent atelectasis versus consolidation" is very common. In these cases, our labeler assigns uncertain for both observations, but we find that in the ground truth panel review that many of these sorts of uncertainty cases are often instead resolved as Atelectasis-positive and Consolidation-negative.

####  Test Results
We compare the performance of our final model to radiologists on a test set. We selected the final model based on the best performing ensemble on each competition task on the validation set: U-Ones for Atelectasis and Edema, U-MultiClass for Cardiomegaly and Pleural Effusion, and U-SelfTrained for Consolidation.