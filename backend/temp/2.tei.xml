<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/alramalho/grobid-0.6.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-02-21">21 Feb 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
						</author>
						<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-02-21">21 Feb 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1405.0312v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.1" ident="GROBID" when="2023-01-06T14:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
				<p>We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the primary goals of computer vision is the understanding of visual scenes. Scene understanding involves numerous tasks including recognizing what objects are present, localizing the objects in 2D and 3D, determining the objects' and scene's attributes, characterizing relationships between objects and providing a semantic description of the scene. The current object classification and detection datasets <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> help us explore the first challenges related to scene understanding. For instance the ImageNet dataset <ref type="bibr" target="#b0">[1]</ref>, which contains an unprecedented number of images, has recently enabled breakthroughs in both object classification and detection research <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The community has also created datasets containing object attributes <ref type="bibr" target="#b7">[8]</ref>, scene attributes <ref type="bibr" target="#b8">[9]</ref>, keypoints <ref type="bibr" target="#b9">[10]</ref>, and 3D scene information <ref type="bibr" target="#b10">[11]</ref>. This leads us to the obvious question: what datasets will best continue our advance towards our ultimate goal of scene understanding?</p><p>We introduce a new large-scale dataset that addresses three core research problems in scene understanding: detecting non-iconic views (or non-canonical perspectives <ref type="bibr" target="#b11">[12]</ref>) of objects, contextual reasoning between objects and the precise 2D localization of objects. For many categories of objects, there exists an iconic view. For example, when performing a web-based image search for the object category "bike," the top-ranked retrieved examples appear in profile, unobstructed near the center of a neatly composed photo. We posit that current recognition systems perform fairly well on iconic views, but struggle to recognize objects otherwise -in the  We introduce a large, richly-annotated dataset comprised of images depicting complex everyday scenes of common objects in their natural context. background, partially occluded, amid clutter <ref type="bibr" target="#b12">[13]</ref> -reflecting the composition of actual everyday scenes. We verify this experimentally; when evaluated on everyday scenes, models trained on our data perform better than those trained with prior datasets. A challenge is finding natural images that contain multiple objects. The identity of many objects can only be resolved using context, due to small size or ambiguous appearance in the image. To push research in contextual reasoning, images depicting scenes <ref type="bibr" target="#b2">[3]</ref> rather than objects in isolation are necessary. Finally, we argue that detailed spatial understanding of object layout will be a core component of scene analysis. An object's spatial location can be defined coarsely using a bounding box <ref type="bibr" target="#b1">[2]</ref> or with a precise pixel-level segmentation <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. As we demonstrate, to measure either kind of localization performance it is essential for the dataset to have every instance of every object category labeled and fully segmented. Our dataset is unique in its annotation of instance-level segmentation masks, <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>To create a large-scale dataset that accomplishes these three goals we employed a novel pipeline for gathering data with extensive use of Amazon Mechanical Turk. First and most importantly, we harvested a large set of images containing contextual relationships and noniconic object views. We accomplished this using a surprisingly simple yet effective technique that queries for pairs of objects in conjunction with images retrieved via scene-based queries <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Next, each image was labeled as containing particular object categories using a hierarchical labeling approach <ref type="bibr" target="#b17">[18]</ref>. For each category found, the individual instances were labeled, verified, and finally segmented. Given the inherent ambiguity of labeling, each of these stages has numerous tradeoffs that we explored in detail.</p><p>The Microsoft Common Objects in COntext (MS COCO) dataset contains 91 common object categories with 82 of them having more than 5,000 labeled instances, <ref type="figure" target="#fig_6">Fig. 6</ref>. In total the dataset has 2,500,000 labeled instances in 328,000 images. In contrast to the popular ImageNet dataset <ref type="bibr" target="#b0">[1]</ref>, COCO has fewer categories but more instances per category. This can aid in learning detailed object models capable of precise 2D localization. The dataset is also significantly larger in number of instances per category than the PASCAL VOC <ref type="bibr" target="#b1">[2]</ref> and SUN <ref type="bibr" target="#b2">[3]</ref> datasets. Additionally, a critical distinction between our dataset and others is the number of labeled instances per image which may aid in learning contextual information, <ref type="figure" target="#fig_5">Fig. 5</ref>. MS COCO contains considerably more object instances per image (7.7) as compared to ImageNet (3.0) and PASCAL <ref type="bibr">(2.3)</ref>. In contrast, the SUN dataset, which contains significant contextual information, has over 17 objects and "stuff" per image but considerably fewer object instances overall.</p><p>An abridged version of this work appeared in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Throughout the history of computer vision research datasets have played a critical role. They not only provide a means to train and evaluate algorithms, they drive research in new and more challenging directions. The creation of ground truth stereo and optical flow datasets <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> helped stimulate a flood of interest in these areas. The early evolution of object recognition datasets <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> facilitated the direct comparison of hundreds of image recognition algorithms while simultaneously pushing the field towards more complex problems. Recently, the ImageNet dataset <ref type="bibr" target="#b0">[1]</ref> containing millions of images has enabled breakthroughs in both object classification and detection research using a new class of deep learning algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Datasets related to object recognition can be roughly split into three groups: those that primarily address object classification, object detection and semantic scene labeling. We address each in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Classification</head><p>The task of object classification requires binary labels indicating whether objects are present in an image; see <ref type="figure" target="#fig_1">Fig. 1(a)</ref>. Early datasets of this type comprised images containing a single object with blank backgrounds, such as the MNIST handwritten digits <ref type="bibr" target="#b24">[25]</ref> or COIL household objects <ref type="bibr" target="#b25">[26]</ref>. Caltech 101 <ref type="bibr" target="#b21">[22]</ref> and Caltech 256 <ref type="bibr" target="#b22">[23]</ref> marked the transition to more realistic object images retrieved from the internet while also increasing the number of object categories to 101 and 256, respectively. Popular datasets in the machine learning community due to the larger number of training examples, CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b26">[27]</ref> offered 10 and 100 categories from a dataset of tiny 32 × 32 images <ref type="bibr" target="#b27">[28]</ref>. While these datasets contained up to 60,000 images and hundreds of categories, they still only captured a small fraction of our visual world.</p><p>Recently, ImageNet <ref type="bibr" target="#b0">[1]</ref> made a striking departure from the incremental increase in dataset sizes. They proposed the creation of a dataset containing 22k categories with 500-1000 images each. Unlike previous datasets containing entry-level categories <ref type="bibr" target="#b28">[29]</ref>, such as "dog" or "chair," like <ref type="bibr" target="#b27">[28]</ref>, ImageNet used the WordNet Hierarchy <ref type="bibr" target="#b29">[30]</ref> to obtain both entry-level and fine-grained <ref type="bibr" target="#b30">[31]</ref> categories. Currently, the ImageNet dataset contains over 14 million labeled images and has enabled significant advances in image classification <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p><p>Object detection Detecting an object entails both stating that an object belonging to a specified class is present, and localizing it in the image. The location of an object is typically represented by a bounding box, <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. Early algorithms focused on face detection <ref type="bibr" target="#b31">[32]</ref> using various ad hoc datasets. Later, more realistic and challenging face detection datasets were created <ref type="bibr" target="#b32">[33]</ref>. Another popular challenge is the detection of pedestrians for which several datasets have been created <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The Caltech Pedestrian Dataset <ref type="bibr" target="#b3">[4]</ref> contains 350,000 labeled instances with bounding boxes.</p><p>For the detection of basic object categories, a multiyear effort from 2005 to 2012 was devoted to the creation and maintenance of a series of benchmark datasets that were widely adopted. The PASCAL VOC <ref type="bibr" target="#b1">[2]</ref> datasets contained 20 object categories spread over 11,000 images. Over 27,000 object instance bounding boxes were labeled, of which almost 7,000 had detailed segmentations. Recently, a detection challenge has been created from 200 object categories using a subset of 400,000 images from ImageNet <ref type="bibr" target="#b33">[34]</ref>. An impressive 350,000 objects have been labeled using bounding boxes.</p><p>Since the detection of many objects such as sunglasses, cellphones or chairs is highly dependent on contextual information, it is important that detection datasets contain objects in their natural environments. In our dataset we strive to collect images rich in contextual information. The use of bounding boxes also limits the accuracy for which detection algorithms may be evaluated. We propose the use of fully segmented instances to enable more accurate detector evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic scene labeling</head><p>The task of labeling semantic objects in a scene requires that each pixel of an image be labeled as belonging to a category, such as sky, chair, floor, street, etc. In contrast to the detection task, individual instances of objects do not need to be segmented, <ref type="figure" target="#fig_1">Fig. 1(c)</ref>. This enables the labeling of objects for which individual instances are hard to define, such as grass, streets, or walls. Datasets exist for both indoor <ref type="bibr" target="#b10">[11]</ref> and outdoor <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b13">[14]</ref> scenes. Some datasets also include depth information <ref type="bibr" target="#b10">[11]</ref>. Similar to semantic scene labeling, our goal is to measure the pixel-wise accuracy of object labels. However, we also aim to distinguish between individual instances of an object, which requires a solid understanding of each object's extent.</p><p>A novel dataset that combines many of the properties of both object detection and semantic scene labeling datasets is the SUN dataset <ref type="bibr" target="#b2">[3]</ref> for scene understanding. SUN contains 908 scene categories from the WordNet dictionary <ref type="bibr" target="#b29">[30]</ref> with segmented objects. The 3,819 object categories span those common to object detection datasets (person, chair, car) and to semantic scene labeling (wall, sky, floor). Since the dataset was collected by finding images depicting various scene types, the number of instances per object category exhibits the long tail phenomenon. That is, a few categories have a large number of instances (wall: 20,213, window: 16,080, chair: 7,971) while most have a relatively modest number of instances (boat: 349, airplane: 179, floor lamp: 276). In our dataset, we ensure that each object category has a significant number of instances, <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>Other vision datasets Datasets have spurred the advancement of numerous fields in computer vision. Some notable datasets include the Middlebury datasets for stereo vision <ref type="bibr" target="#b19">[20]</ref>, multi-view stereo <ref type="bibr" target="#b35">[36]</ref> and optical flow <ref type="bibr" target="#b20">[21]</ref>. The Berkeley Segmentation Data Set (BSDS500) <ref type="bibr" target="#b36">[37]</ref> has been used extensively to evaluate both segmentation and edge detection algorithms. Datasets have also been created to recognize both scene <ref type="bibr" target="#b8">[9]</ref> and object attributes <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Indeed, numerous areas of vision have benefited from challenging datasets that helped catalyze progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMAGE COLLECTION</head><p>We next describe how the object categories and candidate images are selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Common Object Categories</head><p>The selection of object categories is a non-trivial exercise. The categories must form a representative set of all categories, be relevant to practical applications and occur with high enough frequency to enable the collection of a large dataset. Other important decisions are whether to include both "thing" and "stuff" categories <ref type="bibr" target="#b38">[39]</ref> and whether fine-grained <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b0">[1]</ref> and object-part categories should be included. "Thing" categories include objects for which individual instances may be easily labeled (person, chair, car) where "stuff" categories include materials and objects with no clear boundaries (sky, street, grass). Since we are primarily interested in precise localization of object instances, we decided to only include "thing" categories and not "stuff." However, since "stuff" categories can provide significant contextual information, we believe the future labeling of "stuff" categories would be beneficial.</p><p>The specificity of object categories can vary significantly. For instance, a dog could be a member of the "mammal", "dog", or "German shepherd" categories. To enable the practical collection of a significant number of instances per category, we chose to limit our dataset to entry-level categories, i.e. category labels that are commonly used by humans when describing objects (dog, chair, person). It is also possible that some object categories may be parts of other object categories. For instance, a face may be part of a person. We anticipate the inclusion of object-part categories (face, hands, wheels) would be beneficial for many real-world applications.</p><p>We used several sources to collect entry-level object categories of "things." We first compiled a list of categories by combining categories from PASCAL VOC <ref type="bibr" target="#b1">[2]</ref> and a subset of the 1200 most frequently used words that denote visually identifiable objects <ref type="bibr" target="#b39">[40]</ref>. To further augment our set of candidate categories, several children ranging in ages from 4 to 8 were asked to name every object they see in indoor and outdoor environments. The final 272 candidates may be found in the appendix. Finally, the co-authors voted on a 1 to scale for each category taking into account how commonly they occur, their usefulness for practical applications, and their diversity relative to other categories. The final selection of categories attempts to pick categories with high votes, while keeping the number of categories per supercategory (animals, vehicles, furniture, etc.) balanced. Categories for which obtaining a large number of instances (greater than 5,000) was difficult were also removed. To ensure backwards compatibility all categories from PASCAL VOC <ref type="bibr" target="#b1">[2]</ref> are also included. Our final list of 91 proposed categories is in <ref type="figure" target="#fig_5">Fig. 5</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-iconic Image Collection</head><p>Given the list of object categories, our next goal was to collect a set of candidate images. We may roughly group images into three types, <ref type="figure" target="#fig_2">Fig. 2</ref>: iconic-object images <ref type="bibr" target="#b40">[41]</ref>, iconic-scene images <ref type="bibr" target="#b2">[3]</ref> and non-iconic images. Typical iconic-object images have a single large object in a canonical perspective centered in the image, <ref type="figure" target="#fig_2">Fig. 2(a)</ref>. Iconic-scene images are shot from canonical viewpoints and commonly lack people, <ref type="figure" target="#fig_2">Fig. 2(b)</ref>. Iconic images have the benefit that they may be easily found by directly searching for specific categories using Google or Bing image search. While iconic images generally provide high quality object instances, they can lack important contextual information and non-canonical viewpoints.</p><p>Our goal was to collect a dataset such that a majority of images are non-iconic, <ref type="figure" target="#fig_2">Fig. 2</ref>(c). It has been shown that datasets containing more non-iconic images are better at generalizing <ref type="bibr" target="#b41">[42]</ref>. We collected non-iconic images using two strategies. First as popularized by PASCAL VOC <ref type="bibr" target="#b1">[2]</ref>, we collected images from Flickr which tends to have fewer iconic images. Flickr contains photos uploaded by amateur photographers with searchable metadata and keywords. Second, we did not search for object categories in isolation. A search for "dog" will tend to return iconic images of large, centered dogs. However, if we searched for pairwise combinations of object categories, such as "dog + car" we found many more non-iconic images. Surprisingly, these images typically do not just contain the two categories specified in the search, but numerous other categories as well. To further supplement our dataset we also searched for scene/object category pairs, see the appendix. We downloaded at most 5 photos taken by a single photographer within a short time window. In the rare cases in which enough images could not be found, we searched for single categories and performed an explicit filtering stage to remove iconic images. The result is a collection of 328,000 images with rich contextual relationships between objects as shown in Figs. 2(c) and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMAGE ANNOTATION</head><p>We next describe how we annotated our image collection. Due to our desire to label over 2.5 million object instances, the design of a cost efficient yet high quality annotation pipeline was critical. The annotation pipeline is outlined in <ref type="figure" target="#fig_3">Fig. 3</ref>. For all crowdsourcing tasks we used workers on Amazon's Mechanical Turk (AMT). Our user interfaces are described in detail in the appendix. Note that, since the original version of this work <ref type="bibr" target="#b18">[19]</ref>, we have taken a number of steps to further improve the quality of the annotations. In particular, we have increased the number of annotators for the category labeling and instance spotting stages to eight. We also added a stage to verify the instance segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Category Labeling</head><p>The first task in annotating our dataset is determining which object categories are present in each image, <ref type="figure" target="#fig_3">Fig. 3(a)</ref>. Since we have 91 categories and a large number of images, asking workers to answer 91 binary classification questions per image would be prohibitively expensive. Instead, we used a hierarchical approach <ref type="bibr" target="#b17">[18]</ref>. We group the object categories into 11 super-categories (see the appendix). For a given image, a worker was presented with each group of categories in turn and asked to indicate whether any instances exist for that super-category. This greatly reduces the time needed to classify the various categories. For example, a worker may easily determine no animals are present in the image without having to specifically look for cats, dogs, etc. If a worker determines instances from the super-category (animal) are present, for each subordinate category (dog, cat, etc.) present, the worker must drag the category's icon onto the image over one instance of the category. The placement of these icons is critical for the following stage. We emphasize that only a single instance of each category needs to be annotated in this stage. To ensure high recall, 8 workers were asked to label each image. A category is considered present if any worker indicated the category; false positives are handled in subsequent stages. A detailed analysis of performance is presented in §4.4. This stage took ∼20k worker hours to complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instance Spotting</head><p>In the next stage all instances of the object categories in an image were labeled, <ref type="figure" target="#fig_3">Fig. 3(b)</ref>. In the previous stage each worker labeled one instance of a category, but multiple object instances may exist. Therefore, for each image, a worker was asked to place a cross on top of each instance of a specific category found in the previous stage. To boost recall, the location of the instance found by a worker in the previous stage was shown to the current worker. Such priming helped workers quickly find an initial instance upon first seeing the image. The workers could also use a magnifying glass to find small instances. Each worker was asked to label at most 10 instances of a given category per image. Each image was labeled by 8 workers for a total of ∼10k worker hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Instance Segmentation</head><p>Our final stage is the laborious task of segmenting each object instance, <ref type="figure" target="#fig_3">Fig. 3(c)</ref>. For this stage we modified the excellent user interface developed by Bell et al. <ref type="bibr" target="#b15">[16]</ref> for image segmentation. Our interface asks the worker to segment an object instance specified by a worker in the previous stage. If other instances have already been segmented in the image, those segmentations are shown to the worker. A worker may also indicate there are no object instances of the given category in the image (implying a false positive label from the previous stage) or that all object instances are already segmented.</p><p>Segmenting 2,500,000 object instances is an extremely time consuming task requiring over 22 worker hours per 1,000 segmentations. To minimize cost we only had a single worker segment each instance. However, when first completing the task, most workers produced only coarse instance outlines. As a consequence, we required all workers to complete a training task for each object category. The training task required workers to segment an object instance. Workers could not complete the task until their segmentation adequately matched the ground truth. The use of a training task vastly improved the quality of the workers (approximately 1 in 3 workers passed the training stage) and resulting segmentations. Example segmentations may be viewed in <ref type="figure" target="#fig_6">Fig. 6</ref>.</p><p>While the training task filtered out most bad workers, we also performed an explicit verification step on each segmented instance to ensure good quality. Multiple workers (3 to 5) were asked to judge each segmentation and indicate whether it matched the instance well or not. Segmentations of insufficient quality were discarded and the corresponding instances added back to the pool of unsegmented objects. Finally, some approved workers consistently produced poor segmentations; all work obtained from such workers was discarded.</p><p>For images containing 10 object instances or fewer of a given category, every instance was individually segmented (note that in some images up to 15 instances were segmented). Occasionally the number of instances is drastically higher; for example, consider a dense crowd of people or a truckload of bananas. In such cases, many instances of the same category may be tightly grouped together and distinguishing individual instances is difficult. After 10-15 instances of a category were segmented in an image, the remaining instances were marked as "crowds" using a single (possibly multipart) segment. For the purpose of evaluation, areas marked as crowds will be ignored and not affect a detector's score. Details are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Annotation Performance Analysis</head><p>We analyzed crowd worker quality on the category labeling task by comparing to dedicated expert workers, see <ref type="figure" target="#fig_4">Fig. 4(a)</ref>. We compared precision and recall of seven expert workers (co-authors of the paper) with the results obtained by taking the union of one to ten AMT workers. Ground truth was computed using majority vote of the experts. For this task recall is of primary importance as false positives could be removed in later stages. <ref type="figure" target="#fig_4">Fig. 4(a)</ref> shows that the union of 8 AMT workers, the same number as was used to collect our labels, achieved greater recall than any of the expert workers. Note that worker recall saturates at around 9-10 AMT workers.</p><p>Object category presence is often ambiguous. Indeed as <ref type="figure" target="#fig_4">Fig. 4(a)</ref> indicates, even dedicated experts often disagree on object presence, e.g. due to inherent ambiguity in the image or disagreement about category definitions. For any unambiguous examples having a probability of over 50% of being annotated, the probability all 8 annotators missing such a case is at most .5 8 ≈ .004. Additionally, by observing how recall increased as we added annotators, we estimate that in practice over 99% of all object categories not later rejected as false positives are detected given 8 annotators. Note that a similar analysis may be done for instance spotting in which 8 annotators were also used.</p><p>Finally, <ref type="figure" target="#fig_4">Fig. 4(b)</ref> re-examines precision and recall of AMT workers on category labeling on a much larger set of images. The number of workers (circle size) and average number of jobs per worker (circle color) is shown for each precision/recall range. Unlike in <ref type="figure" target="#fig_4">Fig. 4(a)</ref>, we used a leave-one-out evaluation procedure where a category was considered present if any of the remaining workers named the category. Therefore, overall worker precision is substantially higher. Workers who completed the most jobs also have the highest precision; all jobs from workers below the black line were rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Caption Annotation</head><p>We added five written caption descriptions to each image in MS COCO. A full description of the caption statistics and how they were gathered will be provided shortly in a separate publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DATASET STATISTICS</head><p>Next, we analyze the properties of the Microsoft Common Objects in COntext (MS COCO) dataset in comparison to several other popular datasets. These include ImageNet <ref type="bibr" target="#b0">[1]</ref>, PASCAL VOC 2012 <ref type="bibr" target="#b1">[2]</ref>, and SUN <ref type="bibr" target="#b2">[3]</ref>. Each of these datasets varies significantly in size, list of labeled categories and types of images. ImageNet was created to capture a large number of object categories, many of which are fine-grained. SUN focuses on labeling scene types and the objects that commonly occur in them. Finally, PASCAL VOC's primary application is object detection in natural images. MS COCO is designed for the detection and segmentation of objects occurring in their natural context.</p><p>The number of instances per category for all 91 categories is shown in <ref type="figure" target="#fig_5">Fig. 5(a)</ref>. A summary of the datasets showing the number of object categories and the number of instances per category is shown in <ref type="figure" target="#fig_5">Fig. 5(d)</ref>. While MS COCO has fewer categories than ImageNet and SUN, it has more instances per category which we hypothesize will be useful for learning complex models capable of precise localization. In comparison to PASCAL VOC, MS COCO has both more categories and instances.</p><p>An important property of our dataset is we strive to find non-iconic images containing objects in their natural context. The amount of contextual information present in an image can be estimated by examining the average number of object categories and instances per image, <ref type="figure" target="#fig_5">Fig. 5(b, c)</ref>. For ImageNet we plot the object detection validation set, since the training data only has a single object labeled. On average our dataset contains 3.5 categories and 7.7 instances per image. In comparison ImageNet and PASCAL VOC both have less than 2 categories and 3 instances per image on average. Another interesting observation is only 10% of the images in MS COCO have only one category per image, in comparison, over 60% of images contain a single object category in ImageNet and PASCAL VOC. As expected, the SUN dataset has the most contextual information since it is scene-based and uses an unrestricted set of categories.</p><p>Finally, we analyze the average size of objects in the datasets. Generally smaller objects are harder to recognize and require more contextual reasoning to recognize. As shown in <ref type="figure" target="#fig_5">Fig. 5(e)</ref>, the average sizes of objects is smaller for both MS COCO and SUN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DATASET SPLITS</head><p>To accommodate a faster release schedule, we split the MS COCO dataset into two roughly equal parts. The first half of the dataset was released in 2014, the second half will be released in 2015. The 2014 release contains 82,783 training, 40,504 validation, and 40,775 testing images (approximately <ref type="bibr" target="#b0">1</ref> train, <ref type="bibr" target="#b0">1</ref> val, and 1 test). There are nearly 270k segmented people and a total of 886k segmented object instances in the 2014 train+val data alone. The cumulative 2015 release will contain a total of 165,482 train, 81,208 val, and 81,434 test images. We took care to minimize the chance of near-duplicate images existing across splits by explicitly removing near duplicates (detected with <ref type="bibr" target="#b42">[43]</ref>) and grouping images by photographer and date taken.</p><p>Following established protocol, annotations for train and validation data will be released, but not for test. We are currently finalizing the evaluation server for automatic evaluation on the test set. A full discussion of evaluation metrics will be added once the evaluation server is complete.</p><p>Note that we have limited the 2014 release to a subset of 80 categories. We did not collect segmentations for the following 11 categories: hat, shoe, eyeglasses (too many instances), mirror, window, door, street sign (ambiguous and difficult to label), plate, desk (due to confusion with bowl and dining table, respectively) and blender, hair brush (too few instances). We may add segmentations for some of these categories in the cumulative 2015 release. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ALGORITHMIC ANALYSIS</head><p>Bounding-box detection For the following experiments we take a subset of 55,000 images from our dataset 1 and obtain tight-fitting bounding boxes from the annotated segmentation masks. We evaluate models tested on both MS COCO and PASCAL, see <ref type="table" target="#tab_0">Table 1</ref>. We evaluate two different models. DPMv5-P: the latest implementation 1. These preliminary experiments were performed before our final split of the dataset intro train, val, and test. Baselines on the actual test set will be added once the evaluation server is complete.</p><p>of <ref type="bibr" target="#b43">[44]</ref> (release 5 <ref type="bibr" target="#b44">[45]</ref>) trained on PASCAL VOC 2012. DPMv5-C: the same implementation trained on COCO (5000 positive and 10000 negative images). We use the default parameter settings for training COCO models.</p><p>If we compare the average performance of DPMv5-P on PASCAL VOC and MS COCO, we find that average performance on MS COCO drops by nearly a factor of 2, suggesting that MS COCO does include more difficult (non-iconic) images of objects that are partially occluded, amid clutter, etc. We notice a similar drop in performance plane bike bird boat bottle bus car cat chair cow table dog horse moto person plant sheep sofa train tv avg.   <ref type="figure">C)</ref>. For DPMv5-C we used 5000 positive and 10000 negative training examples. While MS COCO is considerably more challenging than PASCAL, use of more training data coupled with more sophisticated approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> should improve performance substantially.</p><p>for the model trained on MS COCO (DPMv5-C).</p><p>The effect on detection performance of training on PASCAL VOC or MS COCO may be analyzed by comparing DPMv5-P and DPMv5-C. They use the same implementation with different sources of training data. <ref type="table" target="#tab_0">Table 1</ref> shows DPMv5-C still outperforms DPMv5-P in 6 out of 20 categories when testing on PASCAL VOC. In some categories (e.g., dog, cat, people), models trained on MS COCO perform worse, while on others (e.g., bus, tv, horse), models trained on our data are better.</p><p>Consistent with past observations <ref type="bibr" target="#b45">[46]</ref>, we find that including difficult (non-iconic) images during training may not always help. Such examples may act as noise and pollute the learned model if the model is not rich enough to capture such appearance variability. Our dataset allows for the exploration of such issues.</p><p>Torralba and Efros <ref type="bibr" target="#b41">[42]</ref> proposed a metric to measure cross-dataset generalization which computes the 'performance drop' for models that train on one dataset and test on another. The performance difference of the DPMv5-P models across the two datasets is 12.7 AP while the DPMv5-C models only have 7.7 AP difference. Moreover, overall performance is much lower on MS COCO. These observations support two hypotheses: 1) MS COCO is significantly more difficult than PASCAL VOC and 2) models trained on MS COCO can generalize better to easier datasets such as PASCAL VOC given more training data. To gain insight into the differences between the datasets, see the appendix for visualizations of person and chair examples from the two datasets.</p><p>Generating segmentations from detections We now describe a simple method for generating object bounding boxes and segmentation masks, following prior work that produces segmentations from object detections <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. We learn aspect-specific pixel-level segmentation masks for different categories. These are readily learned by averaging together segmentation masks from aligned training instances. We learn different masks corresponding to the different mixtures in our DPM detector. Sample masks are visualized in <ref type="figure">Fig. 7</ref>.</p><p>Detection evaluated by segmentation Segmentation is a challenging task even assuming a detector reports correct results as it requires fine localization of object part <ref type="figure">Fig. 7</ref>: We visualize our mixture-specific shape masks. We paste thresholded shape masks on each candidate detection to generate candidate segments. <ref type="figure">Fig. 8</ref>: Evaluating instance detections with segmentation masks versus bounding boxes. Bounding boxes are a particularly crude approximation for articulated objects; in this case, the majority of the pixels in the (blue) tightfitting bounding-box do not lie on the object. Our (green) instance-level segmentation masks allows for a more accurate measure of object detection and localization.</p><p>boundaries. To decouple segmentation evaluation from detection correctness, we benchmark segmentation quality using only correct detections. Specifically, given that the detector reports a correct bounding box, how well does the predicted segmentation of that object match the ground truth segmentation? As criterion for correct detection, we impose the standard requirement that intersection over union between predicted and ground truth boxes is at least 0.5. We then measure the intersection over union of the predicted and ground truth segmentation masks, see <ref type="figure">Fig. 8</ref>. To establish a baseline for our dataset, we project learned DPM part masks onto the image to create segmentation masks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION</head><p>We introduced a new dataset for detecting and segmenting objects found in everyday life in their natural environments. Utilizing over 70,000 worker hours, a vast collection of object instances was gathered, annotated and organized to drive the advancement of object detection and segmentation algorithms. Emphasis was placed on finding non-iconic images of objects in natural environments and varied viewpoints. Dataset statistics indicate the images contain rich contextual information with many objects present per image. There are several promising directions for future annotations on our dataset. We currently only label "things", but labeling "stuff" may also provide significant contextual information that may be useful for detection. Many object detection algorithms benefit from additional annotations, such as the amount an instance is occluded <ref type="bibr" target="#b3">[4]</ref> or the location of keypoints on the object <ref type="bibr" target="#b9">[10]</ref>. Finally, our dataset could provide a good benchmark for other types of labels, including scene types <ref type="bibr" target="#b2">[3]</ref>, attributes <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b7">[8]</ref> and full sentence written descriptions <ref type="bibr" target="#b50">[51]</ref>. We are actively exploring adding various such annotations.</p><p>To download and learn more about MS COCO please see the project website <ref type="bibr" target="#b1">2</ref> . MS COCO will evolve and grow over time; up to date information is available online.</p><p>Acknowledgments Funding for all crowd worker tasks was provided by Microsoft. P.P. and D.R. were supported by ONR MURI Grant N00014-10-1-0933. We would like to thank all members of the community who provided valuable feedback throughout the process of defining and collecting the dataset.</p><p>2. http://mscoco.org/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX OVERVIEW</head><p>In the appendix, we provide detailed descriptions of the AMT user interfaces and the full list of 272 candidate categories (from which our final 91 were selected) and 40 scene categories (used for scene-object queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I: USER INTERFACES</head><p>We describe and visualize our user interfaces for collecting non-iconic images, category labeling, instance spotting, instance segmentation, segmentation verification and finally crowd labeling.</p><p>Non-iconic Image Collection Flickr provides a rich image collection associated with text captions. However, captions might be inaccurate and images may be iconic. To construct a high-quality set of non-iconic images, we first collected candidate images by searching for pairs of object categories, or pairs of object and scene categories. We then created an AMT filtering task that allowed users to remove invalid or iconic images from a grid of 128 candidates, <ref type="figure" target="#fig_1">Fig. 10</ref>. We found the choice of instructions to be crucial, and so provided users with examples of iconic and non-iconic images. Some categories rarely co-occurred with others. In such cases, we collected candidates using only the object category as the search term, but apply a similar filtering step, <ref type="figure" target="#fig_1">Fig. 10(b)</ref>.</p><p>Category Labeling <ref type="figure" target="#fig_1">Fig. 12(a)</ref> shows our interface for category labeling. We designed the labeling task to encourage workers to annotate all categories present in the image. Workers annotate categories by dragging and dropping icons from the bottom category panel onto a corresponding object instance. Only a single instance of each object category needs to be annotated in the image. We group icons by the super-categories from <ref type="figure" target="#fig_1">Fig. 11</ref>, allowing workers to quickly skip categories that are unlikely to be present.  <ref type="figure" target="#fig_1">Fig. 12</ref>(b) depicts our interface for labeling all instances of a given category. The interface is initialized with a blinking icon specifying a single instance obtained from the previous category-labeling stage. Workers are then asked to spot and click on up to 10 total instances of the given category, placing a single cross anywhere within the region of each instance. In order to spot small objects, we found it crucial to include a "magnifying glass" feature that doubles the resolution of a worker's currently selected region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instance Spotting</head><p>Instance Segmentation <ref type="figure" target="#fig_1">Fig. 12(c)</ref> shows our user interface for instance segmentation. We modified source code from the OpenSurfaces project <ref type="bibr" target="#b15">[16]</ref>, which defines a single AMT task for segmenting multiple regions of a homogenous material in real-scenes. In our case, we define a single task for segmenting a single object instance labeled from the previous annotation stage. To aid the segmentation process, we added a visualization of the object category icon to remind workers of the category to be segmented. Crucially, we also added zoom-in functionality to allow for efficient annotation of small objects and curved boundaries. In the previous annotation stage, to ensure high coverage of all object instances, we used multiple workers to label all instances per image. We would like to segment all such object instances, but instance annotations across different workers may refer to different or redundant instances. To resolve this correspondence ambiguity, we sequentially post AMT segmentation tasks, ignoring instance annotations that are already covered by an existing segmentation mask.</p><p>Segmentation Verification <ref type="figure" target="#fig_1">Fig. 12(d)</ref> shows our user interface for segmentation verification. Due to the time consuming nature of the previous task, each object instance is segmented only once. The purpose of the veri-fication stage is therefore to ensure that each segmented instance from the previous stage is of sufficiently high quality. Workers are shown a grid of 64 segmentations and asked to select poor quality segmentations. Four of the 64 segmentation are known to be bad; a worker must identify 3 of the 4 known bad segmentations to complete the task. Each segmentation is initially shown to 3 annotators. If any of the annotators indicates the segmentation is bad, it is shown to 2 additional workers. At this point, any segmentation that doesn't receive at least 4 of 5 favorable votes is discarded and the corresponding instance added back to the pool of unsegmented objects. Examples of borderline cases that either passed (4/5 votes) or were rejected (3/5 votes) are shown in <ref type="figure" target="#fig_1">Fig. 15</ref>.</p><p>Crowd Labeling <ref type="figure" target="#fig_1">Fig. 12</ref>(e) shows our user interface for crowd labeling. As discussed, for images containing ten object instances or fewer of a given category, every object instance was individually segmented. In some images, however, the number of instances of a given category is much higher. In such cases crowd labeling provided a more efficient method for annotation. Rather than requiring workers to draw exact polygonal masks around each object instance, we allow workers to "paint" all pixels belonging to the category in question. Crowd labeling is similar to semantic segmentation as object instance are not individually identified. We emphasize that crowd labeling is only necessary for images containing more than ten object instances of a given category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II: OBJECT &amp; SCENE CATEGORIES</head><p>Our dataset contains 91 object categories (the 2014 release contains segmentation masks for 80 of these categories). We began with a list of frequent object categories taken from WordNet, LabelMe, SUN and other sources as well as categories derived from a free recall experiment with young children. The authors then voted on the resulting 272 categories with the aim of sampling a diverse and computationally challenging set of categories; see §3 for details. The list in <ref type="table" target="#tab_2">Table 2</ref> enumerates those 272 categories in descending order of votes. As discussed, the final selection of 91 categories attempts to pick categories with high votes, while keeping the number of categories per super-category (animals, vehicles, furniture, etc.) balanced.</p><p>As discussed in §3, in addition to using object-object queries to gather non-iconic images, object-scene queries also proved effective. For this task we selected a subset of scene categories from the SUN dataset that frequently co-occurred with object categories of interest. Table enumerates the 40 scene categories (evenly split between indoor and outdoor scenes). <ref type="figure" target="#fig_1">Fig. 11</ref>: Icons of 91 categories in the MS COCO dataset grouped by 11 super-categories. We use these icons in our annotation pipeline to help workers quickly reference the indicated object category. <ref type="figure" target="#fig_1">Fig. 12</ref>: User interfaces for collecting instance annotations, see text for details.        </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>T.Y. Lin and S. Belongie are with Cornell NYC Tech and the Cornell Computer Science Department. • M. Maire is with the Toyota Technological Institute at Chicago. • L. Bourdev and P. Dollár are with Facebook AI Research. The majority of this work was performed while P. Dollár was with Microsoft Research. • R. Girshick and C. L. Zitnick are with Microsoft Research, Redmond. • J. Hays is with Brown University. • P. Perona is with the California Institute of Technology. • D. Ramanan is with the University of California at Irvine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>While previous object recognition datasets have focused on (a) image classification, (b) object bounding box localization or (c) semantic pixel-level segmentation, we focus on (d) segmenting individual object instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Example of (a) iconic object images, (b) iconic scene images, and (c) non-iconic images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Our annotation pipeline is split into 3 primary tasks: (a) labeling the categories present in the image ( §4.1), (b) locating and marking all instances of the labeled categories ( §4.2), and (c) segmenting each object instance ( §4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Worker precision and recall for the category labeling task. (a) The union of multiple AMT workers (blue) has better recall than any expert (red). Ground truth was computed using majority vote of the experts. (b) Shows the number of workers (circle size) and average number of jobs per worker (circle color) for each precision/recall range. Most workers have high precision; such workers generally also complete more jobs. For this plot ground truth for each worker is the union of responses from all other AMT workers. See §4.4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Number of annotated instances per category for MS COCO and PASCAL VOC. (b,c) Number of annotated categories and annotated instances, respectively, per image for MS COCO, ImageNet Detection, PASCAL VOC and SUN (average number of categories and instances are shown in parentheses). (d) Number of categories vs. the number of instances per category for a number of popular object recognition datasets. (e) The distribution of instance sizes for the MS COCO, ImageNet Detection, PASCAL VOC and SUN datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Samples of annotated images in the MS COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>DPMv5</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig.shows results of this segmentation baseline for the DPM learned on the 20 PASCAL categories and tested on our dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>y c le b ir d b o a t b o t t le b u s c a r c a t c h a ir c o w d in in g t a b le d o g h o r s e m o t o r b ik e p e r s o n p o t t e d p la n t s h e e p s o f a t r a in t v m o n it o rFig. 9 :</head><label>9</label><figDesc>A predicted segmentation might not recover object detail even though detection and ground truth bounding boxes overlap well (left). Sampling from the person category illustrates that predicting segmentations from topdown projection of DPM part masks is difficult even for correct detections (center). Average segmentation overlap measured on MS COCO for the 20 PASCAL VOC categories demonstrates the difficulty of the problem (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>User interfaces for non-iconic image collection. (a) Interface for selecting non-iconic images containing pairs of objects. (b) Interface for selecting non-iconic images for categories that rarely co-occurred with others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) PASCAL VOC. (b) MS COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 :</head><label>14</label><figDesc>Random chair instances from PASCAL VOC and MS COCO. At most one instance is sampled per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig</head><label></label><figDesc>Fig. 15: Examples of borderline segmentations that passed (top) or were rejected (bottom) in the verification stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Top: Detection performance evaluated on PASCAL VOC 2012. DPMv5-P is the performance reported by Girshick et al. in VOC release 5. DPMv5-C uses the same implementation, but is trained with MS COCO.</figDesc><table><row><cell>Bottom: Performance evaluated on MS COCO for DPM models trained with PASCAL VOC 2012 (DPMv5-P) and</cell></row><row><cell>MS COCO (DPMv5-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig. 13: Random person instances from PASCAL VOC and MS COCO. At most one instance is sampled per image.</figDesc><table><row><cell>person chair fridge banana face eye street sign  *  headlights</cell><cell>bicycle couch microwave bread hand mouth umbrella window  *</cell><cell>car potted plant sink toilet apple scissors door  *  desk  *</cell><cell>motorcycle tv oven book keyboard truck fire hydrant computer</cell><cell>bird cow toaster boat backpack traffic light bowl refrigerator</cell><cell>cat airplane bus plate  *  steering wheel eyeglasses  *  teapot pizza</cell><cell>dog hat  *  train cell phone wine glass cup fork squirrel</cell><cell>horse license plate mirror  *  mouse chicken blender  *  knife duck</cell><cell>sheep bed dining table remote zebra hair drier spoon frisbee</cell><cell>bottle laptop elephant clock shoe  *  wheel bear guitar</cell></row><row><cell>nose</cell><cell>teddy bear</cell><cell>tie</cell><cell>stop sign</cell><cell>surfboard</cell><cell>sandwich</cell><cell>pen/pencil</cell><cell>kite</cell><cell>orange</cell><cell>toothbrush</cell></row><row><cell>printer</cell><cell>pans</cell><cell>head</cell><cell>sports ball</cell><cell>broccoli</cell><cell>suitcase</cell><cell>carrot</cell><cell>chandelier</cell><cell>parking meter</cell><cell>fish</cell></row><row><cell>handbag</cell><cell>hot dog</cell><cell>stapler</cell><cell>basketball hoop</cell><cell>donut</cell><cell>vase</cell><cell>baseball bat</cell><cell>baseball glove</cell><cell>giraffe</cell><cell>jacket</cell></row><row><cell>skis</cell><cell>snowboard</cell><cell>table lamp</cell><cell>egg</cell><cell>door handle</cell><cell>power outlet</cell><cell>hair</cell><cell>tiger</cell><cell>table</cell><cell>coffee table</cell></row><row><cell>skateboard chopping board</cell><cell>helicopter washer</cell><cell>tomato lion</cell><cell>tree monkey</cell><cell>bunny hair brush  *</cell><cell>pillow light switch</cell><cell>tennis racket arms</cell><cell>cake legs</cell><cell>feet house</cell><cell>bench cheese</cell></row><row><cell>goat</cell><cell>magazine</cell><cell>key</cell><cell>picture frame</cell><cell>cupcake</cell><cell>fan (ceil/floor)</cell><cell>frogs</cell><cell>rabbit</cell><cell>owl</cell><cell>scarf</cell></row><row><cell>ears</cell><cell>home phone</cell><cell>pig</cell><cell>strawberries</cell><cell>pumpkin</cell><cell>van</cell><cell>kangaroo</cell><cell>rhinoceros</cell><cell>sailboat</cell><cell>deer</cell></row><row><cell>playing cards</cell><cell>towel</cell><cell>hyppo</cell><cell>can</cell><cell>dollar bill</cell><cell>doll</cell><cell>soup</cell><cell>meat</cell><cell>window</cell><cell>muffins</cell></row><row><cell>tire</cell><cell>necklace</cell><cell>tablet</cell><cell>corn</cell><cell>ladder</cell><cell>pineapple</cell><cell>candle</cell><cell>desktop</cell><cell>carpet</cell><cell>cookie</cell></row><row><cell>toy cars</cell><cell>bracelet</cell><cell>bat</cell><cell>balloon</cell><cell>gloves</cell><cell>milk</cell><cell>pants</cell><cell>wheelchair</cell><cell>building</cell><cell>bacon</cell></row><row><cell>box</cell><cell>platypus</cell><cell>pancake</cell><cell>cabinet</cell><cell>whale</cell><cell>dryer</cell><cell>torso</cell><cell>lizard</cell><cell>shirt</cell><cell>shorts</cell></row><row><cell>pasta</cell><cell>grapes</cell><cell>shark</cell><cell>swan</cell><cell>fingers</cell><cell>towel</cell><cell>side table</cell><cell>gate</cell><cell>beans</cell><cell>flip flops</cell></row><row><cell>moon</cell><cell>road/street</cell><cell>fountain</cell><cell>fax machine</cell><cell>bat</cell><cell>hot air balloon</cell><cell>cereal</cell><cell>seahorse</cell><cell>rocket</cell><cell>cabinets</cell></row><row><cell>basketball</cell><cell>telephone</cell><cell>movie (disc)</cell><cell>football</cell><cell>goose</cell><cell>long sleeve shirt</cell><cell>short sleeve shirt</cell><cell>raft</cell><cell>rooster</cell><cell>copier</cell></row><row><cell>radio</cell><cell>fences</cell><cell>goal net</cell><cell>toys</cell><cell>engine</cell><cell>soccer ball</cell><cell>field goal posts</cell><cell>socks</cell><cell>tennis net</cell><cell>seats</cell></row><row><cell>elbows</cell><cell>aardvark</cell><cell>dinosaur</cell><cell>unicycle</cell><cell>honey</cell><cell>legos</cell><cell>fly</cell><cell>roof</cell><cell>baseball</cell><cell>mat</cell></row><row><cell>ipad</cell><cell>iphone</cell><cell>hoop</cell><cell>hen</cell><cell>back</cell><cell>table cloth</cell><cell>soccer nets</cell><cell>turkey</cell><cell>pajamas</cell><cell>underpants</cell></row><row><cell>goldfish</cell><cell>robot</cell><cell>crusher</cell><cell>animal crackers</cell><cell>basketball court</cell><cell>horn</cell><cell>firefly</cell><cell>armpits</cell><cell>nectar</cell><cell>super hero costume</cell></row><row><cell>jetpack</cell><cell>robots</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table /><note>Candidate category list (272). Bold: selected categories (91). Bold* : omitted categories in 2014 release (11).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.<ref type="bibr" target="#b14">15</ref>: Examples of borderline segmentations that passed (top) or were rejected (bottom) in the verification stage.</figDesc><table><row><cell>library</cell><cell>church</cell><cell>office</cell><cell>restaurant</cell><cell cols="2">kitchen living room</cell><cell>bathroom</cell><cell>factory</cell><cell>campus</cell><cell>bedroom</cell></row><row><cell cols="2">child's room dining room</cell><cell>auditorium</cell><cell>shop</cell><cell>home</cell><cell>hotel</cell><cell cols="3">classroom cafeteria hospital room</cell><cell>food court</cell></row><row><cell>street</cell><cell>park</cell><cell>beach</cell><cell>river</cell><cell>village</cell><cell>valley</cell><cell>market</cell><cell>harbor</cell><cell>yard</cell><cell>parking lot</cell></row><row><cell>lighthouse</cell><cell>railway</cell><cell cols="2">playground swimming pool</cell><cell>forest</cell><cell>gas station</cell><cell>garden</cell><cell>farm</cell><cell>mountain</cell><cell>plaza</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 :</head><label>3</label><figDesc></figDesc><table /><note>Scene category list.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-ageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, &quot;Im- ageNet: A Large-Scale Hierarchical Image Database,&quot; in CVPR, 2009.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zis- serman, &quot;The PASCAL visual object classes (VOC) challenge,&quot; IJCV, vol. 88, no. 2, pp. 303-338, Jun. 2010.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, &quot;SUN database: Large-scale scene recognition from abbey to zoo,&quot; in CVPR, 2010.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Dollár, C. Wojek, B. Schiele, and P. Perona, &quot;Pedestrian detec- tion: An evaluation of the state of the art,&quot; PAMI, vol. 34, 2012.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Krizhevsky, I. Sutskever, and G. Hinton, &quot;ImageNet classifica- tion with deep convolutional neural networks,&quot; in NIPS, 2012.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Girshick, J. Donahue, T. Darrell, and J. Malik, &quot;Rich feature hierarchies for accurate object detection and semantic segmenta- tion,&quot; in CVPR, 2014.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Sermanet, D. Eigen, S. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, &quot;OverFeat: Integrated recognition, localization and detection using convolutional networks,&quot; in ICLR, April 2014.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth, &quot;Describing objects by their attributes,&quot; in CVPR, 2009.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SUN attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Patterson and J. Hays, &quot;SUN attribute database: Discovering, annotating, and recognizing scene attributes,&quot; in CVPR, 2012.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">L. Bourdev and J. Malik, &quot;Poselets: Body part detectors trained using 3D human pose annotations,&quot; in ICCV, 2009.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, &quot;Indoor seg- mentation and support inference from RGBD images,&quot; in ECCV, 2012.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Canonical perspective and the perception of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention and performance IX</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Palmer, E. Rosch, and P. Chase, &quot;Canonical perspective and the perception of objects,&quot; Attention and performance IX, vol. 1, p. 4, 1981.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hoiem, D, Y. Chodpathumwan, and Q. Dai, &quot;Diagnosing error in object detectors,&quot; in ECCV, 2012.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PRL</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Brostow, J. Fauqueur, and R. Cipolla, &quot;Semantic object classes in video: A high-definition ground truth database,&quot; PRL, vol. 30, no. 2, pp. 88-97, 2009.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Russell, A. Torralba, K. Murphy, and W. Freeman, &quot;LabelMe: a database and web-based tool for image annotation,&quot; IJCV, vol. 77, no. 1-3, pp. 157-173, 2008.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OpenSurfaces: A richly annotated catalog of surface appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Bell, P. Upchurch, N. Snavely, and K. Bala, &quot;OpenSurfaces: A richly annotated catalog of surface appearance,&quot; SIGGRAPH, vol. 32, no. 4, 2013.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Ordonez, G. Kulkarni, and T. Berg, &quot;Im2text: Describing images using 1 million captioned photographs.&quot; in NIPS, 2011.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scalable multi-label annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<editor>CHI</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Deng, O. Russakovsky, J. Krause, M. Bernstein, A. Berg, and L. Fei-Fei, &quot;Scalable multi-label annotation,&quot; in CHI, 2014.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, &quot;Microsoft COCO: Common objects in context,&quot; in ECCV, 2014.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Scharstein and R. Szeliski, &quot;A taxonomy and evaluation of dense two-frame stereo correspondence algorithms,&quot; IJCV, vol. 47, no. 1-3, pp. 7-42, 2002.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Baker, D. Scharstein, J. Lewis, S. Roth, M. Black, and R. Szeliski, &quot;A database and evaluation methodology for optical flow,&quot; IJCV, vol. 92, no. 1, pp. 1-31, 2011.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop of Generative Model Based Vision (WGMBV)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="raw_reference">L. Fei-Fei, R. Fergus, and P. Perona, &quot;Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,&quot; in CVPR Workshop of Generative Model Based Vision (WGMBV), 2004.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">7694</biblScope>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">G. Griffin, A. Holub, and P. Perona, &quot;Caltech-256 object category dataset,&quot; California Institute of Technology, Tech. Rep. 7694, 2007.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Dalal and B. Triggs, &quot;Histograms of oriented gradients for human detection,&quot; in CVPR, 2005.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Lecun and C. Cortes, &quot;The MNIST database of handwritten digits,&quot; 1998. [Online]. Available: http://yann.lecun.com/exdb/ mnist/</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Columbia object image library (coil-20)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Columbia Universty, Tech. Rep</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Nene, S. K. Nayar, and H. Murase, &quot;Columbia object image library (coil-20),&quot; Columbia Universty, Tech. Rep., 1996.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto, Tech. Rep</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">A. Krizhevsky and G. Hinton, &quot;Learning multiple layers of fea- tures from tiny images,&quot; Computer Science Department, University of Toronto, Tech. Rep, 2009.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Torralba, R. Fergus, and W. T. Freeman, &quot;80 million tiny images: A large data set for nonparametric object and scene recognition,&quot; PAMI, vol. 30, no. 11, pp. 1958-1970, 2008.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Ordonez, J. Deng, Y. Choi, A. Berg, and T. Berg, &quot;From large scale image categorization to entry-level categories,&quot; in ICCV, 2013.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Blackwell Books</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Fellbaum, WordNet: An electronic lexical database. Blackwell Books, 1998.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-201</idno>
	</analytic>
	<monogr>
		<title level="j">Caltech</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note type="raw_reference">P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, &quot;Caltech-UCSD Birds 200,&quot; Caltech, Tech. Rep. CNS-TR-201, 2010.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hjelmås</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="236" to="274" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Hjelmås and B. Low, &quot;Face detection: A survey,&quot; CVIU, vol. 83, no. 3, pp. 236-274, 2001.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="7" to="49" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
	<note type="raw_reference">G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller, &quot;Labeled faces in the wild,&quot; University of Massachusetts, Amherst, Tech. Rep. 07-49, October 2007.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Detecting avocados to zucchinis: what have we done, and where are we going?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>in ICCV</note>
	<note type="raw_reference">O. Russakovsky, J. Deng, Z. Huang, A. Berg, and L. Fei-Fei, &quot;Detecting avocados to zucchinis: what have we done, and where are we going?&quot; in ICCV, 2013.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="23" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Shotton, J. Winn, C. Rother, and A. Criminisi, &quot;TextonBoost for image understanding: Multi-class object recognition and seg- mentation by jointly modeling texture, layout, and context,&quot; IJCV, vol. 81, no. 1, pp. 2-23, 2009.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. M. Seitz, B. Curless, J. Diebel, D. Scharstein, and R. Szeliski, &quot;A comparison and evaluation of multi-view stereo reconstruction algorithms,&quot; in CVPR, 2006.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, &quot;Contour detec- tion and hierarchical image segmentation,&quot; PAMI, vol. 33, no. 5, pp. 898-916, 2011.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Lampert, H. Nickisch, and S. Harmeling, &quot;Learning to detect unseen object classes by between-class attribute transfer,&quot; in CVPR, 2009.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Heitz and D. Koller, &quot;Learning spatial context: Using stuff to find things,&quot; in ECCV, 2008.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sitton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spelling</forename><surname>Sourcebook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Egger Publishing</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Sitton, Spelling Sourcebook. Egger Publishing, 1996.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Finding iconic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Berg and A. Berg, &quot;Finding iconic images,&quot; in CVPR, 2009.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Torralba and A. Efros, &quot;Unbiased look at dataset bias,&quot; in CVPR, 2011.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluation of gist descriptors for web-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sandhawalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Douze, H. Jégou, H. Sandhawalia, L. Amsaleg, and C. Schmid, &quot;Evaluation of gist descriptors for web-scale image search,&quot; in CIVR, 2009.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan, &quot;Object detection with discriminatively trained part-based mod- els,&quot; PAMI, vol. 32, no. 9, pp. 1627-1645, 2010.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models, release 5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Girshick, P. Felzenszwalb, and D. McAllester, &quot;Discriminatively trained deformable part models, release 5,&quot; PAMI, 2012.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Do we need more training data or better models for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>BMVC</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes, &quot;Do we need more training data or better models for object detection?&quot; in BMVC, 2012.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Object segmentation by alignment of poselet activations to image contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Brox, L. Bourdev, S. Maji, and J. Malik, &quot;Object segmentation by alignment of poselet activations to image contours,&quot; in CVPR, 2011.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1731" to="1743" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Yang, S. Hallman, D. Ramanan, and C. Fowlkes, &quot;Layered object models for image segmentation,&quot; PAMI, vol. 34, no. 9, pp. 1731-1743, 2012.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using segmentation to verify object hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ramanan, &quot;Using segmentation to verify object hypotheses,&quot; in CVPR, 2007.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to localize detected objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Dai and D. Hoiem, &quot;Learning to localize detected objects,&quot; in CVPR, 2012.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Collecting image annotations using Amazon&apos;s Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier, &quot;Col- lecting image annotations using Amazon&apos;s Mechanical Turk,&quot; in NAACL Workshop, 2010.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
