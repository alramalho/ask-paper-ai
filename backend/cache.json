{"Please answer the following request, denoted by \"Request:\" in the best way possible with the given context that bounded by \"Start context\" and \"End context\".Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the charactersitics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characterstics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart context\nThe emergence of deep learning has considerably advanced the state-of-the-art in cardiac magnetic resonance (CMR) segmentation. Many techniques have been proposed over the last few years, bringing the accuracy of automated segmentation close to human performance. However, these models have been all too often trained and validated using cardiac imaging samples from single clinical centres or homogeneous imaging protocols. This has prevented the development and validation of models that are generalizable across different clinical centres, imaging conditions or scanner vendors. To promote further research and scientific benchmarking in the field of generalizable deep learning for cardiac segmentation, this paper presents the results of the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge, which was recently organized as part of the MICCAI 2020 Conference. A total of 14 teams submitted different solutions to the problem, combining various baseline models, data augmentation strategies, and domain adaptation techniques. The obtained results indicate the importance of intensity-driven data augmentation, as well as the need for further research to improve generalizability towards unseen scanner vendors or new imaging protocols. Furthermore, we present a new resource of 375 heterogeneous CMR datasets acquired by using four different scanner vendors in six hospitals and three different countries (Spain, Canada and Germany), which we provide as open-access for the community to enable future research in the field.A total of six clinical centres from Spain, Canada and Germany (numbered to 6 in this work) contributed to this III  AVERAGE SPECIFICATIONS FOR THE IMAGES ACQUIRED IN THE DIFFERENT CENTRES challenge by providing a different number of CMR studies from different scanner vendors, as detailed in Table I . In total, 375 studies were included in this challenge. The subjects considered for this multi-disease study were selected among groups of various cardiovascular diseases, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, coronary heart disease, abnormal right ventricle, myocarditis and ischemic cardiomyopathy as well as healthy volunteers (see Table II for more details on the distribution of these cases). The specific scanner manufacturers are: 1) Siemens (Siemens Healthineers, Germany), 2) Philips (Philips Healthcare, Netherlands), 3) General Electric (GE, GE Healthcare, USA) and 4) Canon (Canon Inc., Japan). These four manufacturers were coded as A, B, C and D during the challenge, respectively. The CMR images derived from these four vendors are illustrated in Fig. 1 . More specific details on the studies are given in Table III .\nEvery CMR study was annotated manually by an expert clinician from the centre of origin, with experiences ranging from 3 to more than 10 years. Following the clinical protocol, short-axis views were annotated at the end-diastolic (ED) and end-systolic (ES) phases, as they correspond to the phases used to compute the relevant clinical biomarkers for cardiac diagnosis and follow-up. Three main regions were considered: the left and right ventricle (LV and RV, respectively) cavities and the left ventricle myocardium (MYO). In order to reduce the inter-observer and inter-centre variability in the contours, in particular at the apical and basal regions, a detailed revision of the provided segmentations was performed by four researchers in pairs. They applied the same SOP across all CMR datasets to obtain the final ground truth. To generate consistent annotations for the research community, we chose to apply the SOP that was already used by the ACDC challenge, as follows:\na) The LV and RV cavities must be completely covered, including the papillary muscles. b) No interpolation of the MYO boundaries must be performed at the basal region. c) The RV must have a larger surface at the ED time-frame compared to ES. d) The RV does not include the pulmonary artery. Clinical delineations as well as later corrections were performed using CVI42 software (Circle Cardiovascular Imaging Inc., Calgary, Alberta, Canada). All studies were provided in DICOM format and contours were extracted in cvi42 workspace format (.cvi42ws). An in-house software was then used to extract the contours and transform the images into the NIFTI format, representing the final files delivered to the challenge participants.\nAll participants in the challenge (except P11) used some form of data augmentation to enhance their models. Specifically, two families of data augmentations were considered: (1) spatial transformations to increase sample size through rotation, flipping, scaling or deformation of the original images; (2) intensity-driven techniques, which maintain the spatial configuration of the anatomical structures but modify their image appearance. The second type of augmentation seems particularly relevant for the M&Ms as it may increase the variability in image appearance, with the hypothesis that this may lead to improved adaptation to varying imaging protocols and scanner vendors. Two teams performed data augmentation using only spatial transformations (P4, P6). Eleven teams additionally implemented intensity-based transformations using one of two main approaches: (i) standard image transformations such as histogram matching, blurring, change in brightness, gamma and contrast, or addition of Gaussian noise (P1-P3, P7-P8, P10, P13) (see 3 for a visualization of a subset of these transformations on a training slice); (ii) advanced image synthesis by using generative adversarial networks (GANs) (P5, P8, P14) or variational auto-encoders (VAE) (P12). For the latter one, the generation of synthetic images for the unseen vendor D is not feasible since it was not included in the training. Note that the majority of the teams participating in the challenge (10 out of 14) relied solely on data augmentation of the training sample to address the domain-shift problem posed by the M&Ms challenge.\nAdditionally, some teams (P1-P3, P9, P13) applied testtime augmentation techniques, which consist of passing to the model two or more transformed versions of the same inference image to obtain several predictions. These predictions are then combined to obtain one final outcome, usually by averaging them. This method has been shown to improve the final performance in small data size scenarios and a net improvement with a scale effect that depends on the model architecture [38] .\nEnd context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| M&Ms Challenge | 375 studies | Various cardiovascular diseases, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, coronary heart disease, abnormal right ventricle, myocarditis and ischemic cardiomyopathy as well as healthy volunteers | Spain, Canada and Germany | https://www.sciencedirect.com/science/article/pii/S1361841520302676 |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nWe present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases. When did the current expedition to the ISS launch? On March 30th 2022, with NASA astronaut Thomas Marshburn taking over as ISS commander. User Sparrow What do they do up there? User The expedition began upon the departure of Soyuz MS-19 on 30 March 2022, with NASA astronaut Thomas Marshburn taking over as ISS commander. Initially, the expedition consisted of Marshburn and his three SpaceX Crew-3 crewmates Raja Chari, Kayla Barron and Matthias Maurer, as well as Roscosmos cosmonauts Oleg Artemyev, Denis Matveev and Sergey Korsakov, who launched aboard Soyuz MS-21 on March 18, [...] [Source: Expedition 67-Wikipedia]Similar to ; Stiennon et al. (2020) and others, our method involves a continuous cycle of evaluation and training as illustrated in fig. 3 . We start with DPC described in section 2.2, as the initial dialogue agent. We then ask human participants to interact with the agent in two main settings: per-turn response preference and adversarial probing.\nPer-turn response preference In this task, human raters are given an incomplete dialogue and multiple possible statements to continue the dialogue, each corresponding to a different sample or model. The human raters select the response that they think is best ( fig. 20) . In contrast to Askell et al.\n(2021), a model generates both the User and Agent turns, and in both cases the human raters are asked to select the best response. The selected response is then used to continue the dialogue. Per-turn response preference data lets us estimate a preference rate which measures how frequently a model is preferred over one or more competing models. When responses are combined with supporting evidence, human raters give additional per-response feedback, as described in section 2.4.\nThe majority of the training data for the preference reward model comes from the per-turn response preference data collections. We also reuse the data collected for GopherCite. The exact breakdown is presented in table 11. The dataset includes both Sparrow turns (with or without evidence) and User turns.\nWe break each of the multi-option comparisons into pairs comparing chosen best continuation to every other non-selected option. We use these to form training tuples of (best_completion, other_completion, distractor). Table 12 | Preference reward model training data format. We show an example context, with statements, A, B, C, and two possible completions, either with or without evidence.\nWe use rule violation ratings from adversarial probing tasks and free dialogue tasks (see section 2.3) to train our rule RM. Overall, we collected 14576 dialogues from the adversarial probing tasks, and rated each for one or more rules. The dialogues are randomly split in 70 % training data and 30% test data; we train over 6 epochs of the training data. The Likert scale of the rule violation ratings Table 13 | Breakdown of the rule ratings used to train the rule RM. Dialogues were collected from many models as research and model training progressed. More harmful rules were targeted later in development, and data collections run more sparingly, hence the variation in collected volumes.\nIn this section we provide more detail of our human data collection methodology.\nEnd paper context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| Sparrow | 14576 dialogues |  | Adversarial probing tasks and free dialogue tasks |  |\n| Preference reward model training data format |  |  |  |  |\n|  |  |  |  |  |\n|  |  |  |  |  |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nBackground: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication .The chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images were imported into the software for delineating, and the images with VOI information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\nAs depicted in Table 1 , we randomly split the whole RibFrac Dataset (900 cases, 332,483 CT slices in total) into 3 cohorts: training (720 cases, to train the deep learning system), tuning (60 cases, to tune hyper-parameters of the deep learning system) and test (120 cases, to evaluate the model and human performance). In standard machine learning terminology, tuning is regarded as \"validation\"; in standard medical terminology, test in regarded as \"validation\".\nConsidering several practical issues, we open source a subset of 600 cases (221,308 CT slices in total), with 420 cases for training, 60 cases for tuning and 120 for test. To our knowledge, it is the first open research dataset in this application. On the open-source subset of RibFrac Dataset, a deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance. Please refer to Supplementary Materials for details. Our algorithm follows a data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures. Notably, the proposed FracNet does not rely on the extraction of rib centerlines in typical rib analysis algorithms [27] . As illustrated in Fig. 2 (a), our model pipeline consists of three stages: (a) pre-processing, (b) sliding-window prediction, and (c) post-processing.\n(a) Pre-processing: To speed up the detection, we extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering). The original spacing was preserved since only thin-section CT scans were included in our dataset. The intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n(b) Sliding-window prediction: Considering the elongated shape of rib fractures, standard labeling with bounding boxes could be missing much details. Therefore, we formulated the rib fracture detection as a 3D segmentation task. A customized 3D UNet, named FracNet (Section 2.2.2.), was developed to perform segmentation in a sliding-window fashion. Since a whole-volume CT scan could be too large to fit in a regular GPU memory, we cropped 64 \u00c2 64 \u00c2 64patches in a sliding-window fashion with a stride of 48 and feed them to our network. A raw segmentation was obtained by assembling patches of prediction. Maximum values were kept in the overlapping regions of multiple predictions.\n(c) Post-processing: To efficiently reduce the false positive in our predictions, predictions of small sizes (smaller than 200 voxels) were filtered out. We also removed the spine regions according to their coordinates on the raw segmentation. To generate detection proposal, we first binarized the post-processed segmentation results with a low threshold of 0.1, and then computed connected components on the binary segmentation. Each connected component was regarded as a detection proposal, with a probability calculated by averaging raw segmentation scores over all voxels within the connected component.\nThe data and code from Huadong Hospital affiliated to Fudan University used in this study are available at https://m3dv.github.io/Frac Net/ to users who agree with our data license (CC BY-NC 4.0) and code license (Apache-2.0 License).\nEnd paper context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| RibFrac Dataset | 7,473 annotated traumatic rib fractures from 900 patients in a single center | Enrolled with a human-in-the-loop labeling procedure | A full list of funding bodies that contributed to this study can be found in the Acknowledgements section | https://m3dv.github.io/FracNet/ |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nLarge, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. 1CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain. We report the prevalences of the labels for the different obsevations in Table 1 .\nWe retrospectively collected chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports. From these, we sampled a set of 1000 reports for manual review by a boardcertified radiologist to determine feasibility for extraction of observations. We decided on 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Society's recommended glossary (Hansell et al. 2008 ) whenever applicable. \"Pneumonia\", despite being a clinical diagnosis, was included as a label in order to represent the images that suggested primary infection as the diagnosis. The \"No Finding\" observation was intended to capture the absence of all pathologies.\n1. unremarkable cardiomediastinal silhouette 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation.\n3. no pleural effusion or pneumothorax 4. mild degenerative changes in the lumbar spine and old right rib fractures. Figure 2 : Output of the labeler when run on a report sampled from our dataset. In this case, the labeler correctly extracts all of the mentions in the report (underline) and classifies the uncertainties (bolded) and negations (italicized).\nOne of the main obstacles in the development of chest radiograph interpretation models has been the lack of datasets with strong radiologist-annotated groundtruth and expert scores against which researchers can compare their models. There are few chest radiographic imaging datasets that are publicly available, but none of them have test sets with strong ground truth or radiologist performances. The Indiana Network for Patient Care hosts the OpenI dataset (Demner-Fushman et al. 2015) consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators . The National Cancer Institute hosts the PLCO Lung dataset (Gohagan et al. 2000) of chest radiographs obtained during a study on lung cancer screening . The dataset contains 185,421 full resolution images, but due to the nature of the collection process, it is has a low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly. The MIMIC-CXR dataset (Rubin et al. 2018) has been recently announced but is not yet publicly available.\nThe most commonly used benchmark for developing chest radiograph interpretation models has been the ChestX-ray14 dataset . Due to the introduction of this large dataset, substantial progress has been made towards developing automated chest radiograph interpretation models (Yao et al. 2017; Rajpurkar et al. 2017; Li et al. 2017b; Kumar, Grewal, and Srivastava 2018; Guan et al. 2018; Yao et al. 2018) . However, using the NIH dataset as a benchmark on which to compare models is problematic as the labels in the test set are extracted from reports using an automatic labeler. The CheXpert dataset that we introduce features radiologist-labeled validation and test sets which serve as strong reference standards, as well as expert scores to allow for robust evaluation of different algorithms.\nEnd paper context": "| Name                                                        | Size                                                        | Demographic information                                                        | Origin                                                        | Data or code link to find more                                                        |\n| ----------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------------- |\n| CheXpert                                                    | 224,316 chest radiographs of 65,240 patients                  | 1. unremarkable cardiomediastinal silhouette <br/> 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation. <br/> 3. no pleural effusion or pneumothorax <br/> 4. mild degenerative changes in the lumbar spine and old right rib fractures. | Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports. | https://stanfordmlgroup.github.io/competitions/chexpert/ |\n| OpenI                                                       | 7,470 frontal-view radiographs                              |                                                                                   |                                                              | https://openi.nlm.nih.gov/                                                             |\n| PLCO Lung                                                   | 185,421 full resolution images                               |                                                                                   |                                                              | https://wiki.cancerimagingarchive.net/display/Public/PLCO+Lung+Cancer+Screening+Trial |\n| MIMIC-CXR                                                   |                                                              |                                                                                   |                                                              |                                                                                       |\n| ChestX-ray14                                                |                                                              |                                                                                   |                                                              |                                                                                       |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: What does the paper talk about?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.\nEnd paper context": "The paper talks about the increasing number of datasets being published on the Web, and how Google's Dataset Search extracts metadata from these datasets. The paper also analyses the corpus of datasets and discusses where they originate from, what topics they cover, which form they take, and what people searching for datasets are interested in.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: What are the identified limitations on their solution?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.\nEnd paper context": "The identified limitations on their solution are that they only extract dataset metadata from web pages, and that the number of datasets described in schema.org is still relatively small.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: What metadata standards do they use?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.\nEnd paper context": "The authors do not explicitly mention which metadata standards they use, but based on the context it can be inferred that they use schema.org and similar vocabularies.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \\Request: What is the paper about?\nStart paper context\nBackground: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication .Recent advances in artificial intelligence and computer vision lead to a rapid development of deep learning technology [1] in medical image analysis and digital medicine [2\u00c07] . With end-to-end learning of deep representation, deep supervised learning, as a unified methodology, achieved remarkable success in numerous 2D and 3D medical image tasks, e.g., classification [8] , detection [9] , segmentation [10] . With the rise of deep learning, infrastructures, algorithms and data (with annotations) are known to be the keys to its success. Computer-aided diagnosis with a high-performance deep learning is expected to save human labor, improve diagnosis consistency and accuracy, personalize patient treatment, and improve patient\u00c0doctor relationship. [11] Rib fracture represents an important indicator of trauma severity; the number of fractured ribs increases morbidity and mortality [12] . Multidetector computed tomography (CT) provides a more accurate assessment to evaluate for the presence of rib fractures when standard posteroanterior (PA) chest radiograph is specific but insensitive [12\u00c015]. Definite diagnosis (counting) of the number of rib fractures is also an important indicator in forensic examination for degree of disability [14, 16, 17] . However, the identification of rib fracture in CT images using conventional axial thin (1\u00c01.5 mm) images is a difficult and labor-intensive task. Each rib has a complex shape with a diagonal course across numerous CT sections [18] , which leads to missing rib fracture diagnosis (detection) in clinical practice. For instance, buckle fractures are the most frequently missing type of fracture reported in 2012 [19, 20] , due to the confusing appearance; nondisplaced rib fractures could be missing when parallel to the scan plane of the CT images. Besides, diagnosing subtle fractures is tedious and time-consuming for a large number of CT slices, which must be evaluated sequentially, rib-by-rib and side-by-side [18] .\nIn this study, we aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Few prior studies explore the development and validation of deep learning algorithms in this application. We proposed an automatic system named FracNet based on 3D UNet [21] , trained and evaluated with a large-scale dataset, named RibFrac Dataset, consisting of 7,473 voxellevel rib fracture segmentation from 900 chest-abdomen CT scans (332,483 CT slices). The annotation of RibFrac Dataset followed a human-in-the-loop labeling procedure, which ensures a high standard of annotation quality. On RibFrac test cohort, the proposed Frac-Net system achieved a detection sensitivity of 92.9% (with 5.27 false positives per scan) and a segmentation Dice Coefficient of 71.5%, which outperformed counterpart methods based on 3D variants of FCN [22] and DeepLab v3+ [23] with a 3D ResNet-18 backbone [24, 25] . Furthermore, observer studies with two experienced radiologists, including independent human-only study and humancollobration study, were designed to validate the clinical value of the proposed system. Our system achieved higher detection sensitivities than human experts. Importantly, human-computer collaboration significantly improved detection sensitivities over computer-only and human-only diagnosis, with reduced clinical time compared to human-only diagnosis.\nAs the first open research in this application, a subset of the annotated RibFrac Dataset (600 CT scans, 221,308 CT slices) and our code for model training and evaluation will be open-source. We believe this large-scale dataset could facilitate both clinical research for automatic rib fracture diagnosis and engineering research for 3D computer vision.\n\n\nThis retrospective study was approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146), which waived the requirement for informed consent.\nFrom January 2017 to December 2018, a search of the electronic medical records and the radiology information systems of the hospital for patients with traumatic rib fractures identified on chest-abdomen CT scans (1\u00c01.25 mm) was performed by one author. A total of 7,473 traumatic rib fractures from 900 patients [mean age, 55.1 years 11.82 (standard deviation); range, 21\u00c094 years] were enrolled in the study. There were 580 men [63.8%] and 329 women [36.2%] . Traumatic abdomen-thorax CT was performed by using the following two CT scanners: 16 cm wide coverage detector CT (Revolution CT, GE Healthcare, WI, USA); second-generation dual-source CT scanner (Somatom Definition Flash, Siemens Healthcare, Forchheim, Germany) with following parameters: 120 kVp; 100\u00c0200 mAs; pitch, 0.75\u00c01.5; and collimation, 1\u00c01.25 mm, respectively. All imaging data were reconstructed by using a bone or medium sharp reconstruction algorithm with a thickness of 1\u00c01.25 mm.\nAs detailed in Fig. 1 (a), the inclusion criteria are as follows: (1) Traumatic patients with thin-slice chest-abdomen CT images (1\u00c01.25 mm) containing all ribs, and (2) Thin-slice CT images without breathing artifact debasing diagnostic accuracy.\nIn the whole labeling procedure, there were 5 radiologists involved: A (3-5 years), B (10-20 years), C (5 years), D (5 years), E (20 years); numbers in the brackets denote the years of experience in chest CT interpretation.\nAll enrolled CT scans were first randomly diagnosed by two radiologists A and B in radiology department after the CT examinations in 48 hours, who did not participate in this study. Two junior radiologists C and D manually delineated the volume of interest (VOI) of the traumatic rib fractures with diagnosed CT reports at voxel level on axial CT images with the help of the diagnosis reports (by the radiologists A or B) and a medical image processing and navigation software 3D Slicer (version 4.8.1, Brigham and Women's Hospital). The broken ends of fractured bone were included as much as possible for the volume of the fractures as Fig. 1 (b) ; Besides, as illustrated in Fig. 1 (c ), axial images combining manually curve planar reformation images were used together to insure the accuracy of labeling the real fractures [14] , as rib fractures can be variable and inconspicuous if the fracture line is not present or parallels the detection plane [18] . After labeling by C and D, the VOIs were then confirmed by another senior radiologist E.\nEvidence before this study Quickly and precisely identifying the rib fractures in a large number of CT images is a tough and important task, which plays an important role in identifying trauma severity. Deep leanring has achieved a great success in medical image analysis. In this study, we aimed at a clinically applicable deep learning system to automatically detect and segment rib fractures.\nWe present a deep learning system, named FracNet, for automatic detection and segmentation of the rib fractures. The proposed FracNet achieved high detection sensitivity, acceptable false positive per scan and segmentation overlap, which was proven to improve the human detection sensitivity with reduced clinical time comsued in our observer study. Besides, a subset of our dataset was open-source to research community, which is the first open large-scale dataset in this application.\nThe proposed FracNet could help the radiologists in the diagnosis of rib fractures, to increase the efficiency of the clinical workflow, without decreasing the diagnostic accuracy at the same time.\nAn initial deep learning model following a same pipeline as Frac-Net (Section 2.2) was developed on the RibFrac training cohort (Section 2.1.3). The initial system was used to predict fractures on the RibFrac training, tuning and test cohorts. We excluded all predicted fractures with high overlap between any initial label; all remaining predictions were feedback to the radiologist E to verify (reduce false positives). This procedure was assisted by an interactive visual tool (see Supplementary Materials). Around 20% annotations were missing from initial labeling and added with the human-in-the-loop labeling. The verified annotations were used for the development and validation of the deep learning system. Please note that there was no data leakage issue in the human-in-the-loop labeling procedure and the following development and validation, since our deep learning system was only trained on the training cohort.\nThe chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images were imported into the software for delineating, and the images with VOI information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\nAs depicted in Table 1 , we randomly split the whole RibFrac Dataset (900 cases, 332,483 CT slices in total) into 3 cohorts: training (720 cases, to train the deep learning system), tuning (60 cases, to tune hyper-parameters of the deep learning system) and test (120 cases, to evaluate the model and human performance). In standard machine learning terminology, tuning is regarded as \"validation\"; in standard medical terminology, test in regarded as \"validation\".\nConsidering several practical issues, we open source a subset of 600 cases (221,308 CT slices in total), with 420 cases for training, 60 cases for tuning and 120 for test. To our knowledge, it is the first open research dataset in this application. On the open-source subset of RibFrac Dataset, a deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance. Please refer to Supplementary Materials for details. Our algorithm follows a data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures. Notably, the proposed FracNet does not rely on the extraction of rib centerlines in typical rib analysis algorithms [27] . As illustrated in Fig. 2 (a), our model pipeline consists of three stages: (a) pre-processing, (b) sliding-window prediction, and (c) post-processing.\n(a) Pre-processing: To speed up the detection, we extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering). The original spacing was preserved since only thin-section CT scans were included in our dataset. The intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n(b) Sliding-window prediction: Considering the elongated shape of rib fractures, standard labeling with bounding boxes could be missing much details. Therefore, we formulated the rib fracture detection as a 3D segmentation task. A customized 3D UNet, named FracNet (Section 2.2.2.), was developed to perform segmentation in a sliding-window fashion. Since a whole-volume CT scan could be too large to fit in a regular GPU memory, we cropped 64 \u00c2 64 \u00c2 64patches in a sliding-window fashion with a stride of 48 and feed them to our network. A raw segmentation was obtained by assembling patches of prediction. Maximum values were kept in the overlapping regions of multiple predictions.\n(c) Post-processing: To efficiently reduce the false positive in our predictions, predictions of small sizes (smaller than 200 voxels) were filtered out. We also removed the spine regions according to their coordinates on the raw segmentation. To generate detection proposal, we first binarized the post-processed segmentation results with a low threshold of 0.1, and then computed connected components on the binary segmentation. Each connected component was regarded as a detection proposal, with a probability calculated by averaging raw segmentation scores over all voxels within the connected component": ".\nEnd paper context\n\nThis paper is about developing a deep learning system, named FracNet, for automatic detection and segmentation of rib fractures from chest-abdomen CT scans. The proposed FracNet achieved high detection sensitivity, acceptable false positive per scan and segmentation overlap, which was proven to improve the human detection sensitivity with reduced clinical time comsued in the observer study. A subset of the annotated RibFrac Dataset (600 CT scans, 221,308 CT slices) and the code for model training and evaluation were open-sourced to research community.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \\Request: What is the paper about?\nStart paper context\nBackground: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication .Recent advances in artificial intelligence and computer vision lead to a rapid development of deep learning technology [1] in medical image analysis and digital medicine [2\u00c07] . With end-to-end learning of deep representation, deep supervised learning, as a unified methodology, achieved remarkable success in numerous 2D and 3D medical image tasks, e.g., classification [8] , detection [9] , segmentation [10] . With the rise of deep learning, infrastructures, algorithms and data (with annotations) are known to be the keys to its success. Computer-aided diagnosis with a high-performance deep learning is expected to save human labor, improve diagnosis consistency and accuracy, personalize patient treatment, and improve patient\u00c0doctor relationship. [11] Rib fracture represents an important indicator of trauma severity; the number of fractured ribs increases morbidity and mortality [12] . Multidetector computed tomography (CT) provides a more accurate assessment to evaluate for the presence of rib fractures when standard posteroanterior (PA) chest radiograph is specific but insensitive [12\u00c015]. Definite diagnosis (counting) of the number of rib fractures is also an important indicator in forensic examination for degree of disability [14, 16, 17] . However, the identification of rib fracture in CT images using conventional axial thin (1\u00c01.5 mm) images is a difficult and labor-intensive task. Each rib has a complex shape with a diagonal course across numerous CT sections [18] , which leads to missing rib fracture diagnosis (detection) in clinical practice. For instance, buckle fractures are the most frequently missing type of fracture reported in 2012 [19, 20] , due to the confusing appearance; nondisplaced rib fractures could be missing when parallel to the scan plane of the CT images. Besides, diagnosing subtle fractures is tedious and time-consuming for a large number of CT slices, which must be evaluated sequentially, rib-by-rib and side-by-side [18] .\nIn this study, we aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Few prior studies explore the development and validation of deep learning algorithms in this application. We proposed an automatic system named FracNet based on 3D UNet [21] , trained and evaluated with a large-scale dataset, named RibFrac Dataset, consisting of 7,473 voxellevel rib fracture segmentation from 900 chest-abdomen CT scans (332,483 CT slices). The annotation of RibFrac Dataset followed a human-in-the-loop labeling procedure, which ensures a high standard of annotation quality. On RibFrac test cohort, the proposed Frac-Net system achieved a detection sensitivity of 92.9% (with 5.27 false positives per scan) and a segmentation Dice Coefficient of 71.5%, which outperformed counterpart methods based on 3D variants of FCN [22] and DeepLab v3+ [23] with a 3D ResNet-18 backbone [24, 25] . Furthermore, observer studies with two experienced radiologists, including independent human-only study and humancollobration study, were designed to validate the clinical value of the proposed system. Our system achieved higher detection sensitivities than human experts. Importantly, human-computer collaboration significantly improved detection sensitivities over computer-only and human-only diagnosis, with reduced clinical time compared to human-only diagnosis.\nAs the first open research in this application, a subset of the annotated RibFrac Dataset (600 CT scans, 221,308 CT slices) and our code for model training and evaluation will be open-source. We believe this large-scale dataset could facilitate both clinical research for automatic rib fracture diagnosis and engineering research for 3D computer vision.\n\n\nThis retrospective study was approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146), which waived the requirement for informed consent.\nFrom January 2017 to December 2018, a search of the electronic medical records and the radiology information systems of the hospital for patients with traumatic rib fractures identified on chest-abdomen CT scans (1\u00c01.25 mm) was performed by one author. A total of 7,473 traumatic rib fractures from 900 patients [mean age, 55.1 years 11.82 (standard deviation); range, 21\u00c094 years] were enrolled in the study. There were 580 men [63.8%] and 329 women [36.2%] . Traumatic abdomen-thorax CT was performed by using the following two CT scanners: 16 cm wide coverage detector CT (Revolution CT, GE Healthcare, WI, USA); second-generation dual-source CT scanner (Somatom Definition Flash, Siemens Healthcare, Forchheim, Germany) with following parameters: 120 kVp; 100\u00c0200 mAs; pitch, 0.75\u00c01.5; and collimation, 1\u00c01.25 mm, respectively. All imaging data were reconstructed by using a bone or medium sharp reconstruction algorithm with a thickness of 1\u00c01.25 mm.\nAs detailed in Fig. 1 (a), the inclusion criteria are as follows: (1) Traumatic patients with thin-slice chest-abdomen CT images (1\u00c01.25 mm) containing all ribs, and (2) Thin-slice CT images without breathing artifact debasing diagnostic accuracy.\nIn the whole labeling procedure, there were 5 radiologists involved: A (3-5 years), B (10-20 years), C (5 years), D (5 years), E (20 years); numbers in the brackets denote the years of experience in chest CT interpretation.\nAll enrolled CT scans were first randomly diagnosed by two radiologists A and B in radiology department after the CT examinations in 48 hours, who did not participate in this study. Two junior radiologists C and D manually delineated the volume of interest (VOI) of the traumatic rib fractures with diagnosed CT reports at voxel level on axial CT images with the help of the diagnosis reports (by the radiologists A or B) and a medical image processing and navigation software 3D Slicer (version 4.8.1, Brigham and Women's Hospital). The broken ends of fractured bone were included as much as possible for the volume of the fractures as Fig. 1 (b) ; Besides, as illustrated in Fig. 1 (c ), axial images combining manually curve planar reformation images were used together to insure the accuracy of labeling the real fractures [14] , as rib fractures can be variable and inconspicuous if the fracture line is not present or parallels the detection plane [18] . After labeling by C and D, the VOIs were then confirmed by another senior radiologist E.\nEvidence before this study Quickly and precisely identifying the rib fractures in a large number of CT images is a tough and important task, which plays an important role in identifying trauma severity. Deep leanring has achieved a great success in medical image analysis. In this study, we aimed at a clinically applicable deep learning system to automatically detect and segment rib fractures.\nWe present a deep learning system, named FracNet, for automatic detection and segmentation of the rib fractures. The proposed FracNet achieved high detection sensitivity, acceptable false positive per scan and segmentation overlap, which was proven to improve the human detection sensitivity with reduced clinical time comsued in our observer study. Besides, a subset of our dataset was open-source to research community, which is the first open large-scale dataset in this application.\nThe proposed FracNet could help the radiologists in the diagnosis of rib fractures, to increase the efficiency of the clinical workflow, without decreasing the diagnostic accuracy at the same time.\nAn initial deep learning model following a same pipeline as Frac-Net (Section 2.2) was developed on the RibFrac training cohort (Section 2.1.3). The initial system was used to predict fractures on the RibFrac training, tuning and test cohorts. We excluded all predicted fractures with high overlap between any initial label; all remaining predictions were feedback to the radiologist E to verify (reduce false positives). This procedure was assisted by an interactive visual tool (see Supplementary Materials). Around 20% annotations were missing from initial labeling and added with the human-in-the-loop labeling. The verified annotations were used for the development and validation of the deep learning system. Please note that there was no data leakage issue in the human-in-the-loop labeling procedure and the following development and validation, since our deep learning system was only trained on the training cohort.\nThe chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images were imported into the software for delineating, and the images with VOI information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\nAs depicted in Table 1 , we randomly split the whole RibFrac Dataset (900 cases, 332,483 CT slices in total) into 3 cohorts: training (720 cases, to train the deep learning system), tuning (60 cases, to tune hyper-parameters of the deep learning system) and test (120 cases, to evaluate the model and human performance). In standard machine learning terminology, tuning is regarded as \"validation\"; in standard medical terminology, test in regarded as \"validation\".\nConsidering several practical issues, we open source a subset of 600 cases (221,308 CT slices in total), with 420 cases for training, 60 cases for tuning and 120 for test. To our knowledge, it is the first open research dataset in this application. On the open-source subset of RibFrac Dataset, a deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance. Please refer to Supplementary Materials for details. Our algorithm follows a data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures. Notably, the proposed FracNet does not rely on the extraction of rib centerlines in typical rib analysis algorithms [27] . As illustrated in Fig. 2 (a), our model pipeline consists of three stages: (a) pre-processing, (b) sliding-window prediction, and (c) post-processing.\n(a) Pre-processing: To speed up the detection, we extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering). The original spacing was preserved since only thin-section CT scans were included in our dataset. The intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n(b) Sliding-window prediction: Considering the elongated shape of rib fractures, standard labeling with bounding boxes could be missing much details. Therefore, we formulated the rib fracture detection as a 3D segmentation task. A customized 3D UNet, named FracNet (Section 2.2.2.), was developed to perform segmentation in a sliding-window fashion. Since a whole-volume CT scan could be too large to fit in a regular GPU memory, we cropped 64 \u00c2 64 \u00c2 64patches in a sliding-window fashion with a stride of 48 and feed them to our network. A raw segmentation was obtained by assembling patches of prediction. Maximum values were kept in the overlapping regions of multiple predictions.\n(c) Post-processing: To efficiently reduce the false positive in our predictions, predictions of small sizes (smaller than 200 voxels) were filtered out. We also removed the spine regions according to their coordinates on the raw segmentation. To generate detection proposal, we first binarized the post-processed segmentation results with a low threshold of 0.1, and then computed connected components on the binary segmentation. Each connected component was regarded as a detection proposal, with a probability calculated by averaging raw segmentation scores over all voxels within the connected component\nEnd paper context": "This paper is about developing a deep learning system, named FracNet, for automatic detection and segmentation of rib fractures from chest-abdomen CT scans. The proposed FracNet achieved high detection sensitivity, acceptable false positive per scan and segmentation overlap, which was proven to improve the human detection sensitivity with reduced clinical time comsued in the observer study. A subset of the annotated RibFrac Dataset (600 CT scans, 221,308 CT slices) and the code for model training and evaluation were open-sourced to research community.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \\Request: What is the paper about?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.We live in a data-driven world. Scientists, governments, journalists, commercial companies, and many others publish millions of datasets online. There are thousands of Web sites that publish datasets-some publish a handful, some publish hundreds of thousands [3] . Google's Dataset Search 1 is a search engine for datasets on the Web [16] . It relies on schema.org and similar open standards to extract the semantics of dataset metadata and to make it searchable.\nArguably, the mere existence of Dataset Search and its reliance on semantic markup provided a strong incentive for dataset providers to add such markup to their Web pages. Indeed, we have seen an explosive growth of dataset metadata on the Web since we started the work on Dataset Search. In the Fall of 2016, there were about 500K Web pages that included schema.org/Dataset markup, with half of them coming from data.gov, the US Open Government portal [10] . Today, there are tens of millions of such pages, from thousands of sites.\nA recent comprehensive survey highlights a variety of approaches to help users find datasets [3] : these approaches range from searching within a collection of tables with different schemas [14] , to finding data in repositories, such as Figshare, Zenodo, or DataDryad, to using metadata search engines, like Dataset Search. There are a number of well respected directories of dataset publishers (e.g., DataCite [19] , re3data [12] , Scientific Data in Nature [15] ), but they inevitably miss new datasets or repositories [4] . To the best of our knowledge, Dataset Search is the only collection of dataset metadata that includes all semantically annotated datasets on the Web.\nWe chose to rely primarily on schema.org for describing dataset metadata because both search engines and open-source tools have used it successfully to build an open ecosystem for various types of content [8] . In recent years, the scientific community has also embraced it for publishing data, by creating mappings from other metadata standards to schema.org. For example, Sansone and colleagues define a mapping from the DATS standard in the biomedical community to schema.org [20] . Wang and colleagues use schema.org to describe research-graph data, comprised of researchers, datasets and scholarly articles [24] . Efforts such as bioschemas.org [6] extend schema.org to include domain-specific terminology and relationships.\nIn this paper, we analyze the Dataset Search corpus of metadata. As of March 2020, the corpus contained 28 million datasets from more than 3,700 sites. While limited to the dataset metadata that is available in schema.org or DCAT, this corpus contains a sizable snapshot of the datasets on the Web. And because many researchers and scientists rely on search engines to find datasets [7] , learning from this corpus can inform both the work to improve search engines for datasets and, more important, highlight the gaps in representation and coverage for the community at large. Specifically, in this paper, we make the following contributions:\n-We present methods for analyzing an organically created corpus of metadata for 28 million datasets on the Web (Section 2). -We identify a set of research questions that such a corpus can help analyze and present results of the analysis of the corpus (Section 3). -We discuss lessons learned from the corpus analysis (Section 4).\nIn this section, we describe the methods that we used to collect the metadata and to prepare it for the analysis in Section 3. In the remainder of this paper, we abbreviate the schema.org namespace as so# and the DCAT namespace as dct#.\nWe described the details of the Dataset Search architecture elsewhere [16] . In brief, Dataset Search relies on the Google Web crawl to find pages that contain dataset metadata and to extract the corresponding triples. A post-processing of the Web crawl data parses RDFa, Microdata, and JSON-LD into a common graph data model, broadly equivalent to W3C's RDF triples [18] . We keep so#Dataset, dct#Dataset, and all the related entities and their properties.\nWe enhance, normalize, and augment this corpus in a variety of ways in order to provide users with a meaningful search experience. In this section, we focus only on those processing steps that are relevant to the subsequent data analysis. The processing happens at multiple levels of granularity: At the corpus level, we ensure that datasets are unique and attempt to remove non-datasets (i.e., pages that include dataset markup, but do not describe datasets). At the dataset level, we augment the metadata with inferred properties. Finally, at the property level, we clean up and normalize values.\nThe corpus-level analysis starts by removing duplicates within each site [16] . We found that many dataset repositories add markup both to the dataset landing pages and to the pages that list search results within that repository. We keep only the former in the corpus through simple heuristics: When the same dataset (according to values of key properties) appears on multiple pages, we keep the page that contains only one dataset. We also remove dataset metadata that does not have values for basic properties such as title and description [5] .\nAt the dataset level, we process the values for properties such as title and description as well as the terms on the Web page itself in order to identify the main topics covered by the dataset. We use the topics from re3data.org [12] and a similar set of topics from the Google Knowledge Graph [17] as our vocabulary.\nIn addition, our page-level analysis collects information from the Web page that the dataset originated from, such as the domain of the page and its language.\nFor individual properties, we normalize, clean, and reconcile values for:\n-Data downloads and formats: We identify the patterns that data providers use to represent download information and normalize them to a single representation [16] . Providers may specify file formats through the so#fileFormat or so#encodingType properties. When both of these properties are missing, we extract a file extension from the data-download URL. -DOIs and compact identifiers: Persistent citable identifiers, such as Digital Object Identifiers (DOIs) and Compact Identifiers [26] , may appear in several properties, such as so#identifier, so#url, or even so#sameAs, and so#alternateName. We use regular expressions to find patterns that correspond to these identifiers, and look for known prefixes from identifiers.org in all of these properties. -Level of access: Two properties define the level of access of a dataset: so#isAccessibleForFree is a boolean value that indicates whether or not the dataset requires a payment. so#license links to a license or specifies one inline. We normalize the license information into known classes of licenses, such as Creative Commons and open government licenses. Any license that allows redistribution essentially makes the dataset available for free. We count datasets with these licenses as well as datasets with so#isAccessibleForFree set to true as the datasets that are \"open.\" -Providers: There is some ambiguity in schema.org on how to specify the the source of a dataset. We use the so#publisher and so#provider properties to identify the organization that provided the dataset. As with other properties, the value may be a string or an Organization object. Wherever possible, we reconcile the organization to the corresponding entity in the Google Knowledge Graph. -Updated date: There are several date properties associated with a dataset: so#dateCreated, so#datePublished, so#dateModified (and similar properties in DCAT). There is little consistency in how dataset publishers distinguish between them. However, the most recent value across these dates is usually a reliable approximation on when a dataset was last updated. We use several parsers to understand dates expressed in common formats and to normalize them. If there is no date in the metadata, we use the date when the Web page itself was last updated as a proxy.\nFinally, to analyze the usage of datasets in the Dataset Search application, we look at logs for two weeks in May 2020. We extract the identifiers of the datasets that appeared in search results, and join them with their metadata to analyze search behavior in aggregate.\nAll the data and analyses in this paper are based on a snapshot of the Dataset Search corpus from March 26, 2020. We also compare the status of the corpus with a version from ten months prior, in June 2019.\nWhile we believe that our corpus is a reasonably representative snapshot of the datasets published on the Web, we recognize that it has limitations. Indeed, we have no way of measuring how well the corpus covers all the datasets available on the Web.\nFirst, the corpus contains only the datasets that have semantic descriptions of their metadata in schema.org or DCAT. If a dataset page does not have metadata in a machine-readable format and in a vocabulary that we recognize, it will not be in our corpus (and will not be discoverable in Dataset Search).\nSecond, if a dataset page is not accessible to the Google crawler or is not being crawled for some reason (e.g., because of robots.txt restrictions), it will not be in our corpus. When the crawler processes the page, it often needs to execute JavaScript to get the metadata. If a page is slow to render, we may not obtain dataset metadata from it.\nThird, our methods for inferring new values, such as dataset topics, may be imprecise, and we have not formally evaluated their accuracy yet.\nFourth, in our analysis, we \"trust\" what the metadata says. For instance, if a dataset's metadata says that the dataset is accessible for free, we count it among the open datasets. In some cases, the reality may be different when users try to download the dataset.\nFinally, a significant amount of pages on the Web are designed for Search Engine Optimization or are simply spam. A page may have so#Dataset on it but not actually describe any dataset metadata. While we do our best to weed out such pages, our techniques are not perfect, and we cannot be certain that all the datasets in the corpus that we describe are indeed datasets.\nWe start with the results of a corpus-level analysis (Section 3.1), then look at specific metadata properties (Section 3.2) and finally present our observations on the usage of datasets in Dataset Search (Section 3.3). \nLooking at the corpus as a whole, as well as characteristics of the Web pages that we extracted metadata from, enables us to answer the following questions. Which metadata fields are frequently populated, and which ones rarely have any values? These numbers give us probably the most actionable items in terms of improving metadata quality.\nThe snapshot that we analyze in the rest of this section, taken on March 26, 2020, contains 28M datasets from 3,700 domains. The number of datasets per domain mostly follows a power law distribution, as the logarithmic scale plot in Figure 1a shows: A small number of domains publish millions or hundreds of thousands of datasets, while the long tail of domains hosts just a handful of datasets. The two domains with the largest number of datasets (ceicdata.com and data.gov) have more than 3 million datasets each. The ten largest domains (Figure 1b ) account for 65% of all datasets.\nWhile \"typical\" Web pages about datasets correspond to a single dataset, some pages may have multiple datasets on them. For instance, a page may describe a large dataset and break down its components as multiple datasets; or a page may be dynamically generated in response to a search in a dataset repository. In our corpus, we found that over 90% of datasets come from pages that contain exactly one dataset. Still, more than 1.6M datasets come from pages with more than ten datasets.\nTable 1 shows the distribution of datasets by top-level internet domains. The vast majority of the datasets come from .com domains, but both .org and government domains are well represented. For the country-specific domains, Austria, Spain, Germany, and France are at the top of the list. If we combine all government domains across the world (.gov, .gouv.* , .gv.*, .gov.*, .gob.*, etc.), we find 3.7M datasets on these government domains.\nTo get a more complete picture of the international coverage of datasets, Table 2 breaks them down by language, as specified by or extracted from the Web pages that contain them. More than 18M datasets, or 64% are in English, followed by \nEnd paper context": "Request: What is the paper about?\nThis paper is about analyzing a corpus of 28 million datasets from the Web, extracted from Google's Dataset Search. It discusses the methods used to collect the metadata and prepare it for analysis, and presents the results of the analysis. It looks at the origin of the datasets, the topics they cover, the form they take, and what people searching for datasets are interested in. It also identifies gaps and possible future work to help make data more discoverable.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \nRequest: What is the paper about?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.We live in a data-driven world. Scientists, governments, journalists, commercial companies, and many others publish millions of datasets online. There are thousands of Web sites that publish datasets-some publish a handful, some publish hundreds of thousands [3] . Google's Dataset Search 1 is a search engine for datasets on the Web [16] . It relies on schema.org and similar open standards to extract the semantics of dataset metadata and to make it searchable.\nArguably, the mere existence of Dataset Search and its reliance on semantic markup provided a strong incentive for dataset providers to add such markup to their Web pages. Indeed, we have seen an explosive growth of dataset metadata on the Web since we started the work on Dataset Search. In the Fall of 2016, there were about 500K Web pages that included schema.org/Dataset markup, with half of them coming from data.gov, the US Open Government portal [10] . Today, there are tens of millions of such pages, from thousands of sites.\nA recent comprehensive survey highlights a variety of approaches to help users find datasets [3] : these approaches range from searching within a collection of tables with different schemas [14] , to finding data in repositories, such as Figshare, Zenodo, or DataDryad, to using metadata search engines, like Dataset Search. There are a number of well respected directories of dataset publishers (e.g., DataCite [19] , re3data [12] , Scientific Data in Nature [15] ), but they inevitably miss new datasets or repositories [4] . To the best of our knowledge, Dataset Search is the only collection of dataset metadata that includes all semantically annotated datasets on the Web.\nWe chose to rely primarily on schema.org for describing dataset metadata because both search engines and open-source tools have used it successfully to build an open ecosystem for various types of content [8] . In recent years, the scientific community has also embraced it for publishing data, by creating mappings from other metadata standards to schema.org. For example, Sansone and colleagues define a mapping from the DATS standard in the biomedical community to schema.org [20] . Wang and colleagues use schema.org to describe research-graph data, comprised of researchers, datasets and scholarly articles [24] . Efforts such as bioschemas.org [6] extend schema.org to include domain-specific terminology and relationships.\nIn this paper, we analyze the Dataset Search corpus of metadata. As of March 2020, the corpus contained 28 million datasets from more than 3,700 sites. While limited to the dataset metadata that is available in schema.org or DCAT, this corpus contains a sizable snapshot of the datasets on the Web. And because many researchers and scientists rely on search engines to find datasets [7] , learning from this corpus can inform both the work to improve search engines for datasets and, more important, highlight the gaps in representation and coverage for the community at large. Specifically, in this paper, we make the following contributions:\n-We present methods for analyzing an organically created corpus of metadata for 28 million datasets on the Web (Section 2). -We identify a set of research questions that such a corpus can help analyze and present results of the analysis of the corpus (Section 3). -We discuss lessons learned from the corpus analysis (Section 4).\nIn this section, we describe the methods that we used to collect the metadata and to prepare it for the analysis in Section 3. In the remainder of this paper, we abbreviate the schema.org namespace as so# and the DCAT namespace as dct#.\nWe described the details of the Dataset Search architecture elsewhere [16] . In brief, Dataset Search relies on the Google Web crawl to find pages that contain dataset metadata and to extract the corresponding triples. A post-processing of the Web crawl data parses RDFa, Microdata, and JSON-LD into a common graph data model, broadly equivalent to W3C's RDF triples [18] . We keep so#Dataset, dct#Dataset, and all the related entities and their properties.\nWe enhance, normalize, and augment this corpus in a variety of ways in order to provide users with a meaningful search experience. In this section, we focus only on those processing steps that are relevant to the subsequent data analysis. The processing happens at multiple levels of granularity: At the corpus level, we ensure that datasets are unique and attempt to remove non-datasets (i.e., pages that include dataset markup, but do not describe datasets). At the dataset level, we augment the metadata with inferred properties. Finally, at the property level, we clean up and normalize values.\nThe corpus-level analysis starts by removing duplicates within each site [16] . We found that many dataset repositories add markup both to the dataset landing pages and to the pages that list search results within that repository. We keep only the former in the corpus through simple heuristics: When the same dataset (according to values of key properties) appears on multiple pages, we keep the page that contains only one dataset. We also remove dataset metadata that does not have values for basic properties such as title and description [5] .\nAt the dataset level, we process the values for properties such as title and description as well as the terms on the Web page itself in order to identify the main topics covered by the dataset. We use the topics from re3data.org [12] and a similar set of topics from the Google Knowledge Graph [17] as our vocabulary.\nIn addition, our page-level analysis collects information from the Web page that the dataset originated from, such as the domain of the page and its language.\nFor individual properties, we normalize, clean, and reconcile values for:\n-Data downloads and formats: We identify the patterns that data providers use to represent download information and normalize them to a single representation [16] . Providers may specify file formats through the so#fileFormat or so#encodingType properties. When both of these properties are missing, we extract a file extension from the data-download URL. -DOIs and compact identifiers: Persistent citable identifiers, such as Digital Object Identifiers (DOIs) and Compact Identifiers [26] , may appear in several properties, such as so#identifier, so#url, or even so#sameAs, and so#alternateName. We use regular expressions to find patterns that correspond to these identifiers, and look for known prefixes from identifiers.org in all of these properties. -Level of access: Two properties define the level of access of a dataset: so#isAccessibleForFree is a boolean value that indicates whether or not the dataset requires a payment. so#license links to a license or specifies one inline. We normalize the license information into known classes of licenses, such as Creative Commons and open government licenses. Any license that allows redistribution essentially makes the dataset available for free. We count datasets with these licenses as well as datasets with so#isAccessibleForFree set to true as the datasets that are \"open.\" -Providers: There is some ambiguity in schema.org on how to specify the the source of a dataset. We use the so#publisher and so#provider properties to identify the organization that provided the dataset. As with other properties, the value may be a string or an Organization object. Wherever possible, we reconcile the organization to the corresponding entity in the Google Knowledge Graph. -Updated date: There are several date properties associated with a dataset: so#dateCreated, so#datePublished, so#dateModified (and similar properties in DCAT). There is little consistency in how dataset publishers distinguish between them. However, the most recent value across these dates is usually a reliable approximation on when a dataset was last updated. We use several parsers to understand dates expressed in common formats and to normalize them. If there is no date in the metadata, we use the date when the Web page itself was last updated as a proxy.\nFinally, to analyze the usage of datasets in the Dataset Search application, we look at logs for two weeks in May 2020. We extract the identifiers of the datasets that appeared in search results, and join them with their metadata to analyze search behavior in aggregate.\nAll the data and analyses in this paper are based on a snapshot of the Dataset Search corpus from March 26, 2020. We also compare the status of the corpus with a version from ten months prior, in June 2019.\nWhile we believe that our corpus is a reasonably representative snapshot of the datasets published on the Web, we recognize that it has limitations. Indeed, we have no way of measuring how well the corpus covers all the datasets available on the Web.\nFirst, the corpus contains only the datasets that have semantic descriptions of their metadata in schema.org or DCAT. If a dataset page does not have metadata in a machine-readable format and in a vocabulary that we recognize, it will not be in our corpus (and will not be discoverable in Dataset Search).\nSecond, if a dataset page is not accessible to the Google crawler or is not being crawled for some reason (e.g., because of robots.txt restrictions), it will not be in our corpus. When the crawler processes the page, it often needs to execute JavaScript to get the metadata. If a page is slow to render, we may not obtain dataset metadata from it.\nThird, our methods for inferring new values, such as dataset topics, may be imprecise, and we have not formally evaluated their accuracy yet.\nFourth, in our analysis, we \"trust\" what the metadata says. For instance, if a dataset's metadata says that the dataset is accessible for free, we count it among the open datasets. In some cases, the reality may be different when users try to download the dataset.\nFinally, a significant amount of pages on the Web are designed for Search Engine Optimization or are simply spam. A page may have so#Dataset on it but not actually describe any dataset metadata. While we do our best to weed out such pages, our techniques are not perfect, and we cannot be certain that all the datasets in the corpus that we describe are indeed datasets.\nWe start with the results of a corpus-level analysis (Section 3.1), then look at specific metadata properties (Section 3.2) and finally present our observations on the usage of datasets in Dataset Search (Section 3.3). \nLooking at the corpus as a whole, as well as characteristics of the Web pages that we extracted metadata from, enables us to answer the following questions. Which metadata fields are frequently populated, and which ones rarely have any values? These numbers give us probably the most actionable items in terms of improving metadata quality.\nThe snapshot that we analyze in the rest of this section, taken on March 26, 2020, contains 28M datasets from 3,700 domains. The number of datasets per domain mostly follows a power law distribution, as the logarithmic scale plot in Figure 1a shows: A small number of domains publish millions or hundreds of thousands of datasets, while the long tail of domains hosts just a handful of datasets. The two domains with the largest number of datasets (ceicdata.com and data.gov) have more than 3 million datasets each. The ten largest domains (Figure 1b ) account for 65% of all datasets.\nWhile \"typical\" Web pages about datasets correspond to a single dataset, some pages may have multiple datasets on them. For instance, a page may describe a large dataset and break down its components as multiple datasets; or a page may be dynamically generated in response to a search in a dataset repository. In our corpus, we found that over 90% of datasets come from pages that contain exactly one dataset. Still, more than 1.6M datasets come from pages with more than ten datasets.\nTable 1 shows the distribution of datasets by top-level internet domains. The vast majority of the datasets come from .com domains, but both .org and government domains are well represented. For the country-specific domains, Austria, Spain, Germany, and France are at the top of the list. If we combine all government domains across the world (.gov, .gouv.* , .gv.*, .gov.*, .gob.*, etc.), we find 3.7M datasets on these government domains.\nTo get a more complete picture of the international coverage of datasets, Table 2 breaks them down by language, as specified by or extracted from the Web pages that contain them. More than 18M datasets, or 64% are in English, followed by \nEnd paper context": "The paper is about analyzing a corpus of 28 million datasets from the Web, which is the largest and most diverse of its kind. It discusses the origin of the datasets, the topics they cover, the form they take, and what people searching for datasets are interested in. The paper also presents methods for analyzing the corpus and discusses the gaps in representation and coverage for the dataset community.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \nRequest: What are the limitations presented by them?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.We live in a data-driven world. Scientists, governments, journalists, commercial companies, and many others publish millions of datasets online. There are thousands of Web sites that publish datasets-some publish a handful, some publish hundreds of thousands [3] . Google's Dataset Search 1 is a search engine for datasets on the Web [16] . It relies on schema.org and similar open standards to extract the semantics of dataset metadata and to make it searchable.\nArguably, the mere existence of Dataset Search and its reliance on semantic markup provided a strong incentive for dataset providers to add such markup to their Web pages. Indeed, we have seen an explosive growth of dataset metadata on the Web since we started the work on Dataset Search. In the Fall of 2016, there were about 500K Web pages that included schema.org/Dataset markup, with half of them coming from data.gov, the US Open Government portal [10] . Today, there are tens of millions of such pages, from thousands of sites.\nA recent comprehensive survey highlights a variety of approaches to help users find datasets [3] : these approaches range from searching within a collection of tables with different schemas [14] , to finding data in repositories, such as Figshare, Zenodo, or DataDryad, to using metadata search engines, like Dataset Search. There are a number of well respected directories of dataset publishers (e.g., DataCite [19] , re3data [12] , Scientific Data in Nature [15] ), but they inevitably miss new datasets or repositories [4] . To the best of our knowledge, Dataset Search is the only collection of dataset metadata that includes all semantically annotated datasets on the Web.\nWe chose to rely primarily on schema.org for describing dataset metadata because both search engines and open-source tools have used it successfully to build an open ecosystem for various types of content [8] . In recent years, the scientific community has also embraced it for publishing data, by creating mappings from other metadata standards to schema.org. For example, Sansone and colleagues define a mapping from the DATS standard in the biomedical community to schema.org [20] . Wang and colleagues use schema.org to describe research-graph data, comprised of researchers, datasets and scholarly articles [24] . Efforts such as bioschemas.org [6] extend schema.org to include domain-specific terminology and relationships.\nIn this paper, we analyze the Dataset Search corpus of metadata. As of March 2020, the corpus contained 28 million datasets from more than 3,700 sites. While limited to the dataset metadata that is available in schema.org or DCAT, this corpus contains a sizable snapshot of the datasets on the Web. And because many researchers and scientists rely on search engines to find datasets [7] , learning from this corpus can inform both the work to improve search engines for datasets and, more important, highlight the gaps in representation and coverage for the community at large. Specifically, in this paper, we make the following contributions:\n-We present methods for analyzing an organically created corpus of metadata for 28 million datasets on the Web (Section 2). -We identify a set of research questions that such a corpus can help analyze and present results of the analysis of the corpus (Section 3). -We discuss lessons learned from the corpus analysis (Section 4).\nIn this section, we describe the methods that we used to collect the metadata and to prepare it for the analysis in Section 3. In the remainder of this paper, we abbreviate the schema.org namespace as so# and the DCAT namespace as dct#.\nWe described the details of the Dataset Search architecture elsewhere [16] . In brief, Dataset Search relies on the Google Web crawl to find pages that contain dataset metadata and to extract the corresponding triples. A post-processing of the Web crawl data parses RDFa, Microdata, and JSON-LD into a common graph data model, broadly equivalent to W3C's RDF triples [18] . We keep so#Dataset, dct#Dataset, and all the related entities and their properties.\nWe enhance, normalize, and augment this corpus in a variety of ways in order to provide users with a meaningful search experience. In this section, we focus only on those processing steps that are relevant to the subsequent data analysis. The processing happens at multiple levels of granularity: At the corpus level, we ensure that datasets are unique and attempt to remove non-datasets (i.e., pages that include dataset markup, but do not describe datasets). At the dataset level, we augment the metadata with inferred properties. Finally, at the property level, we clean up and normalize values.\nThe corpus-level analysis starts by removing duplicates within each site [16] . We found that many dataset repositories add markup both to the dataset landing pages and to the pages that list search results within that repository. We keep only the former in the corpus through simple heuristics: When the same dataset (according to values of key properties) appears on multiple pages, we keep the page that contains only one dataset. We also remove dataset metadata that does not have values for basic properties such as title and description [5] .\nAt the dataset level, we process the values for properties such as title and description as well as the terms on the Web page itself in order to identify the main topics covered by the dataset. We use the topics from re3data.org [12] and a similar set of topics from the Google Knowledge Graph [17] as our vocabulary.\nIn addition, our page-level analysis collects information from the Web page that the dataset originated from, such as the domain of the page and its language.\nFor individual properties, we normalize, clean, and reconcile values for:\n-Data downloads and formats: We identify the patterns that data providers use to represent download information and normalize them to a single representation [16] . Providers may specify file formats through the so#fileFormat or so#encodingType properties. When both of these properties are missing, we extract a file extension from the data-download URL. -DOIs and compact identifiers: Persistent citable identifiers, such as Digital Object Identifiers (DOIs) and Compact Identifiers [26] , may appear in several properties, such as so#identifier, so#url, or even so#sameAs, and so#alternateName. We use regular expressions to find patterns that correspond to these identifiers, and look for known prefixes from identifiers.org in all of these properties. -Level of access: Two properties define the level of access of a dataset: so#isAccessibleForFree is a boolean value that indicates whether or not the dataset requires a payment. so#license links to a license or specifies one inline. We normalize the license information into known classes of licenses, such as Creative Commons and open government licenses. Any license that allows redistribution essentially makes the dataset available for free. We count datasets with these licenses as well as datasets with so#isAccessibleForFree set to true as the datasets that are \"open.\" -Providers: There is some ambiguity in schema.org on how to specify the the source of a dataset. We use the so#publisher and so#provider properties to identify the organization that provided the dataset. As with other properties, the value may be a string or an Organization object. Wherever possible, we reconcile the organization to the corresponding entity in the Google Knowledge Graph. -Updated date: There are several date properties associated with a dataset: so#dateCreated, so#datePublished, so#dateModified (and similar properties in DCAT). There is little consistency in how dataset publishers distinguish between them. However, the most recent value across these dates is usually a reliable approximation on when a dataset was last updated. We use several parsers to understand dates expressed in common formats and to normalize them. If there is no date in the metadata, we use the date when the Web page itself was last updated as a proxy.\nFinally, to analyze the usage of datasets in the Dataset Search application, we look at logs for two weeks in May 2020. We extract the identifiers of the datasets that appeared in search results, and join them with their metadata to analyze search behavior in aggregate.\nAll the data and analyses in this paper are based on a snapshot of the Dataset Search corpus from March 26, 2020. We also compare the status of the corpus with a version from ten months prior, in June 2019.\nWhile we believe that our corpus is a reasonably representative snapshot of the datasets published on the Web, we recognize that it has limitations. Indeed, we have no way of measuring how well the corpus covers all the datasets available on the Web.\nFirst, the corpus contains only the datasets that have semantic descriptions of their metadata in schema.org or DCAT. If a dataset page does not have metadata in a machine-readable format and in a vocabulary that we recognize, it will not be in our corpus (and will not be discoverable in Dataset Search).\nSecond, if a dataset page is not accessible to the Google crawler or is not being crawled for some reason (e.g., because of robots.txt restrictions), it will not be in our corpus. When the crawler processes the page, it often needs to execute JavaScript to get the metadata. If a page is slow to render, we may not obtain dataset metadata from it.\nThird, our methods for inferring new values, such as dataset topics, may be imprecise, and we have not formally evaluated their accuracy yet.\nFourth, in our analysis, we \"trust\" what the metadata says. For instance, if a dataset's metadata says that the dataset is accessible for free, we count it among the open datasets. In some cases, the reality may be different when users try to download the dataset.\nFinally, a significant amount of pages on the Web are designed for Search Engine Optimization or are simply spam. A page may have so#Dataset on it but not actually describe any dataset metadata. While we do our best to weed out such pages, our techniques are not perfect, and we cannot be certain that all the datasets in the corpus that we describe are indeed datasets.\nWe start with the results of a corpus-level analysis (Section 3.1), then look at specific metadata properties (Section 3.2) and finally present our observations on the usage of datasets in Dataset Search (Section 3.3). \nLooking at the corpus as a whole, as well as characteristics of the Web pages that we extracted metadata from, enables us to answer the following questions. Which metadata fields are frequently populated, and which ones rarely have any values? These numbers give us probably the most actionable items in terms of improving metadata quality.\nThe snapshot that we analyze in the rest of this section, taken on March 26, 2020, contains 28M datasets from 3,700 domains. The number of datasets per domain mostly follows a power law distribution, as the logarithmic scale plot in Figure 1a shows: A small number of domains publish millions or hundreds of thousands of datasets, while the long tail of domains hosts just a handful of datasets. The two domains with the largest number of datasets (ceicdata.com and data.gov) have more than 3 million datasets each. The ten largest domains (Figure 1b ) account for 65% of all datasets.\nWhile \"typical\" Web pages about datasets correspond to a single dataset, some pages may have multiple datasets on them. For instance, a page may describe a large dataset and break down its components as multiple datasets; or a page may be dynamically generated in response to a search in a dataset repository. In our corpus, we found that over 90% of datasets come from pages that contain exactly one dataset. Still, more than 1.6M datasets come from pages with more than ten datasets.\nTable 1 shows the distribution of datasets by top-level internet domains. The vast majority of the datasets come from .com domains, but both .org and government domains are well represented. For the country-specific domains, Austria, Spain, Germany, and France are at the top of the list. If we combine all government domains across the world (.gov, .gouv.* , .gv.*, .gov.*, .gob.*, etc.), we find 3.7M datasets on these government domains.\nTo get a more complete picture of the international coverage of datasets, Table 2 breaks them down by language, as specified by or extracted from the Web pages that contain them. More than 18M datasets, or 64% are in Engli\nEnd paper context": "The limitations presented by the paper context are that the corpus only contains datasets that have semantic descriptions of their metadata in schema.org or DCAT, the corpus may not include datasets that are not accessible to the Google crawler, the methods for inferring new values may be imprecise, the metadata may not accurately reflect the reality of the dataset, and the corpus may contain pages that are designed for Search Engine Optimization or are simply spam.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \nRequest: Just that?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.We live in a data-driven world. Scientists, governments, journalists, commercial companies, and many others publish millions of datasets online. There are thousands of Web sites that publish datasets-some publish a handful, some publish hundreds of thousands [3] . Google's Dataset Search 1 is a search engine for datasets on the Web [16] . It relies on schema.org and similar open standards to extract the semantics of dataset metadata and to make it searchable.\nArguably, the mere existence of Dataset Search and its reliance on semantic markup provided a strong incentive for dataset providers to add such markup to their Web pages. Indeed, we have seen an explosive growth of dataset metadata on the Web since we started the work on Dataset Search. In the Fall of 2016, there were about 500K Web pages that included schema.org/Dataset markup, with half of them coming from data.gov, the US Open Government portal [10] . Today, there are tens of millions of such pages, from thousands of sites.\nA recent comprehensive survey highlights a variety of approaches to help users find datasets [3] : these approaches range from searching within a collection of tables with different schemas [14] , to finding data in repositories, such as Figshare, Zenodo, or DataDryad, to using metadata search engines, like Dataset Search. There are a number of well respected directories of dataset publishers (e.g., DataCite [19] , re3data [12] , Scientific Data in Nature [15] ), but they inevitably miss new datasets or repositories [4] . To the best of our knowledge, Dataset Search is the only collection of dataset metadata that includes all semantically annotated datasets on the Web.\nWe chose to rely primarily on schema.org for describing dataset metadata because both search engines and open-source tools have used it successfully to build an open ecosystem for various types of content [8] . In recent years, the scientific community has also embraced it for publishing data, by creating mappings from other metadata standards to schema.org. For example, Sansone and colleagues define a mapping from the DATS standard in the biomedical community to schema.org [20] . Wang and colleagues use schema.org to describe research-graph data, comprised of researchers, datasets and scholarly articles [24] . Efforts such as bioschemas.org [6] extend schema.org to include domain-specific terminology and relationships.\nIn this paper, we analyze the Dataset Search corpus of metadata. As of March 2020, the corpus contained 28 million datasets from more than 3,700 sites. While limited to the dataset metadata that is available in schema.org or DCAT, this corpus contains a sizable snapshot of the datasets on the Web. And because many researchers and scientists rely on search engines to find datasets [7] , learning from this corpus can inform both the work to improve search engines for datasets and, more important, highlight the gaps in representation and coverage for the community at large. Specifically, in this paper, we make the following contributions:\n-We present methods for analyzing an organically created corpus of metadata for 28 million datasets on the Web (Section 2). -We identify a set of research questions that such a corpus can help analyze and present results of the analysis of the corpus (Section 3). -We discuss lessons learned from the corpus analysis (Section 4).\nIn this section, we describe the methods that we used to collect the metadata and to prepare it for the analysis in Section 3. In the remainder of this paper, we abbreviate the schema.org namespace as so# and the DCAT namespace as dct#.\nWe described the details of the Dataset Search architecture elsewhere [16] . In brief, Dataset Search relies on the Google Web crawl to find pages that contain dataset metadata and to extract the corresponding triples. A post-processing of the Web crawl data parses RDFa, Microdata, and JSON-LD into a common graph data model, broadly equivalent to W3C's RDF triples [18] . We keep so#Dataset, dct#Dataset, and all the related entities and their properties.\nWe enhance, normalize, and augment this corpus in a variety of ways in order to provide users with a meaningful search experience. In this section, we focus only on those processing steps that are relevant to the subsequent data analysis. The processing happens at multiple levels of granularity: At the corpus level, we ensure that datasets are unique and attempt to remove non-datasets (i.e., pages that include dataset markup, but do not describe datasets). At the dataset level, we augment the metadata with inferred properties. Finally, at the property level, we clean up and normalize values.\nThe corpus-level analysis starts by removing duplicates within each site [16] . We found that many dataset repositories add markup both to the dataset landing pages and to the pages that list search results within that repository. We keep only the former in the corpus through simple heuristics: When the same dataset (according to values of key properties) appears on multiple pages, we keep the page that contains only one dataset. We also remove dataset metadata that does not have values for basic properties such as title and description [5] .\nAt the dataset level, we process the values for properties such as title and description as well as the terms on the Web page itself in order to identify the main topics covered by the dataset. We use the topics from re3data.org [12] and a similar set of topics from the Google Knowledge Graph [17] as our vocabulary.\nIn addition, our page-level analysis collects information from the Web page that the dataset originated from, such as the domain of the page and its language.\nFor individual properties, we normalize, clean, and reconcile values for:\n-Data downloads and formats: We identify the patterns that data providers use to represent download information and normalize them to a single representation [16] . Providers may specify file formats through the so#fileFormat or so#encodingType properties. When both of these properties are missing, we extract a file extension from the data-download URL. -DOIs and compact identifiers: Persistent citable identifiers, such as Digital Object Identifiers (DOIs) and Compact Identifiers [26] , may appear in several properties, such as so#identifier, so#url, or even so#sameAs, and so#alternateName. We use regular expressions to find patterns that correspond to these identifiers, and look for known prefixes from identifiers.org in all of these properties. -Level of access: Two properties define the level of access of a dataset: so#isAccessibleForFree is a boolean value that indicates whether or not the dataset requires a payment. so#license links to a license or specifies one inline. We normalize the license information into known classes of licenses, such as Creative Commons and open government licenses. Any license that allows redistribution essentially makes the dataset available for free. We count datasets with these licenses as well as datasets with so#isAccessibleForFree set to true as the datasets that are \"open.\" -Providers: There is some ambiguity in schema.org on how to specify the the source of a dataset. We use the so#publisher and so#provider properties to identify the organization that provided the dataset. As with other properties, the value may be a string or an Organization object. Wherever possible, we reconcile the organization to the corresponding entity in the Google Knowledge Graph. -Updated date: There are several date properties associated with a dataset: so#dateCreated, so#datePublished, so#dateModified (and similar properties in DCAT). There is little consistency in how dataset publishers distinguish between them. However, the most recent value across these dates is usually a reliable approximation on when a dataset was last updated. We use several parsers to understand dates expressed in common formats and to normalize them. If there is no date in the metadata, we use the date when the Web page itself was last updated as a proxy.\nFinally, to analyze the usage of datasets in the Dataset Search application, we look at logs for two weeks in May 2020. We extract the identifiers of the datasets that appeared in search results, and join them with their metadata to analyze search behavior in aggregate.\nAll the data and analyses in this paper are based on a snapshot of the Dataset Search corpus from March 26, 2020. We also compare the status of the corpus with a version from ten months prior, in June 2019.\nWhile we believe that our corpus is a reasonably representative snapshot of the datasets published on the Web, we recognize that it has limitations. Indeed, we have no way of measuring how well the corpus covers all the datasets available on the Web.\nFirst, the corpus contains only the datasets that have semantic descriptions of their metadata in schema.org or DCAT. If a dataset page does not have metadata in a machine-readable format and in a vocabulary that we recognize, it will not be in our corpus (and will not be discoverable in Dataset Search).\nSecond, if a dataset page is not accessible to the Google crawler or is not being crawled for some reason (e.g., because of robots.txt restrictions), it will not be in our corpus. When the crawler processes the page, it often needs to execute JavaScript to get the metadata. If a page is slow to render, we may not obtain dataset metadata from it.\nThird, our methods for inferring new values, such as dataset topics, may be imprecise, and we have not formally evaluated their accuracy yet.\nFourth, in our analysis, we \"trust\" what the metadata says. For instance, if a dataset's metadata says that the dataset is accessible for free, we count it among the open datasets. In some cases, the reality may be different when users try to download the dataset.\nFinally, a significant amount of pages on the Web are designed for Search Engine Optimization or are simply spam. A page may have so#Dataset on it but not actually describe any dataset metadata. While we do our best to weed out such pages, our techniques are not perfect, and we cannot be certain that all the datasets in the corpus that we describe are indeed datasets.\nWe start with the results of a corpus-level analysis (Section 3.1), then look at specific metadata properties (Section 3.2) and finally present our observations on the usage of datasets in Dataset Search (Section 3.3). \nLooking at the corpus as a whole, as well as characteristics of the Web pages that we extracted metadata from, enables us to answer the following questions. Which metadata fields are frequently populated, and which ones rarely have any values? These numbers give us probably the most actionable items in terms of improving metadata quality.\nThe snapshot that we analyze in the rest of this section, taken on March 26, 2020, contains 28M datasets from 3,700 domains. The number of datasets per domain mostly follows a power law distribution, as the logarithmic scale plot in Figure 1a shows: A small number of domains publish millions or hundreds of thousands of datasets, while the long tail of domains hosts just a handful of datasets. The two domains with the largest number of datasets (ceicdata.com and data.gov) have more than 3 million datasets each. The ten largest domains (Figure 1b ) account for 65% of all datasets.\nWhile \"typical\" Web pages about datasets correspond to a single dataset, some pages may have multiple datasets on them. For instance, a page may describe a large dataset and break down its components as multiple datasets; or a page may be dynamically generated in response to a search in a dataset repository. In our corpus, we found that over 90% of datasets come from pages that contain exactly one dataset. Still, more than 1.6M datasets come from pages with more than ten datasets.\nTable 1 shows the distribution of datasets by top-level internet domains. The vast majority of the datasets come from .com domains, but both .org and government domains are well represented. For the country-specific domains, Austria, Spain, Germany, and France are at the top of the list. If we combine all government domains across the world (.gov, .gouv.* , .gv.*, .gov.*, .gob.*, etc.), we find 3.7M datasets on these government domains.\nTo get a more complete picture of the international coverage of datasets, Table 2 breaks them down by language, as specified by or extracted from the Web pages that contain them. More than 18M datasets, or 64% are in English, followed by datasets in Chin\nEnd paper context": "No, there is more to the paper than that.", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". \nRequest: What is your name?\nStart paper context\nScientists, governments, and companies increasingly publish datasets on the Web. Google's Dataset Search extracts dataset metadataexpressed using schema.org and similar vocabularies-from Web pages in order to make datasets discoverable. Since we started the work on Dataset Search in 2016, the number of datasets described in schema.org has grown from about 500K to almost 30M. Thus, this corpus has become a valuable snapshot of data on the Web. To the best of our knowledge, this corpus is the largest and most diverse of its kind. We analyze this corpus and discuss where the datasets originate from, what topics they cover, which form they take, and what people searching for datasets are interested in. Based on this analysis, we identify gaps and possible future work to help make data more discoverable.We live in a data-driven world. Scientists, governments, journalists, commercial companies, and many others publish millions of datasets online. There are thousands of Web sites that publish datasets-some publish a handful, some publish hundreds of thousands [3] . Google's Dataset Search 1 is a search engine for datasets on the Web [16] . It relies on schema.org and similar open standards to extract the semantics of dataset metadata and to make it searchable.\nArguably, the mere existence of Dataset Search and its reliance on semantic markup provided a strong incentive for dataset providers to add such markup to their Web pages. Indeed, we have seen an explosive growth of dataset metadata on the Web since we started the work on Dataset Search. In the Fall of 2016, there were about 500K Web pages that included schema.org/Dataset markup, with half of them coming from data.gov, the US Open Government portal [10] . Today, there are tens of millions of such pages, from thousands of sites.\nA recent comprehensive survey highlights a variety of approaches to help users find datasets [3] : these approaches range from searching within a collection of tables with different schemas [14] , to finding data in repositories, such as Figshare, Zenodo, or DataDryad, to using metadata search engines, like Dataset Search. There are a number of well respected directories of dataset publishers (e.g., DataCite [19] , re3data [12] , Scientific Data in Nature [15] ), but they inevitably miss new datasets or repositories [4] . To the best of our knowledge, Dataset Search is the only collection of dataset metadata that includes all semantically annotated datasets on the Web.\nWe chose to rely primarily on schema.org for describing dataset metadata because both search engines and open-source tools have used it successfully to build an open ecosystem for various types of content [8] . In recent years, the scientific community has also embraced it for publishing data, by creating mappings from other metadata standards to schema.org. For example, Sansone and colleagues define a mapping from the DATS standard in the biomedical community to schema.org [20] . Wang and colleagues use schema.org to describe research-graph data, comprised of researchers, datasets and scholarly articles [24] . Efforts such as bioschemas.org [6] extend schema.org to include domain-specific terminology and relationships.\nIn this paper, we analyze the Dataset Search corpus of metadata. As of March 2020, the corpus contained 28 million datasets from more than 3,700 sites. While limited to the dataset metadata that is available in schema.org or DCAT, this corpus contains a sizable snapshot of the datasets on the Web. And because many researchers and scientists rely on search engines to find datasets [7] , learning from this corpus can inform both the work to improve search engines for datasets and, more important, highlight the gaps in representation and coverage for the community at large. Specifically, in this paper, we make the following contributions:\n-We present methods for analyzing an organically created corpus of metadata for 28 million datasets on the Web (Section 2). -We identify a set of research questions that such a corpus can help analyze and present results of the analysis of the corpus (Section 3). -We discuss lessons learned from the corpus analysis (Section 4).\nIn this section, we describe the methods that we used to collect the metadata and to prepare it for the analysis in Section 3. In the remainder of this paper, we abbreviate the schema.org namespace as so# and the DCAT namespace as dct#.\nWe described the details of the Dataset Search architecture elsewhere [16] . In brief, Dataset Search relies on the Google Web crawl to find pages that contain dataset metadata and to extract the corresponding triples. A post-processing of the Web crawl data parses RDFa, Microdata, and JSON-LD into a common graph data model, broadly equivalent to W3C's RDF triples [18] . We keep so#Dataset, dct#Dataset, and all the related entities and their properties.\nWe enhance, normalize, and augment this corpus in a variety of ways in order to provide users with a meaningful search experience. In this section, we focus only on those processing steps that are relevant to the subsequent data analysis. The processing happens at multiple levels of granularity: At the corpus level, we ensure that datasets are unique and attempt to remove non-datasets (i.e., pages that include dataset markup, but do not describe datasets). At the dataset level, we augment the metadata with inferred properties. Finally, at the property level, we clean up and normalize values.\nThe corpus-level analysis starts by removing duplicates within each site [16] . We found that many dataset repositories add markup both to the dataset landing pages and to the pages that list search results within that repository. We keep only the former in the corpus through simple heuristics: When the same dataset (according to values of key properties) appears on multiple pages, we keep the page that contains only one dataset. We also remove dataset metadata that does not have values for basic properties such as title and description [5] .\nAt the dataset level, we process the values for properties such as title and description as well as the terms on the Web page itself in order to identify the main topics covered by the dataset. We use the topics from re3data.org [12] and a similar set of topics from the Google Knowledge Graph [17] as our vocabulary.\nIn addition, our page-level analysis collects information from the Web page that the dataset originated from, such as the domain of the page and its language.\nFor individual properties, we normalize, clean, and reconcile values for:\n-Data downloads and formats: We identify the patterns that data providers use to represent download information and normalize them to a single representation [16] . Providers may specify file formats through the so#fileFormat or so#encodingType properties. When both of these properties are missing, we extract a file extension from the data-download URL. -DOIs and compact identifiers: Persistent citable identifiers, such as Digital Object Identifiers (DOIs) and Compact Identifiers [26] , may appear in several properties, such as so#identifier, so#url, or even so#sameAs, and so#alternateName. We use regular expressions to find patterns that correspond to these identifiers, and look for known prefixes from identifiers.org in all of these properties. -Level of access: Two properties define the level of access of a dataset: so#isAccessibleForFree is a boolean value that indicates whether or not the dataset requires a payment. so#license links to a license or specifies one inline. We normalize the license information into known classes of licenses, such as Creative Commons and open government licenses. Any license that allows redistribution essentially makes the dataset available for free. We count datasets with these licenses as well as datasets with so#isAccessibleForFree set to true as the datasets that are \"open.\" -Providers: There is some ambiguity in schema.org on how to specify the the source of a dataset. We use the so#publisher and so#provider properties to identify the organization that provided the dataset. As with other properties, the value may be a string or an Organization object. Wherever possible, we reconcile the organization to the corresponding entity in the Google Knowledge Graph. -Updated date: There are several date properties associated with a dataset: so#dateCreated, so#datePublished, so#dateModified (and similar properties in DCAT). There is little consistency in how dataset publishers distinguish between them. However, the most recent value across these dates is usually a reliable approximation on when a dataset was last updated. We use several parsers to understand dates expressed in common formats and to normalize them. If there is no date in the metadata, we use the date when the Web page itself was last updated as a proxy.\nFinally, to analyze the usage of datasets in the Dataset Search application, we look at logs for two weeks in May 2020. We extract the identifiers of the datasets that appeared in search results, and join them with their metadata to analyze search behavior in aggregate.\nAll the data and analyses in this paper are based on a snapshot of the Dataset Search corpus from March 26, 2020. We also compare the status of the corpus with a version from ten months prior, in June 2019.\nWhile we believe that our corpus is a reasonably representative snapshot of the datasets published on the Web, we recognize that it has limitations. Indeed, we have no way of measuring how well the corpus covers all the datasets available on the Web.\nFirst, the corpus contains only the datasets that have semantic descriptions of their metadata in schema.org or DCAT. If a dataset page does not have metadata in a machine-readable format and in a vocabulary that we recognize, it will not be in our corpus (and will not be discoverable in Dataset Search).\nSecond, if a dataset page is not accessible to the Google crawler or is not being crawled for some reason (e.g., because of robots.txt restrictions), it will not be in our corpus. When the crawler processes the page, it often needs to execute JavaScript to get the metadata. If a page is slow to render, we may not obtain dataset metadata from it.\nThird, our methods for inferring new values, such as dataset topics, may be imprecise, and we have not formally evaluated their accuracy yet.\nFourth, in our analysis, we \"trust\" what the metadata says. For instance, if a dataset's metadata says that the dataset is accessible for free, we count it among the open datasets. In some cases, the reality may be different when users try to download the dataset.\nFinally, a significant amount of pages on the Web are designed for Search Engine Optimization or are simply spam. A page may have so#Dataset on it but not actually describe any dataset metadata. While we do our best to weed out such pages, our techniques are not perfect, and we cannot be certain that all the datasets in the corpus that we describe are indeed datasets.\nWe start with the results of a corpus-level analysis (Section 3.1), then look at specific metadata properties (Section 3.2) and finally present our observations on the usage of datasets in Dataset Search (Section 3.3). \nLooking at the corpus as a whole, as well as characteristics of the Web pages that we extracted metadata from, enables us to answer the following questions. Which metadata fields are frequently populated, and which ones rarely have any values? These numbers give us probably the most actionable items in terms of improving metadata quality.\nThe snapshot that we analyze in the rest of this section, taken on March 26, 2020, contains 28M datasets from 3,700 domains. The number of datasets per domain mostly follows a power law distribution, as the logarithmic scale plot in Figure 1a shows: A small number of domains publish millions or hundreds of thousands of datasets, while the long tail of domains hosts just a handful of datasets. The two domains with the largest number of datasets (ceicdata.com and data.gov) have more than 3 million datasets each. The ten largest domains (Figure 1b ) account for 65% of all datasets.\nWhile \"typical\" Web pages about datasets correspond to a single dataset, some pages may have multiple datasets on them. For instance, a page may describe a large dataset and break down its components as multiple datasets; or a page may be dynamically generated in response to a search in a dataset repository. In our corpus, we found that over 90% of datasets come from pages that contain exactly one dataset. Still, more than 1.6M datasets come from pages with more than ten datasets.\nTable 1 shows the distribution of datasets by top-level internet domains. The vast majority of the datasets come from .com domains, but both .org and government domains are well represented. For the country-specific domains, Austria, Spain, Germany, and France are at the top of the list. If we combine all government domains across the world (.gov, .gouv.* , .gv.*, .gov.*, .gob.*, etc.), we find 3.7M datasets on these government domains.\nTo get a more complete picture of the international coverage of datasets, Table 2 breaks them down by language, as specified by or extracted from the Web pages that contain them. More than 18M datasets, or 64% are in English, followed by datase\nEnd paper context": "My name is not mentioned in the paper context."}