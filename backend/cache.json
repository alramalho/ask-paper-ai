{"Please answer the following request, denoted by \"Request:\" in the best way possible with the given context that bounded by \"Start context\" and \"End context\".Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the charactersitics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characterstics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart context\nThe emergence of deep learning has considerably advanced the state-of-the-art in cardiac magnetic resonance (CMR) segmentation. Many techniques have been proposed over the last few years, bringing the accuracy of automated segmentation close to human performance. However, these models have been all too often trained and validated using cardiac imaging samples from single clinical centres or homogeneous imaging protocols. This has prevented the development and validation of models that are generalizable across different clinical centres, imaging conditions or scanner vendors. To promote further research and scientific benchmarking in the field of generalizable deep learning for cardiac segmentation, this paper presents the results of the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge, which was recently organized as part of the MICCAI 2020 Conference. A total of 14 teams submitted different solutions to the problem, combining various baseline models, data augmentation strategies, and domain adaptation techniques. The obtained results indicate the importance of intensity-driven data augmentation, as well as the need for further research to improve generalizability towards unseen scanner vendors or new imaging protocols. Furthermore, we present a new resource of 375 heterogeneous CMR datasets acquired by using four different scanner vendors in six hospitals and three different countries (Spain, Canada and Germany), which we provide as open-access for the community to enable future research in the field.A total of six clinical centres from Spain, Canada and Germany (numbered to 6 in this work) contributed to this III  AVERAGE SPECIFICATIONS FOR THE IMAGES ACQUIRED IN THE DIFFERENT CENTRES challenge by providing a different number of CMR studies from different scanner vendors, as detailed in Table I . In total, 375 studies were included in this challenge. The subjects considered for this multi-disease study were selected among groups of various cardiovascular diseases, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, coronary heart disease, abnormal right ventricle, myocarditis and ischemic cardiomyopathy as well as healthy volunteers (see Table II for more details on the distribution of these cases). The specific scanner manufacturers are: 1) Siemens (Siemens Healthineers, Germany), 2) Philips (Philips Healthcare, Netherlands), 3) General Electric (GE, GE Healthcare, USA) and 4) Canon (Canon Inc., Japan). These four manufacturers were coded as A, B, C and D during the challenge, respectively. The CMR images derived from these four vendors are illustrated in Fig. 1 . More specific details on the studies are given in Table III .\nEvery CMR study was annotated manually by an expert clinician from the centre of origin, with experiences ranging from 3 to more than 10 years. Following the clinical protocol, short-axis views were annotated at the end-diastolic (ED) and end-systolic (ES) phases, as they correspond to the phases used to compute the relevant clinical biomarkers for cardiac diagnosis and follow-up. Three main regions were considered: the left and right ventricle (LV and RV, respectively) cavities and the left ventricle myocardium (MYO). In order to reduce the inter-observer and inter-centre variability in the contours, in particular at the apical and basal regions, a detailed revision of the provided segmentations was performed by four researchers in pairs. They applied the same SOP across all CMR datasets to obtain the final ground truth. To generate consistent annotations for the research community, we chose to apply the SOP that was already used by the ACDC challenge, as follows:\na) The LV and RV cavities must be completely covered, including the papillary muscles. b) No interpolation of the MYO boundaries must be performed at the basal region. c) The RV must have a larger surface at the ED time-frame compared to ES. d) The RV does not include the pulmonary artery. Clinical delineations as well as later corrections were performed using CVI42 software (Circle Cardiovascular Imaging Inc., Calgary, Alberta, Canada). All studies were provided in DICOM format and contours were extracted in cvi42 workspace format (.cvi42ws). An in-house software was then used to extract the contours and transform the images into the NIFTI format, representing the final files delivered to the challenge participants.\nAll participants in the challenge (except P11) used some form of data augmentation to enhance their models. Specifically, two families of data augmentations were considered: (1) spatial transformations to increase sample size through rotation, flipping, scaling or deformation of the original images; (2) intensity-driven techniques, which maintain the spatial configuration of the anatomical structures but modify their image appearance. The second type of augmentation seems particularly relevant for the M&Ms as it may increase the variability in image appearance, with the hypothesis that this may lead to improved adaptation to varying imaging protocols and scanner vendors. Two teams performed data augmentation using only spatial transformations (P4, P6). Eleven teams additionally implemented intensity-based transformations using one of two main approaches: (i) standard image transformations such as histogram matching, blurring, change in brightness, gamma and contrast, or addition of Gaussian noise (P1-P3, P7-P8, P10, P13) (see 3 for a visualization of a subset of these transformations on a training slice); (ii) advanced image synthesis by using generative adversarial networks (GANs) (P5, P8, P14) or variational auto-encoders (VAE) (P12). For the latter one, the generation of synthetic images for the unseen vendor D is not feasible since it was not included in the training. Note that the majority of the teams participating in the challenge (10 out of 14) relied solely on data augmentation of the training sample to address the domain-shift problem posed by the M&Ms challenge.\nAdditionally, some teams (P1-P3, P9, P13) applied testtime augmentation techniques, which consist of passing to the model two or more transformed versions of the same inference image to obtain several predictions. These predictions are then combined to obtain one final outcome, usually by averaging them. This method has been shown to improve the final performance in small data size scenarios and a net improvement with a scale effect that depends on the model architecture [38] .\nEnd context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| M&Ms Challenge | 375 studies | Various cardiovascular diseases, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, coronary heart disease, abnormal right ventricle, myocarditis and ischemic cardiomyopathy as well as healthy volunteers | Spain, Canada and Germany | https://www.sciencedirect.com/science/article/pii/S1361841520302676 |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nWe present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases. When did the current expedition to the ISS launch? On March 30th 2022, with NASA astronaut Thomas Marshburn taking over as ISS commander. User Sparrow What do they do up there? User The expedition began upon the departure of Soyuz MS-19 on 30 March 2022, with NASA astronaut Thomas Marshburn taking over as ISS commander. Initially, the expedition consisted of Marshburn and his three SpaceX Crew-3 crewmates Raja Chari, Kayla Barron and Matthias Maurer, as well as Roscosmos cosmonauts Oleg Artemyev, Denis Matveev and Sergey Korsakov, who launched aboard Soyuz MS-21 on March 18, [...] [Source: Expedition 67-Wikipedia]Similar to ; Stiennon et al. (2020) and others, our method involves a continuous cycle of evaluation and training as illustrated in fig. 3 . We start with DPC described in section 2.2, as the initial dialogue agent. We then ask human participants to interact with the agent in two main settings: per-turn response preference and adversarial probing.\nPer-turn response preference In this task, human raters are given an incomplete dialogue and multiple possible statements to continue the dialogue, each corresponding to a different sample or model. The human raters select the response that they think is best ( fig. 20) . In contrast to Askell et al.\n(2021), a model generates both the User and Agent turns, and in both cases the human raters are asked to select the best response. The selected response is then used to continue the dialogue. Per-turn response preference data lets us estimate a preference rate which measures how frequently a model is preferred over one or more competing models. When responses are combined with supporting evidence, human raters give additional per-response feedback, as described in section 2.4.\nThe majority of the training data for the preference reward model comes from the per-turn response preference data collections. We also reuse the data collected for GopherCite. The exact breakdown is presented in table 11. The dataset includes both Sparrow turns (with or without evidence) and User turns.\nWe break each of the multi-option comparisons into pairs comparing chosen best continuation to every other non-selected option. We use these to form training tuples of (best_completion, other_completion, distractor). Table 12 | Preference reward model training data format. We show an example context, with statements, A, B, C, and two possible completions, either with or without evidence.\nWe use rule violation ratings from adversarial probing tasks and free dialogue tasks (see section 2.3) to train our rule RM. Overall, we collected 14576 dialogues from the adversarial probing tasks, and rated each for one or more rules. The dialogues are randomly split in 70 % training data and 30% test data; we train over 6 epochs of the training data. The Likert scale of the rule violation ratings Table 13 | Breakdown of the rule ratings used to train the rule RM. Dialogues were collected from many models as research and model training progressed. More harmful rules were targeted later in development, and data collections run more sparingly, hence the variation in collected volumes.\nIn this section we provide more detail of our human data collection methodology.\nEnd paper context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| Sparrow | 14576 dialogues |  | Adversarial probing tasks and free dialogue tasks |  |\n| Preference reward model training data format |  |  |  |  |\n|  |  |  |  |  |\n|  |  |  |  |  |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nBackground: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication .The chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images were imported into the software for delineating, and the images with VOI information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\nAs depicted in Table 1 , we randomly split the whole RibFrac Dataset (900 cases, 332,483 CT slices in total) into 3 cohorts: training (720 cases, to train the deep learning system), tuning (60 cases, to tune hyper-parameters of the deep learning system) and test (120 cases, to evaluate the model and human performance). In standard machine learning terminology, tuning is regarded as \"validation\"; in standard medical terminology, test in regarded as \"validation\".\nConsidering several practical issues, we open source a subset of 600 cases (221,308 CT slices in total), with 420 cases for training, 60 cases for tuning and 120 for test. To our knowledge, it is the first open research dataset in this application. On the open-source subset of RibFrac Dataset, a deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance. Please refer to Supplementary Materials for details. Our algorithm follows a data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures. Notably, the proposed FracNet does not rely on the extraction of rib centerlines in typical rib analysis algorithms [27] . As illustrated in Fig. 2 (a), our model pipeline consists of three stages: (a) pre-processing, (b) sliding-window prediction, and (c) post-processing.\n(a) Pre-processing: To speed up the detection, we extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering). The original spacing was preserved since only thin-section CT scans were included in our dataset. The intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n(b) Sliding-window prediction: Considering the elongated shape of rib fractures, standard labeling with bounding boxes could be missing much details. Therefore, we formulated the rib fracture detection as a 3D segmentation task. A customized 3D UNet, named FracNet (Section 2.2.2.), was developed to perform segmentation in a sliding-window fashion. Since a whole-volume CT scan could be too large to fit in a regular GPU memory, we cropped 64 \u00c2 64 \u00c2 64patches in a sliding-window fashion with a stride of 48 and feed them to our network. A raw segmentation was obtained by assembling patches of prediction. Maximum values were kept in the overlapping regions of multiple predictions.\n(c) Post-processing: To efficiently reduce the false positive in our predictions, predictions of small sizes (smaller than 200 voxels) were filtered out. We also removed the spine regions according to their coordinates on the raw segmentation. To generate detection proposal, we first binarized the post-processed segmentation results with a low threshold of 0.1, and then computed connected components on the binary segmentation. Each connected component was regarded as a detection proposal, with a probability calculated by averaging raw segmentation scores over all voxels within the connected component.\nThe data and code from Huadong Hospital affiliated to Fudan University used in this study are available at https://m3dv.github.io/Frac Net/ to users who agree with our data license (CC BY-NC 4.0) and code license (Apache-2.0 License).\nEnd paper context": "| Name | Size | Demographic information | Origin | Data or code link to find more |\n| --- | --- | --- | --- | --- |\n| RibFrac Dataset | 7,473 annotated traumatic rib fractures from 900 patients in a single center | Enrolled with a human-in-the-loop labeling procedure | A full list of funding bodies that contributed to this study can be found in the Acknowledgements section | https://m3dv.github.io/FracNet/ |", "Please answer the following request, denoted by \"Request:\" in the best way possible with the given paper context that bounded by \"Start paper context\" and \"End paper context\". Everytime \"paper\" is mentioned, it is referring to paper context denoted by \"Start paper context\" and \"End paper context\". Request: Please summarize the following text on a markdown table. The text will contain possibly repeated information about the characteristics of one or more datasets. I want you to summarize the whole text into a markdown table that represents the characteristics of all the datasets. The resulting table should be easy to read and contain any information that might be useful for medical researchers thinking about using any of those datasets. Some example fields would be \"Name\", \"Size\", \"Demographic information\", \"Origin\" and \"Data or code link to find more\", but add as many as you think are relevant for a medical researcher. The resulting table should contain as many entries as possible but it should NOT contain any duplicates (columns with the same \"Name\" field) and it should NOT contain any entries where the \"Name\" field is not defined/unknown/ not specified.\nStart paper context\nLarge, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. 1CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain. We report the prevalences of the labels for the different obsevations in Table 1 .\nWe retrospectively collected chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports. From these, we sampled a set of 1000 reports for manual review by a boardcertified radiologist to determine feasibility for extraction of observations. We decided on 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Society's recommended glossary (Hansell et al. 2008 ) whenever applicable. \"Pneumonia\", despite being a clinical diagnosis, was included as a label in order to represent the images that suggested primary infection as the diagnosis. The \"No Finding\" observation was intended to capture the absence of all pathologies.\n1. unremarkable cardiomediastinal silhouette 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation.\n3. no pleural effusion or pneumothorax 4. mild degenerative changes in the lumbar spine and old right rib fractures. Figure 2 : Output of the labeler when run on a report sampled from our dataset. In this case, the labeler correctly extracts all of the mentions in the report (underline) and classifies the uncertainties (bolded) and negations (italicized).\nOne of the main obstacles in the development of chest radiograph interpretation models has been the lack of datasets with strong radiologist-annotated groundtruth and expert scores against which researchers can compare their models. There are few chest radiographic imaging datasets that are publicly available, but none of them have test sets with strong ground truth or radiologist performances. The Indiana Network for Patient Care hosts the OpenI dataset (Demner-Fushman et al. 2015) consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators . The National Cancer Institute hosts the PLCO Lung dataset (Gohagan et al. 2000) of chest radiographs obtained during a study on lung cancer screening . The dataset contains 185,421 full resolution images, but due to the nature of the collection process, it is has a low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly. The MIMIC-CXR dataset (Rubin et al. 2018) has been recently announced but is not yet publicly available.\nThe most commonly used benchmark for developing chest radiograph interpretation models has been the ChestX-ray14 dataset . Due to the introduction of this large dataset, substantial progress has been made towards developing automated chest radiograph interpretation models (Yao et al. 2017; Rajpurkar et al. 2017; Li et al. 2017b; Kumar, Grewal, and Srivastava 2018; Guan et al. 2018; Yao et al. 2018) . However, using the NIH dataset as a benchmark on which to compare models is problematic as the labels in the test set are extracted from reports using an automatic labeler. The CheXpert dataset that we introduce features radiologist-labeled validation and test sets which serve as strong reference standards, as well as expert scores to allow for robust evaluation of different algorithms.\nEnd paper context": "| Name                                                        | Size                                                        | Demographic information                                                        | Origin                                                        | Data or code link to find more                                                        |\n| ----------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------- | ------------------------------------------------------------ | --------------------------------------------------------------------------------------- |\n| CheXpert                                                    | 224,316 chest radiographs of 65,240 patients                  | 1. unremarkable cardiomediastinal silhouette <br/> 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation. <br/> 3. no pleural effusion or pneumothorax <br/> 4. mild degenerative changes in the lumbar spine and old right rib fractures. | Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports. | https://stanfordmlgroup.github.io/competitions/chexpert/ |\n| OpenI                                                       | 7,470 frontal-view radiographs                              |                                                                                   |                                                              | https://openi.nlm.nih.gov/                                                             |\n| PLCO Lung                                                   | 185,421 full resolution images                               |                                                                                   |                                                              | https://wiki.cancerimagingarchive.net/display/Public/PLCO+Lung+Cancer+Screening+Trial |\n| MIMIC-CXR                                                   |                                                              |                                                                                   |                                                              |                                                                                       |\n| ChestX-ray14                                                |                                                              |                                                                                   |                                                              |                                                                                       |"}