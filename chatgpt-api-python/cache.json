{"Extract the datasets and their characteristics in a nested list manner from the following text: Background: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication .": "- Dataset: RibFrac Dataset\n- Characteristics: \n    - 7,473 annotated traumatic rib fractures from 900 patients in a single center\n    - Annotated with a human-in-the-loop labeling procedure\n    - 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively\n- Evaluation method:\n    - Detection performance: Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives \n    - Segmentation performance: Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures\n- Observer studies:\n    - Independent human-only study\n    - Human-collaboration study\n- Availability: \n    - A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: This retrospective study was approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146), which waived the requirement for informed consent.": "- Dataset: None mentioned\n- Characteristics: \n    - This is a retrospective study\n- Ethics committee approval:\n    - Approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146)\n    - Waived the requirement for informed consent.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images were imported into the software for delineating, and the images with VOI information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.": "- Dataset: chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images\n- Characteristics:\n    - Imported into the software for delineating\n    - Images with VOI (volume of interest) information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: As depicted in Table 1 , we randomly split the whole RibFrac Dataset (900 cases, 332,483 CT slices in total) into 3 cohorts: training (720 cases, to train the deep learning system), tuning (60 cases, to tune hyper-parameters of the deep learning system) and test (120 cases, to evaluate the model and human performance). In standard machine learning terminology, tuning is regarded as \"validation\"; in standard medical terminology, test in regarded as \"validation\".": "- Dataset: RibFrac Dataset\n- Characteristics:\n    - 900 cases, 332,483 CT slices in total\n    - Randomly split into 3 cohorts:\n        - Training: 720 cases, to train the deep learning system\n        - Tuning: 60 cases, to tune hyper-parameters of the deep learning system\n        - Test: 120 cases, to evaluate the model and human performance.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Considering several practical issues, we open source a subset of 600 cases (221,308 CT slices in total), with 420 cases for training, 60 cases for tuning and 120 for test. To our knowledge, it is the first open research dataset in this application. On the open-source subset of RibFrac Dataset, a deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance. Please refer to Supplementary Materials for details. Our algorithm follows a data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures. Notably, the proposed FracNet does not rely on the extraction of rib centerlines in typical rib analysis algorithms [27] . As illustrated in Fig. 2 (a) , our model pipeline consists of three stages: (a) pre-processing, (b) sliding-window prediction, and (c) post-processing.": "- Dataset: RibFrac Dataset\n- Characteristics:\n    - 600 cases (221,308 CT slices in total) open sourced\n    - Randomly split into 3 cohorts:\n        - Training: 420 cases\n        - Tuning: 60 cases\n        - Test: 120 cases\n    - First open research dataset in this application.\n- Algorithm approach:\n    - Data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures\n    - Does not rely on the extraction of rib centerlines in typical rib analysis algorithms\n- Model pipeline:\n    - Three stages:\n        - Pre-processing\n        - Sliding-window prediction\n        - Post-processing\n- Additional information: \n    - A deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance on the open-source subset of RibFrac Dataset.\n    - Please refer to Supplementary Materials for details.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: (a) Pre-processing: To speed up the detection, we extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering). The original spacing was preserved since only thin-section CT scans were included in our dataset. The intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].": "- Dataset: Not mentioned\n- Characteristics:\n    - Pre-processing:\n        - Extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering)\n        - The original spacing was preserved since only thin-section CT scans were included in the dataset\n        - Intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n", "Extract the datasets and their characteristics in a nested list manner from the following text: (b) Sliding-window prediction: Considering the elongated shape of rib fractures, standard labeling with bounding boxes could be missing much details. Therefore, we formulated the rib fracture detection as a 3D segmentation task. A customized 3D UNet, named FracNet (Section 2.2.2.), was developed to perform segmentation in a sliding-window fashion. Since a whole-volume CT scan could be too large to fit in a regular GPU memory, we cropped 64 \u00c2 64 \u00c2 64patches in a sliding-window fashion with a stride of 48 and feed them to our network. A raw segmentation was obtained by assembling patches of prediction. Maximum values were kept in the overlapping regions of multiple predictions.": "- Dataset: Not mentioned\n- Characteristics:\n    - Sliding-window prediction:\n        - Formulated the rib fracture detection as a 3D segmentation task\n        - A customized 3D UNet, named FracNet was developed to perform segmentation in a sliding-window fashion\n        - Whole-volume CT scan is too large to fit in a regular GPU memory\n        - Cropped 64x64x64 patches in a sliding-window fashion with a stride of 48 and feed them to the network\n        - A raw segmentation was obtained by assembling patches of prediction\n        - Maximum values were kept in the overlapping regions of multiple predictions.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: (c) Post-processing: To efficiently reduce the false positive in our predictions, predictions of small sizes (smaller than 200 voxels) were filtered out. We also removed the spine regions according to their coordinates on the raw segmentation. To generate detection proposal, we first binarized the post-processed segmentation results with a low threshold of 0.1, and then computed connected components on the binary segmentation. Each connected component was regarded as a detection proposal, with a probability calculated by averaging raw segmentation scores over all voxels within the connected component.": "- Dataset: Not mentioned\n- Characteristics:\n    - Post-processing:\n        - Efficiently reduce the false positive in predictions\n        - Predictions of small sizes (smaller than 200 voxels) were filtered out\n        - Removed the spine regions according to their coordinates on the raw segmentation\n        - Generate detection proposal:\n            - Binarized the post-processed segmentation results with a low threshold of 0.1\n            - Computed connected components on the binary segmentation\n            - Each connected component was regarded as a detection proposal\n            - A probability calculated by averaging raw segmentation scores over all voxels within the connected component.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.": "- Dataset: Not specified \n- Characteristics:\n    - Goal is to advance the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.\n    - Images of complex everyday scenes containing common objects in their natural context\n    - Objects are labeled using per-instance segmentations to aid in precise object localization\n    - Contains photos of 91 objects types that would be easily recognizable by a 4 year old\n    - 2.5 million labeled instances in 328k images\n    - Involved extensive crowd worker involvement via novel user interfaces for category detection, instance spotting, and instance segmentation\n    - Detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN \n    - Baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Next, we analyze the properties of the Microsoft Common Objects in COntext (MS COCO) dataset in comparison to several other popular datasets. These include ImageNet [1] , PASCAL VOC 2012 [2] , and SUN [3] . Each of these datasets varies significantly in size, list of labeled categories and types of images. ImageNet was created to capture a large number of object categories, many of which are fine-grained. SUN focuses on labeling scene types and the objects that commonly occur in them. Finally, PASCAL VOC's primary application is object detection in natural images. MS COCO is designed for the detection and segmentation of objects occurring in their natural context.": "- Dataset: Microsoft Common Objects in COntext (MS COCO)\n- Characteristics:\n    - Compared to several other popular datasets: ImageNet, PASCAL VOC 2012, and SUN\n    - Each of these datasets varies significantly in size, list of labeled categories and types of images\n    - ImageNet was created to capture a large number of object categories, many of which are fine-grained\n    - SUN focuses on labeling scene types and the objects that commonly occur in them\n    - PASCAL VOC's primary application is object detection in natural images\n    - MS COCO is designed for the detection and segmentation of objects occurring in their natural context.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: To accommodate a faster release schedule, we split the MS COCO dataset into two roughly equal parts. The first half of the dataset was released in 2014, the second half will be released in 2015. The 2014 release contains 82,783 training, 40,504 validation, and 40,775 testing images (approximately 1 train, 1 val, and 1 test). There are nearly 270k segmented people and a total of 886k segmented object instances in the 2014 train+val data alone. The cumulative 2015 release will contain a total of 165,482 train, 81,208 val, and 81,434 test images. We took care to minimize the chance of near-duplicate images existing across splits by explicitly removing near duplicates (detected with [43] ) and grouping images by photographer and date taken.": "- Dataset: MS COCO\n- Characteristics:\n    - Split into two roughly equal parts:\n        - First half of the dataset was released in 2014\n        - Second half will be released in 2015\n    - 2014 release contains:\n        - 82,783 training images\n        - 40,504 validation images\n        - 40,775 testing images (approximately 1 train, 1 val, and 1 test)\n        - Nearly 270k segmented people \n        - Total of 886k segmented object instances in the 2014 train+val data alone\n    - 2015 release will contain:\n        - 165,482 training images\n        - 81,208 validation images\n        - 81,434 testing images\n    - Care was taken to minimize the chance of near-duplicate images existing across splits by explicitly removing near duplicates (detected with [43]) and grouping images by photographer and date taken.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Following established protocol, annotations for train and validation data will be released, but not for test. We are currently finalizing the evaluation server for automatic evaluation on the test set. A full discussion of evaluation metrics will be added once the evaluation server is complete.": "- Dataset: Not specified\n- Characteristics:\n    - Following established protocol, annotations for train and validation data will be released, but not for test\n    - Evaluation server for automatic evaluation on the test set is being finalized\n    - A full discussion of evaluation metrics will be added once the evaluation server is complete.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Note that we have limited the 2014 release to a subset of 80 categories. We did not collect segmentations for the following 11 categories: hat, shoe, eyeglasses (too many instances), mirror, window, door, street sign (ambiguous and difficult to label), plate, desk (due to confusion with bowl and dining table, respectively) and blender, hair brush (too few instances). We may add segmentations for some of these categories in the cumulative 2015 release. ": "- Dataset: MS COCO\n- Characteristics:\n    - 2014 release is limited to a subset of 80 categories\n    - The following 11 categories are not included: hat, shoe, eyeglasses, mirror, window, door, street sign, plate, desk, blender, hair brush \n        - Not collected segmentations for these categories because they are too many instances, ambiguous and difficult to label, due to confusion with other categories, or too few instances\n    - May add segmentations for some of these categories in the cumulative 2015 release.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The number of instances per category for all 91 categories is shown in Fig. 5(a) . A summary of the datasets showing the number of object categories and the number of instances per category is shown in Fig. 5(d) . While MS COCO has fewer categories than ImageNet and SUN, it has more instances per category which we hypothesize will be useful for learning complex models capable of precise localization. In comparison to PASCAL VOC, MS COCO has both more categories and instances.": "- Dataset: MS COCO\n- Characteristics:\n    - Number of instances per category for all 91 categories is shown in Fig. 5(a)\n    - Summary of the datasets showing the number of object categories and the number of instances per category is shown in Fig. 5(d)\n    - MS COCO has fewer categories than ImageNet and SUN, but has more instances per category which can be useful for learning complex models capable of precise localization.\n    - In comparison to PASCAL VOC, MS COCO has both more categories and instances.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: An important property of our dataset is we strive to find non-iconic images containing objects in their natural context. The amount of contextual information present in an image can be estimated by examining the average number of object categories and instances per image, Fig. 5(b, c) . For ImageNet we plot the object detection validation set, since the training data only has a single object labeled. On average our dataset contains 3.5 categories and 7.7 instances per image. In comparison ImageNet and PASCAL VOC both have less than 2 categories and 3 instances per image on average. Another interesting observation is only 10% of the images in MS COCO have only one category per image, in comparison, over 60% of images contain a single object category in ImageNet and PASCAL VOC. As expected, the SUN dataset has the most contextual information since it is scene-based and uses an unrestricted set of categories.": "- Dataset: MS COCO\n- Characteristics:\n    - Strive to find non-iconic images containing objects in their natural context\n    - The amount of contextual information present in an image can be estimated by examining the average number of object categories and instances per image.\n    - For ImageNet we plot the object detection validation set, since the training data only has a single object labeled.\n    - On average MS COCO dataset contains 3.5 categories and 7.7 instances per image\n    - In comparison, ImageNet and PASCAL VOC both have less than 2 categories and 3 instances per image on average\n    - Only 10% of the images in MS COCO have only one category per image, in comparison, over 60% of images contain a single object category in ImageNet and PASCAL VOC.\n    - SUN dataset has the most contextual information since it is scene-based and uses an unrestricted set of categories.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Finally, we analyze the average size of objects in the datasets. Generally smaller objects are harder to recognize and require more contextual reasoning to recognize. As shown in Fig. 5(e) , the average sizes of objects is smaller for both MS COCO and SUN.": "- Dataset: MS COCO and SUN\n- Characteristics:\n    - Analyze the average size of objects in the datasets\n    - Generally smaller objects are harder to recognize and require more contextual reasoning to recognize\n    - As shown in Fig. 5(e), the average sizes of objects is smaller for both MS COCO and SUN\n", "Extract the datasets and their characteristics in a nested list manner from the following text: CheXpert is a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain. We report the prevalences of the labels for the different obsevations in Table 1 .": "- Dataset: CheXpert\n- Characteristics:\n    - Large public dataset for chest radiograph interpretation\n    - Consists of 224,316 chest radiographs of 65,240 patients\n    - Labeled for the presence of 14 observations as positive, negative, or uncertain\n    - Table 1 reports the prevalences of the labels for the different observations.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: We retrospectively collected chest radiographic studies from Stanford Hospital, performed between October 2002 and July 2017 in both inpatient and outpatient centers, along with their associated radiology reports. From these, we sampled a set of 1000 reports for manual review by a boardcertified radiologist to determine feasibility for extraction of observations. We decided on 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Society's recommended glossary (Hansell et al. 2008 ) whenever applicable. \"Pneumonia\", despite being a clinical diagnosis, was included as a label in order to represent the images that suggested primary infection as the diagnosis. The \"No Finding\" observation was intended to capture the absence of all pathologies.": "- Dataset: Chest Radiographic Studies\n- Characteristics:\n    - Retrospectively collected from Stanford Hospital\n    - Performed between October 2002 and July 2017 in both inpatient and outpatient centers\n    - Along with their associated radiology reports\n    - Sampled a set of 1000 reports for manual review by a board-certified radiologist to determine feasibility for extraction of observations\n    - 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Society's recommended glossary (Hansell et al. 2008) whenever applicable.\n    - \"Pneumonia\" included as a label to represent the images that suggested primary infection as the diagnosis\n    - \"No Finding\" observation was intended to capture the absence of all pathologies.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: 1. unremarkable cardiomediastinal silhouette 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation.": "- Dataset: Not Mentioned\n- Characteristics:\n    - 1. unremarkable cardiomediastinal silhouette \n    - 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. 1": "- Dataset: CheXpert\n- Characteristics:\n    - Large dataset that contains 224,316 chest radiographs of 65,240 patients\n    - Designed a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation\n    - Investigates different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs\n    - On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, it finds that different uncertainty approaches are useful for different pathologies\n    - Evaluated the best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compared the performance of the model to that of 3 additional radiologists in the detection of 5 selected pathologies\n    - On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points\n    - Released to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: 3. no pleural effusion or pneumothorax 4. mild degenerative changes in the lumbar spine and old right rib fractures. Figure 2 : Output of the labeler when run on a report sampled from our dataset. In this case, the labeler correctly extracts all of the mentions in the report (underline) and classifies the uncertainties (bolded) and negations (italicized).": "- Dataset: Not Mentioned\n- Characteristics:\n    - 3. no pleural effusion or pneumothorax\n    - 4. mild degenerative changes in the lumbar spine and old right rib fractures\n    - Figure 2 : Output of the labeler when run on a report sampled from the dataset. The labeler correctly extracts all of the mentions in the report (underline) and classifies the uncertainties (bolded) and negations (italicized).\n", "Extract the datasets and their characteristics in a nested list manner from the following text: One of the main obstacles in the development of chest radiograph interpretation models has been the lack of datasets with strong radiologist-annotated groundtruth and expert scores against which researchers can compare their models. There are few chest radiographic imaging datasets that are publicly available, but none of them have test sets with strong ground truth or radiologist performances. The Indiana Network for Patient Care hosts the OpenI dataset (Demner-Fushman et al. 2015) consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators . The National Cancer Institute hosts the PLCO Lung dataset (Gohagan et al. 2000) of chest radiographs obtained during a study on lung cancer screening . The dataset contains 185,421 full resolution images, but due to the nature of the collection process, it is has a low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly. The MIMIC-CXR dataset (Rubin et al. 2018) has been recently announced but is not yet publicly available.": "- CheXpert: a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain.\n- OpenI: a dataset consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators.\n- PLCO Lung: a dataset of chest radiographs obtained during a study on lung cancer screening, containing 185,421 full resolution images, but a low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly.\n- MIMIC-CXR: a recently announced dataset, not yet publicly available.", "Extract the datasets and their characteristics in a nested list manner from the following text: The most commonly used benchmark for developing chest radiograph interpretation models has been the ChestX-ray14 dataset . Due to the introduction of this large dataset, substantial progress has been made towards developing automated chest radiograph interpretation models (Yao et al. 2017; Rajpurkar et al. 2017; Li et al. 2017b; Kumar, Grewal, and Srivastava 2018; Guan et al. 2018; Yao et al. 2018) . However, using the NIH dataset as a benchmark on which to compare models is problematic as the labels in the test set are extracted from reports using an automatic labeler. The CheXpert dataset that we introduce features radiologist-labeled validation and test sets which serve as strong reference standards, as well as expert scores to allow for robust evaluation of different algorithms.": "- The CheXpert dataset: a large dataset that contains 224,316 chest radiographs of 65,240 patients. The dataset is designed to detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation.\n- OpenI dataset: consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators\n- PLCO Lung dataset: of chest radiographs obtained during a study on lung cancer screening. The dataset contains 185,421 full resolution images, but has a low prevalence of clinically important pathologies.\n- MIMIC-CXR dataset: recently announced but not yet publicly available.\n- ChestX-ray14 dataset: commonly used benchmark for developing chest radiograph interpretation models. Due to the introduction of this large dataset, substantial progress has been made towards developing automated chest radiograph interpretation models. However, the labels in the test set are extracted from reports using an automatic labeler.\n", "EBased on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. Also do not include duplicate entries. \n--- START OF TEXT\n- Dataset: RibFrac Dataset\n- Characteristics: \n    - 7,473 annotated traumatic rib fractures from 900 patients in a single center\n    - Annotated with a human-in-the-loop labeling procedure\n    - 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively\n- Evaluation method:\n    - Detection performance: Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives \n    - Segmentation performance: Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures\n- Observer studies:\n    - Independent human-only study\n    - Human-collaboration study\n- Availability: \n    - A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research.\n\n- Dataset: None mentioned\n- Characteristics: \n    - This is a retrospective study\n- Ethics committee approval:\n    - Approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146)\n    - Waived the requirement for informed consent.\n\n- Dataset: chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images\n- Characteristics:\n    - Imported into the software for delineating\n    - Images with VOI (volume of interest) information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 900 cases, 332,483 CT slices in total\n    - Randomly split into 3 cohorts:\n        - Training: 720 cases, to train the deep learning system\n        - Tuning: 60 cases, to tune hyper-parameters of the deep learning system\n        - Test: 120 cases, to evaluate the model and human performance.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 600 cases (221,308 CT slices in total) open sourced\n    - Randomly split into 3 cohorts:\n        - Training: 420 cases\n        - Tuning: 60 cases\n        - Test: 120 cases\n    - First open research dataset in this application.\n- Algorithm approach:\n    - Data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures\n    - Does not rely on the extraction of rib centerlines in typical rib analysis algorithms\n- Model pipeline:\n    - Three stages:\n        - Pre-processing\n        - Sliding-window prediction\n        - Post-processing\n- Additional information: \n    - A deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance on the open-source subset of RibFrac Dataset.\n    - Please refer to Supplementary Materials for details.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Pre-processing:\n        - Extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering)\n        - The original spacing was preserved since only thin-section CT scans were included in the dataset\n        - Intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Sliding-window prediction:\n        - Formulated the rib fracture detection as a 3D segmentation task\n        - A customized 3D UNet, named FracNet was developed to perform segmentation in a sliding-window fashion\n        - Whole-volume CT scan is too large to fit in a regular GPU memory\n        - Cropped 64x64x64 patches in a sliding-window fashion with a stride of 48 and feed them to the network\n        - A raw segmentation was obtained by assembling patches of prediction\n        - Maximum values were kept in the overlapping regions of multiple predictions.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Post-processing:\n        - Efficiently reduce the false positive in predictions\n        - Predictions of small sizes (smaller than 200 voxels) were filtered out\n        - Removed the spine regions according to their coordinates on the raw segmentation\n        - Generate detection proposal:\n            - Binarized the post-processed segmentation results with a low threshold of 0.1\n            - Computed connected components on the binary segmentation\n            - Each connected component was regarded as a detection proposal\n            - A probability calculated by averaging raw segmentation scores over all voxels within the connected component.\n--- END OF TEXT": "| Dataset | Size | Data Type | Balance | Originating Institution | Annotation | Format |\n| --- | --- | --- | --- | --- | --- | --- |\n| RibFrac | 7,473 annotated traumatic rib fractures from 900 patients in a single center | Annotated with human-in-the-loop labeling procedure | 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively | Unknown | Yes | CT DICOM |\n| RibFrac | 332,483 CT slices | Unknown | 900 cases randomly split into 3 cohorts: training(720), tuning(60), test(120) | Unknown | Yes | CT DICOM |\n| RibFrac | 221,308 CT slices | Unknown | 600 cases randomly split into 3 cohorts: training(420), tuning(60), test(120) | Unknown | Yes | CT DICOM |", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: RibFrac Dataset\n- Characteristics: \n    - 7,473 annotated traumatic rib fractures from 900 patients in a single center\n    - Annotated with a human-in-the-loop labeling procedure\n    - 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively\n- Evaluation method:\n    - Detection performance: Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives \n    - Segmentation performance: Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures\n- Observer studies:\n    - Independent human-only study\n    - Human-collaboration study\n- Availability: \n    - A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research.\n\n- Dataset: None mentioned\n- Characteristics: \n    - This is a retrospective study\n- Ethics committee approval:\n    - Approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146)\n    - Waived the requirement for informed consent.\n\n- Dataset: chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images\n- Characteristics:\n    - Imported into the software for delineating\n    - Images with VOI (volume of interest) information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 900 cases, 332,483 CT slices in total\n    - Randomly split into 3 cohorts:\n        - Training: 720 cases, to train the deep learning system\n        - Tuning: 60 cases, to tune hyper-parameters of the deep learning system\n        - Test: 120 cases, to evaluate the model and human performance.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 600 cases (221,308 CT slices in total) open sourced\n    - Randomly split into 3 cohorts:\n        - Training: 420 cases\n        - Tuning: 60 cases\n        - Test: 120 cases\n    - First open research dataset in this application.\n- Algorithm approach:\n    - Data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures\n    - Does not rely on the extraction of rib centerlines in typical rib analysis algorithms\n- Model pipeline:\n    - Three stages:\n        - Pre-processing\n        - Sliding-window prediction\n        - Post-processing\n- Additional information: \n    - A deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance on the open-source subset of RibFrac Dataset.\n    - Please refer to Supplementary Materials for details.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Pre-processing:\n        - Extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering)\n        - The original spacing was preserved since only thin-section CT scans were included in the dataset\n        - Intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Sliding-window prediction:\n        - Formulated the rib fracture detection as a 3D segmentation task\n        - A customized 3D UNet, named FracNet was developed to perform segmentation in a sliding-window fashion\n        - Whole-volume CT scan is too large to fit in a regular GPU memory\n        - Cropped 64x64x64 patches in a sliding-window fashion with a stride of 48 and feed them to the network\n        - A raw segmentation was obtained by assembling patches of prediction\n        - Maximum values were kept in the overlapping regions of multiple predictions.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Post-processing:\n        - Efficiently reduce the false positive in predictions\n        - Predictions of small sizes (smaller than 200 voxels) were filtered out\n        - Removed the spine regions according to their coordinates on the raw segmentation\n        - Generate detection proposal:\n            - Binarized the post-processed segmentation results with a low threshold of 0.1\n            - Computed connected components on the binary segmentation\n            - Each connected component was regarded as a detection proposal\n            - A probability calculated by averaging raw segmentation scores over all voxels within the connected component.\n--- END OF TEXT": "| Dataset | Size | Data Type | Balance | Originating Institution | Annotation | Format |\n| --- | --- | --- | --- | --- | --- | --- |\n| RibFrac | 7,473 annotated traumatic rib fractures from 900 patients in a single center; 900 cases, 332,483 CT slices in total; 600 cases (221,308 CT slices in total) open sourced | Annotated with human-in-the-loop labeling procedure | 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively; 900 cases randomly split into 3 cohorts: training(720), tuning(60), test(120); 600 cases randomly split into 3 cohorts: training(420), tuning(60), test(120) | Unknown | Yes | CT DICOM |\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: Not specified \n- Characteristics:\n    - Goal is to advance the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.\n    - Images of complex everyday scenes containing common objects in their natural context\n    - Objects are labeled using per-instance segmentations to aid in precise object localization\n    - Contains photos of 91 objects types that would be easily recognizable by a 4 year old\n    - 2.5 million labeled instances in 328k images\n    - Involved extensive crowd worker involvement via novel user interfaces for category detection, instance spotting, and instance segmentation\n    - Detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN \n    - Baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.\n\n- Dataset: Microsoft Common Objects in COntext (MS COCO)\n- Characteristics:\n    - Compared to several other popular datasets: ImageNet, PASCAL VOC 2012, and SUN\n    - Each of these datasets varies significantly in size, list of labeled categories and types of images\n    - ImageNet was created to capture a large number of object categories, many of which are fine-grained\n    - SUN focuses on labeling scene types and the objects that commonly occur in them\n    - PASCAL VOC's primary application is object detection in natural images\n    - MS COCO is designed for the detection and segmentation of objects occurring in their natural context.\n\n- Dataset: MS COCO\n- Characteristics:\n    - Number of instances per category for all 91 categories is shown in Fig. 5(a)\n    - Summary of the datasets showing the number of object categories and the number of instances per category is shown in Fig. 5(d)\n    - MS COCO has fewer categories than ImageNet and SUN, but has more instances per category which can be useful for learning complex models capable of precise localization.\n    - In comparison to PASCAL VOC, MS COCO has both more categories and instances.\n\n- Dataset: MS COCO\n- Characteristics:\n    - Strive to find non-iconic images containing objects in their natural context\n    - The amount of contextual information present in an image can be estimated by examining the average number of object categories and instances per image.\n    - For ImageNet we plot the object detection validation set, since the training data only has a single object labeled.\n    - On average MS COCO dataset contains 3.5 categories and 7.7 instances per image\n    - In comparison, ImageNet and PASCAL VOC both have less than 2 categories and 3 instances per image on average\n    - Only 10% of the images in MS COCO have only one category per image, in comparison, over 60% of images contain a single object category in ImageNet and PASCAL VOC.\n    - SUN dataset has the most contextual information since it is scene-based and uses an unrestricted set of categories.\n\n- Dataset: MS COCO and SUN\n- Characteristics:\n    - Analyze the average size of objects in the datasets\n    - Generally smaller objects are harder to recognize and require more contextual reasoning to recognize\n    - As shown in Fig. 5(e), the average sizes of objects is smaller for both MS COCO and SUN\n\n- Dataset: MS COCO\n- Characteristics:\n    - Split into two roughly equal parts:\n        - First half of the dataset was released in 2014\n        - Second half will be released in 2015\n    - 2014 release contains:\n        - 82,783 training images\n        - 40,504 validation images\n        - 40,775 testing images (approximately 1 train, 1 val, and 1 test)\n        - Nearly 270k segmented people \n        - Total of 886k segmented object instances in the 2014 train+val data alone\n    - 2015 release will contain:\n        - 165,482 training images\n        - 81,208 validation images\n        - 81,434 testing images\n    - Care was taken to minimize the chance of near-duplicate images existing across splits by explicitly removing near duplicates (detected with [43]) and grouping images by photographer and date taken.\n\n- Dataset: Not specified\n- Characteristics:\n    - Following established protocol, annotations for train and validation data will be released, but not for test\n    - Evaluation server for automatic evaluation on the test set is being finalized\n    - A full discussion of evaluation metrics will be added once the evaluation server is complete.\n\n- Dataset: MS COCO\n- Characteristics:\n    - 2014 release is limited to a subset of 80 categories\n    - The following 11 categories are not included: hat, shoe, eyeglasses, mirror, window, door, street sign, plate, desk, blender, hair brush \n        - Not collected segmentations for these categories because they are too many instances, ambiguous and difficult to label, due to confusion with other categories, or too few instances\n    - May add segmentations for some of these categories in the cumulative 2015 release.\n--- END OF TEXT": "| Dataset | Size | Data Type | Balance | Originating Institution | Annotation | Format |\n| --- | --- | --- | --- | --- | --- | --- |\n| MS COCO | 2.5 million labeled instances in 328k images | Images of complex everyday scenes containing common objects in their natural context | Objects are labeled using per-instance segmentations to aid in precise object localization | Microsoft | Yes | Unknown |\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: CheXpert\n- Characteristics:\n    - Large dataset that contains 224,316 chest radiographs of 65,240 patients\n    - Designed a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation\n    - Investigates different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs\n    - On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, it finds that different uncertainty approaches are useful for different pathologies\n    - Evaluated the best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compared the performance of the model to that of 3 additional radiologists in the detection of 5 selected pathologies\n    - On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points\n    - Released to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.\n\n- Dataset: CheXpert\n- Characteristics:\n    - Large public dataset for chest radiograph interpretation\n    - Consists of 224,316 chest radiographs of 65,240 patients\n    - Labeled for the presence of 14 observations as positive, negative, or uncertain\n    - Table 1 reports the prevalences of the labels for the different observations.\n\n- Dataset: Chest Radiographic Studies\n- Characteristics:\n    - Retrospectively collected from Stanford Hospital\n    - Performed between October 2002 and July 2017 in both inpatient and outpatient centers\n    - Along with their associated radiology reports\n    - Sampled a set of 1000 reports for manual review by a board-certified radiologist to determine feasibility for extraction of observations\n    - 14 observations based on the prevalence in the reports and clinical relevance, conforming to the Fleischner Society's recommended glossary (Hansell et al. 2008) whenever applicable.\n    - \"Pneumonia\" included as a label to represent the images that suggested primary infection as the diagnosis\n    - \"No Finding\" observation was intended to capture the absence of all pathologies.\n\n- Dataset: Not Mentioned\n- Characteristics:\n    - 1. unremarkable cardiomediastinal silhouette \n    - 2. diffuse reticular pattern, which can be seen with an atypical infection or chronic fibrotic change. no focal consolidation.\n\n- Dataset: Not Mentioned\n- Characteristics:\n    - 3. no pleural effusion or pneumothorax\n    - 4. mild degenerative changes in the lumbar spine and old right rib fractures\n    - Figure 2 : Output of the labeler when run on a report sampled from the dataset. The labeler correctly extracts all of the mentions in the report (underline) and classifies the uncertainties (bolded) and negations (italicized).\n\n- CheXpert: a large public dataset for chest radiograph interpretation, consisting of 224,316 chest radiographs of 65,240 patients labeled for the presence of 14 observations as positive, negative, or uncertain.\n- OpenI: a dataset consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators.\n- PLCO Lung: a dataset of chest radiographs obtained during a study on lung cancer screening, containing 185,421 full resolution images, but a low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly.\n- MIMIC-CXR: a recently announced dataset, not yet publicly available.\n- The CheXpert dataset: a large dataset that contains 224,316 chest radiographs of 65,240 patients. The dataset is designed to detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation.\n- OpenI dataset: consisting of 7,470 frontal-view radiographs and radiology reports which have been labeled with key findings by human annotators\n- PLCO Lung dataset: of chest radiographs obtained during a study on lung cancer screening. The dataset contains 185,421 full resolution images, but has a low prevalence of clinically important pathologies.\n- MIMIC-CXR dataset: recently announced but not yet publicly available.\n- ChestX-ray14 dataset: commonly used benchmark for developing chest radiograph interpretation models. Due to the introduction of this large dataset, substantial progress has been made towards developing automated chest radiograph interpretation models. However, the labels in the test set are extracted from reports using an automatic labeler.\n--- END OF TEXT": "|Dataset|Size|Data Type|Balance|Originating Institution|Other Characteristics|\n|-------|----|---------|-------|----------------------|---------------------|\n|CheXpert|224,316 chest radiographs of 65,240 patients|Chest radiographs|Labeled for 14 observations as positive, negative, or uncertain|Stanford Hospital|Validated using 3 board-certified radiologists, compared to performance of 3 additional radiologists in detection of 5 selected pathologies|\n|OpenI|7,470 frontal-view radiographs and radiology reports|Radiographs and radiology reports|Labeled with key findings by human annotators|N/A|N/A|\n|PLCO Lung|185,421 full resolution images|Chest radiographs|Low prevalence of clinically important pathologies such as Pneumothorax, Consolidation, Effusion, and Cardiomegaly|N/A|Obtained during a study on lung cancer screening|\n|MIMIC-CXR|N/A|N/A|N/A|N/A|Recently announced dataset, not yet publicly available|", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: RibFrac Dataset\n- Characteristics: \n    - 7,473 annotated traumatic rib fractures from 900 patients in a single center\n    - Annotated with a human-in-the-loop labeling procedure\n    - 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively\n- Evaluation method:\n    - Detection performance: Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives \n    - Segmentation performance: Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures\n- Observer studies:\n    - Independent human-only study\n    - Human-collaboration study\n- Availability: \n    - A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research.\n\n- Dataset: None mentioned\n- Characteristics: \n    - This is a retrospective study\n- Ethics committee approval:\n    - Approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146)\n    - Waived the requirement for informed consent.\n\n- Dataset: chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images\n- Characteristics:\n    - Imported into the software for delineating\n    - Images with VOI (volume of interest) information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 900 cases, 332,483 CT slices in total\n    - Randomly split into 3 cohorts:\n        - Training: 720 cases, to train the deep learning system\n        - Tuning: 60 cases, to tune hyper-parameters of the deep learning system\n        - Test: 120 cases, to evaluate the model and human performance.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 600 cases (221,308 CT slices in total) open sourced\n    - Randomly split into 3 cohorts:\n        - Training: 420 cases\n        - Tuning: 60 cases\n        - Test: 120 cases\n    - First open research dataset in this application.\n- Algorithm approach:\n    - Data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures\n    - Does not rely on the extraction of rib centerlines in typical rib analysis algorithms\n- Model pipeline:\n    - Three stages:\n        - Pre-processing\n        - Sliding-window prediction\n        - Post-processing\n- Additional information: \n    - A deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance on the open-source subset of RibFrac Dataset.\n    - Please refer to Supplementary Materials for details.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Pre-processing:\n        - Extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering)\n        - The original spacing was preserved since only thin-section CT scans were included in the dataset\n        - Intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Sliding-window prediction:\n        - Formulated the rib fracture detection as a 3D segmentation task\n        - A customized 3D UNet, named FracNet was developed to perform segmentation in a sliding-window fashion\n        - Whole-volume CT scan is too large to fit in a regular GPU memory\n        - Cropped 64x64x64 patches in a sliding-window fashion with a stride of 48 and feed them to the network\n        - A raw segmentation was obtained by assembling patches of prediction\n        - Maximum values were kept in the overlapping regions of multiple predictions.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Post-processing:\n        - Efficiently reduce the false positive in predictions\n        - Predictions of small sizes (smaller than 200 voxels) were filtered out\n        - Removed the spine regions according to their coordinates on the raw segmentation\n        - Generate detection proposal:\n            - Binarized the post-processed segmentation results with a low threshold of 0.1\n            - Computed connected components on the binary segmentation\n            - Each connected component was regarded as a detection proposal\n            - A probability calculated by averaging raw segmentation scores over all voxels within the connected component.\n--- END OF TEXT": "| Dataset | Size | Data type | Demographic balance | Originating Institution | Annotation method | Evaluation method | Observer studies | Availability |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| RibFrac Dataset | 7,473 annotated traumatic rib fractures from 900 patients | CT images | Single center | Huadong Hospital affiliated to Fudan University | human-in-the-loop labeling procedure | Free-Response ROC (FROC) analysis, Intersection-over-Union (IoU), Dice Coefficient (Dice) | Independent human-only study, Human-collaboration study | A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research. |\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The data and code from Huadong Hospital affiliated to Fudan University used in this study are available at https://m3dv.github.io/Frac Net/ to users who agree with our data license (CC BY-NC 4.0) and code license (Apache-2.0 License).": "I'm sorry, there is no mention of any dataset or their characteristics in the given text. The text only mentions that data and code from Huadong Hospital affiliated to Fudan University is available at a certain website with specific data and code licenses.", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: RibFrac Dataset\n- Characteristics: \n    - 7,473 annotated traumatic rib fractures from 900 patients in a single center\n    - Annotated with a human-in-the-loop labeling procedure\n    - 720, 60 and 120 patients were randomly split as training, tuning and test cohort, respectively\n- Evaluation method:\n    - Detection performance: Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives \n    - Segmentation performance: Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures\n- Observer studies:\n    - Independent human-only study\n    - Human-collaboration study\n- Availability: \n    - A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research.\n\n- Dataset: None mentioned\n- Characteristics: \n    - This is a retrospective study\n- Ethics committee approval:\n    - Approved by the ethics committee of Huadong Hospital affiliated to Fudan University (NO.2019K146)\n    - Waived the requirement for informed consent.\n\n- Dataset: chest-abdomen CT DICOM (Digital Imaging and Communications in Medicine) format images\n- Characteristics:\n    - Imported into the software for delineating\n    - Images with VOI (volume of interest) information were then extracted with NII or NIFTI (Neuroimaging Informatics Technology Initiative) format for next-step analysis.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 900 cases, 332,483 CT slices in total\n    - Randomly split into 3 cohorts:\n        - Training: 720 cases, to train the deep learning system\n        - Tuning: 60 cases, to tune hyper-parameters of the deep learning system\n        - Test: 120 cases, to evaluate the model and human performance.\n\n- Dataset: RibFrac Dataset\n- Characteristics:\n    - 600 cases (221,308 CT slices in total) open sourced\n    - Randomly split into 3 cohorts:\n        - Training: 420 cases\n        - Tuning: 60 cases\n        - Test: 120 cases\n    - First open research dataset in this application.\n- Algorithm approach:\n    - Data-driven approach: it relies on the human annotations of rib fractures and learns to directly predict the voxel-level segmentation of fractures\n    - Does not rely on the extraction of rib centerlines in typical rib analysis algorithms\n- Model pipeline:\n    - Three stages:\n        - Pre-processing\n        - Sliding-window prediction\n        - Post-processing\n- Additional information: \n    - A deep learning system with same architecture of FracNet could be developed and validated with an acceptable performance on the open-source subset of RibFrac Dataset.\n    - Please refer to Supplementary Materials for details.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Pre-processing:\n        - Extracted the bone areas through a series of morphological operations (e.g., thresholding and filtering)\n        - The original spacing was preserved since only thin-section CT scans were included in the dataset\n        - Intensity of input voxels was clipped to the bone window (level=450, width=1100) and normalized to [-1,1].\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Sliding-window prediction:\n        - Formulated the rib fracture detection as a 3D segmentation task\n        - A customized 3D UNet, named FracNet was developed to perform segmentation in a sliding-window fashion\n        - Whole-volume CT scan is too large to fit in a regular GPU memory\n        - Cropped 64x64x64 patches in a sliding-window fashion with a stride of 48 and feed them to the network\n        - A raw segmentation was obtained by assembling patches of prediction\n        - Maximum values were kept in the overlapping regions of multiple predictions.\n\n- Dataset: Not mentioned\n- Characteristics:\n    - Post-processing:\n        - Efficiently reduce the false positive in predictions\n        - Predictions of small sizes (smaller than 200 voxels) were filtered out\n        - Removed the spine regions according to their coordinates on the raw segmentation\n        - Generate detection proposal:\n            - Binarized the post-processed segmentation results with a low threshold of 0.1\n            - Computed connected components on the binary segmentation\n            - Each connected component was regarded as a detection proposal\n            - A probability calculated by averaging raw segmentation scores over all voxels within the connected component.\n\nI'm sorry, there is no mention of any dataset or their characteristics in the given text. The text only mentions that data and code from Huadong Hospital affiliated to Fudan University is available at a certain website with specific data and code licenses.--- END OF TEXT": "| Dataset | Size | Data Type | Demographic Balance | Originating Institution | Annotation Method | Evaluation Method | Observer Studies | Availability |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| RibFrac Dataset | 7,473 annotated traumatic rib fractures from 900 patients in a single center | CT DICOM | Single center | Huadong Hospital affiliated to Fudan University | Human-in-the-loop labeling procedure | Free-Response ROC (FROC) analysis for detection performance, Intersection-over-Union (IoU) and Dice Coefficient (Dice) for segmentation performance | Independent human-only study, Human-collaboration study | A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as code for model training and evaluation is open to the research community to facilitate both clinical and engineering research. |\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Image-based tracking of medical instruments is an integral part of surgical data science applications. Previous research has addressed the tasks of detecting, segmenting and tracking medical instruments based on laparoscopic video data. However, the proposed methods still tend to fail when applied to challenging images and do not generalize well to data they have not been trained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set-the first publicly available data set enabling comprehensive benchmarking of medical instrument detection and segmentation algorithms with a specific emphasis on method robustness and generalization capabilities. Our data set comprises 30 laparoscopic videos and corresponding sensor data from medical devices in the operating room for three different types of laparoscopic surgery. Annotations include surgical phase labels for all video frames as well as information on instrument presence and corresponding instance-wise segmentation masks for surgical instruments (if any) in more than 10,000 individual frames. The data has successfully been used to organize international competitions within the Endoscopic Vision Challenges 2017 and 2019. Background & Summary Surgical data science was recently defined as an interdisciplinary research field which aims \"to improve the quality of interventional healthcare and its value through the capture, organization, analysis and modelling of data\" 1. The vision is to derive data science-based methodology to provide physicians with the right assistance at the right time. One active field of research consists in analyzing laparoscopic video data to provide context-aware Division of computer Assisted Medical interventions (cAMi), German cancer Research center (DKfZ), im neuenheimer Feld 223, 69120, Heidelberg, Germany.": "[[\"Heidelberg Colorectal (HeiCo)\", 30, \"laparoscopic videos\", \"sensor data\", \"3 types of laparoscopic surgery\", \"surgical phase labels\", \"instrument presence\", \"instance-wise segmentation masks\", \"more than 10,000 individual frames\", \"Endoscopic Vision Challenges 2017 and 2019\"]]", "Extract the datasets and their characteristics in a nested list manner from the following text: Typically, a challenge has a training phase. At the beginning of the training phase, the challenge organizers release training cases with the relevant reference annotations 6 . These help the participating teams in developing their method (e.g. by training a machine learning algorithm). In the test phase, the participating teams either upload their algorithm, or they receive access to the test cases without the reference annotations and submit the results that their algorithm has achieved for the test cases to the challenge organizers. To enable comparative benchmarking to be executed for this challenge paradigm, our data set was split into a training set and a test set. Our data set was arranged such that it allows for validation of detection/ binary segmentation/multi-instance segmentation algorithms in three test stages:": "[[\"training phase\", \"training cases\", \"reference annotations\"], [\"test phase\", \"test cases\", \"without reference annotations\"], [\"data set\", \"split into training and test set\", \"validation of detection/binary segmentation/multi-instance segmentation algorithms\", \"3 test stages\"]]", "Extract the datasets and their characteristics in a nested list manner from the following text: \u2022 Stage 1: The test data are taken from the procedures (patients) from which the training data were extracted.": "[[\"Stage 1\", \"test data\", \"taken from procedures (patients)\", \"from which the training data were extracted\"]]\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The primary data set of this paper corresponds to the ROBUST-MIS 2019 challenge and features 30 surgical videos along with frame-based instrument annotations for more than 10,000 frames. The annotations for this data set underwent a rigorous multi-stage quality control process. This data set is complemented by surgical phase annotations for the 30 videos which were used in the Surgical Workflow Analysis in the sensorOR challenge organized in 2017.": "[[\"ROBUST-MIS 2019 challenge\", \"30 surgical videos\", \"frame-based instrument annotations\", \"more than 10,000 frames\", \"annotations underwent a rigorous multi-stage quality control process\"], [\"Surgical Workflow Analysis in the sensorOR challenge\", \"30 videos\", \"organized in 2017\"]]\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The data can be accessed in two primary ways: (1) As a complete data set that contains videos and medical device data along with corresponding annotations (surgical workflow and instrument segmentations) following the folder structure shown in Fig. 4 or (2) as ROBUST-MIS challenge data sets that represent the split of the data into training and test sets as used in the ROBUST-MIS challenge 2019 (Fig. 5) .": "[[\"complete data set\", \"videos\", \"medical device data\", \"corresponding annotations (surgical workflow and instrument segmentations)\", \"folder structure shown in Fig. 4\"], [\"ROBUST-MIS challenge data sets\", \"split of the data into training and test sets\", \"used in the ROBUST-MIS challenge 2019\", \"Fig. 5\"]]\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Complete data set. To access the complete data sets (without a split in training and test data), users are www.nature.com/scientificdata www.nature.com/scientificdata/ rOBUST-MIS challenge data set. The segmentation data is additionally provided in the way it was avail-": "[[\"Complete data set\", \"without split in training and test data\", \"accessed through a website\"], [\"ROBUST-MIS challenge data set\", \"segmentation data\", \"provided in the way it was available\"]]\n", "Extract the datasets and their characteristics in a nested list manner from the following text: A limitation of our data set could be seen in the fact that the phase annotations were performed by only a single expert surgeon. It should be noted, however, that the phase annotations merely served as context information while the segmentations, which were generated with a highly quality-controlled process, are in the focus of this work. Furthermore, we acquired data from only one hospital. This implies limited variability with respect to the acquisition conditions as only one specific endoscope and light source were used. Still, to our knowledge, the HeiCo data set is the only publicly available data set (1) based on multiple different surgeries and (2) comprising not only annotated video data but also sensor data from medical devices in the operating room.": "[[\"Heidelberg Colorectal (HeiCo)\", \"30 laparoscopic videos\", \"sensor data\", \"3 types of laparoscopic surgery\", \"surgical phase labels\", \"instrument presence\", \"instance-wise segmentation masks\", \"more than 10,000 individual frames\", \"Endoscopic Vision Challenges 2017 and 2019\"], [\"Limitation\", \"phase annotations performed by only a single expert surgeon\", \"context information\", \"segmentations generated with a highly quality-controlled process\", \"focus of this work\", \"data acquired from only one hospital\", \"limited variability with respect to the acquisition conditions\", \"only one specific endoscope and light source were used\", \"only publicly available data set based on multiple different surgeries and comprising not only annotated video data but also sensor data from medical devices in the operating room\"]]\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n[[\"Heidelberg Colorectal (HeiCo)\", 30, \"laparoscopic videos\", \"sensor data\", \"3 types of laparoscopic surgery\", \"surgical phase labels\", \"instrument presence\", \"instance-wise segmentation masks\", \"more than 10,000 individual frames\", \"Endoscopic Vision Challenges 2017 and 2019\"]]\n[[\"training phase\", \"training cases\", \"reference annotations\"], [\"test phase\", \"test cases\", \"without reference annotations\"], [\"data set\", \"split into training and test set\", \"validation of detection/binary segmentation/multi-instance segmentation algorithms\", \"3 test stages\"]]\n[[\"Stage 1\", \"test data\", \"taken from procedures (patients)\", \"from which the training data were extracted\"]]\n\n[[\"ROBUST-MIS 2019 challenge\", \"30 surgical videos\", \"frame-based instrument annotations\", \"more than 10,000 frames\", \"annotations underwent a rigorous multi-stage quality control process\"], [\"Surgical Workflow Analysis in the sensorOR challenge\", \"30 videos\", \"organized in 2017\"]]\n\n[[\"complete data set\", \"videos\", \"medical device data\", \"corresponding annotations (surgical workflow and instrument segmentations)\", \"folder structure shown in Fig. 4\"], [\"ROBUST-MIS challenge data sets\", \"split of the data into training and test sets\", \"used in the ROBUST-MIS challenge 2019\", \"Fig. 5\"]]\n\n[[\"Complete data set\", \"without split in training and test data\", \"accessed through a website\"], [\"ROBUST-MIS challenge data set\", \"segmentation data\", \"provided in the way it was available\"]]\n\n[[\"Heidelberg Colorectal (HeiCo)\", \"30 laparoscopic videos\", \"sensor data\", \"3 types of laparoscopic surgery\", \"surgical phase labels\", \"instrument presence\", \"instance-wise segmentation masks\", \"more than 10,000 individual frames\", \"Endoscopic Vision Challenges 2017 and 2019\"], [\"Limitation\", \"phase annotations performed by only a single expert surgeon\", \"context information\", \"segmentations generated with a highly quality-controlled process\", \"focus of this work\", \"data acquired from only one hospital\", \"limited variability with respect to the acquisition conditions\", \"only one specific endoscope and light source were used\", \"only publicly available data set based on multiple different surgeries and comprising not only annotated video data but also sensor data from medical devices in the operating room\"]]\n--- END OF TEXT": "|Dataset Name|Size|Data Type|Demographic Balance|Originating Institution|\n|------------|----|---------|-------------------|----------------------|\n|Heidelberg Colorectal (HeiCo)|30|laparoscopic videos|N/A|Endoscopic Vision Challenges 2017 and 2019|\n|ROBUST-MIS 2019 challenge|30|surgical videos|N/A|ROBUST-MIS 2019 challenge|\n|Complete data set|N/A|videos, medical device data|N/A|accessed through a website|\n|ROBUST-MIS challenge data set|N/A|segmentation data|N/A|provided in the way it was available|\n\nIt should be noted that the table above is a summary of the information given in the text, and that it might not be complete or entirely accurate. Also, I was not able to find information regarding demographic balance, originating institution and some other characteristics.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: International challenges have become the de facto standard for comparative assessment of image analysis algorithms. Although segmentation is the most widely investigated medical image processing task, the various challenges have been organized to focus only on specific clinical tasks. We organized the Medical Segmentation Decathlon (MSD)-a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities to investigate the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a customdesigned solution. MSD results confirmed this hypothesis, moreover, MSD winner continued generalizing well to a wide range of other clinical problems for the next two years. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to scientists that are not versed in AI model training.": "It is not clear from the text provided what specific datasets and their characteristics are being referred to in the \"Medical Segmentation Decathlon (MSD)\" challenge. The text states that the MSD is a biomedical image analysis challenge where algorithms compete in a \"multitude of both tasks and modalities\" and that the results confirmed the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task. However, no specific details about the datasets used in the MSD challenge are provided in the text.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Challenge data set. The MSD data set is publicly available under a Creative Commons license CC-BY-SA4.0, allowing broad (including commercial) use. The training data used in this study is available at http://medicaldecathlon.com/. The test data of the challenge cannot be released since the live challenge is still open and users are able to submit their results anytime; we currently have no intentions of closing the challenge.Challenge assessment data. The raw challenge assessment data used to calculate the challenge rankings cannot be made publicly available due to privacy reasons. It contains the DSC and NSD values for every participating team for every task and target region. However, the aggregated results can be found in Table 1 and Supplementary Tables 2-11 . Furthermore, they can be found here: https://phabricator.mitk.org/source/msd_ evaluation/ in the folders descriptive-statistics, mean-values-persubtask and rankings-per-subtask.": "The MSD data set is publicly available under a Creative Commons license CC-BY-SA4.0, allowing broad (including commercial) use. The training data used in this study is available at http://medicaldecathlon.com/. The test data of the challenge cannot be released since the live challenge is still open and users are unable to submit their results. The raw challenge assessment data used to calculate the challenge rankings cannot be made publicly available due to privacy reasons. It contains the DSC and NSD values for every participating team for every task and target region. The aggregated results can be found in Table 1 and Supplementary Tables 2-11 as well as in the folders \"descriptive-statistics\", \"mean-values-persubtask\" and \"rankings-per-subtask\" at this link https://phabricator.mitk.org/source/msd_evaluation/\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\nIt is not clear from the text provided what specific datasets and their characteristics are being referred to in the \"Medical Segmentation Decathlon (MSD)\" challenge. The text states that the MSD is a biomedical image analysis challenge where algorithms compete in a \"multitude of both tasks and modalities\" and that the results confirmed the hypothesis that a method capable of performing well on multiple tasks will generalize well to a previously unseen task. However, no specific details about the datasets used in the MSD challenge are provided in the text.\n\nThe MSD data set is publicly available under a Creative Commons license CC-BY-SA4.0, allowing broad (including commercial) use. The training data used in this study is available at http://medicaldecathlon.com/. The test data of the challenge cannot be released since the live challenge is still open and users are unable to submit their results. The raw challenge assessment data used to calculate the challenge rankings cannot be made publicly available due to privacy reasons. It contains the DSC and NSD values for every participating team for every task and target region. The aggregated results can be found in Table 1 and Supplementary Tables 2-11 as well as in the folders \"descriptive-statistics\", \"mean-values-persubtask\" and \"rankings-per-subtask\" at this link https://phabricator.mitk.org/source/msd_evaluation/\n--- END OF TEXT": "I apologize, but it appears that there is not enough information in the provided text to create a markdown table of dataset characteristics for the \"Medical Segmentation Decathlon (MSD)\" challenge. The text mentions that the MSD data set is publicly available and that the training data is available at a certain website, but it does not provide any specific details about the characteristics of the data such as size, data type, demographic balance, originating institution, and others. Without this information, it is not possible to create a markdown table of dataset characteristics for the MSD challenge.\n", "Extract the datasets and their characteristics in a nested list manner from the following text: In this paper we report the setup and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low-and high-grade glioma patients-manually annotated by up to four raters-and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.": "The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) is a biomedical image analysis challenge organized in conjunction with the MICCAI 2012 and conferences. The dataset consists of a set of 65 multi-contrast MR scans of low-and high-grade glioma patients, which were manually annotated by up to four raters. It also includes 65 comparable scans generated using tumor image simulation software. The quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. The BRATS image data and manual annotations are publicly available through an online evaluation system as an ongoing benchmarking resource.\n\n-Dataset Name: Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\n  -Size: 130 scans (65 multi-contrast MR scans, 65 comparable scans generated using tumor image simulation software)\n  -Data type: multi-contrast MR scans of low-and high-grade glioma patients\n  -Demographic balance: not specified\n  -Originating institution: organized in conjunction with the MICCAI 2012 and conferences\n  -Annotation: manual annotation by up to four raters\n  -Availability: publicly available through an online evaluation system as an ongoing benchmarking resource.\n  -Evaluation: quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%)\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Clinical Image Data: The clinical image data consists of 65 multi-contrast MR scans from glioma patients, out of which 14 have been acquired from low-grade (histological diagnosis: astrocytomas or oligoastrocytomas) and 51 from high-grade (anaplastic astrocytomas and glioblastoma multiforme tumors) glioma patients. The images represent a mix of pre-and post-therapy brain scans, with two volumes showing resections. They were acquired at four different centers-Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital-over the course of several years, using MR scanners from different vendors and with different field strengths (1.5T and 3T) and implementations of the imaging sequences (e.g., 2D or 3D). The image datasets used in the study all share the following four MRI contrasts (Fig. 2) . 1) T1: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1-6 mm slice thickness. 2) T1c: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients. 3) T2: T2-weighted image, axial 2D acquisition, with 2-6 mm slice thickness. 4) FLAIR: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2-6 mm slice thickness. To homogenize these data we co-registered each subject's image volumes rigidly to the T1c MRI, which had the highest spatial resolution in most cases, and resampled all images to 1 mm isotropic resolution in a standardized axial orientation with a linear interpolator. We used a rigid registration model with the mutual information similarity metric as it is implemented in ITK [74] (\"VersorRigid3DTransform\" with \"MattesMutualInformation\" similarity metric and three multi-resolution levels). No attempt was made to put the individual patients in a common reference space. All images were skull stripped [75] to guarantee anomymization of the patients.": "The clinical image data for this study consists of 65 multi-contrast MR scans from glioma patients, 14 of which were acquired from low-grade (histological diagnosis: astrocytomas or oligoastrocytomas) and 51 from high-grade (anaplastic astrocytomas and glioblastoma multiforme tumors) glioma patients. The images represent a mix of pre-and post-therapy brain scans, with two volumes showing resections. They were acquired at four different centers (Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital) over the course of several years, using MR scanners from different vendors and with different field strengths (1.5T and 3T) and implementations of the imaging sequences (e.g., 2D or 3D). \n\n-Dataset Name: Clinical Image Data \n  -Size: 65 multi-contrast MR scans\n  -Data type: multi-contrast MR scans of glioma patients (14 low-grade, 51 high-grade)\n  -Demographic balance: 14 low-grade and 51 high-grade glioma patients\n  -Originating institution: Acquired at four different centers (Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital)\n  -Annotation: Not specified\n  -Availability: Not specified\n  -Preprocessing: co-registered each subject's image volumes rigidly to the T1c MRI, resampled all images to 1 mm isotropic resolution in a standardized axial orientation with a linear interpolator. skull stripped to guarantee anomymization of the patients.\n  -Image contrasts: T1, T1c, T2, FLAIR\n", "Extract the datasets and their characteristics in a nested list manner from the following text: Synthetic Image Data: The synthetic data of the BRATS 2012 challenge consisted of simulated images for 35 high-grade and low-grade gliomas that exhibit comparable tissue contrast properties and segmentation challenges as the clinical dataset (Fig. 2, last row) . The same image modalities as for the real data were simulated, with similar 1 mm resolution. The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema [76] . It models infiltrating edema adjacent to tumors, local distortion of healthy tissue, and central contrast enhancement using the tumor growth model of Clatz et al. [77] , combined with a routine for synthesizing texture similar to that of real MR images. We parameterized the algorithm according to the parameters proposed in [76] , and applied it to anatomical maps of healthy subjects from the BrainWeb simulator [78] , [79] . We synthesized image volumes and degraded them with different noise levels and intensity inhomogeneities, using Gaussian noise and polynomial bias fields with random coefficients.": "The synthetic data of the BRATS 2012 challenge consisted of simulated images for 35 high-grade and low-grade gliomas that exhibit comparable tissue contrast properties and segmentation challenges as the clinical dataset. The same image modalities as for the real data were simulated, with similar 1 mm resolution. The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema.\n\n-Dataset Name: Synthetic Image Data\n  -Size: 35 high-grade and low-grade gliomas\n  -Data type: simulated images \n  -Demographic balance: 35 high-grade and low-grade gliomas\n  -Originating institution: Not specified\n  -Annotation: Not specified\n  -Availability: Not specified\n  -Preprocessing: The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema.\n  -Image contrasts: T1, T1c, T2, FLAIR\n  -Image resolution: 1 mm\n  -Simulation tool: TumorSim\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\nThe Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) is a biomedical image analysis challenge organized in conjunction with the MICCAI 2012 and conferences. The dataset consists of a set of 65 multi-contrast MR scans of low-and high-grade glioma patients, which were manually annotated by up to four raters. It also includes 65 comparable scans generated using tumor image simulation software. The quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. The BRATS image data and manual annotations are publicly available through an online evaluation system as an ongoing benchmarking resource.\n\n-Dataset Name: Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\n  -Size: 130 scans (65 multi-contrast MR scans, 65 comparable scans generated using tumor image simulation software)\n  -Data type: multi-contrast MR scans of low-and high-grade glioma patients\n  -Demographic balance: not specified\n  -Originating institution: organized in conjunction with the MICCAI 2012 and conferences\n  -Annotation: manual annotation by up to four raters\n  -Availability: publicly available through an online evaluation system as an ongoing benchmarking resource.\n  -Evaluation: quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%)\n\nThe clinical image data for this study consists of 65 multi-contrast MR scans from glioma patients, 14 of which were acquired from low-grade (histological diagnosis: astrocytomas or oligoastrocytomas) and 51 from high-grade (anaplastic astrocytomas and glioblastoma multiforme tumors) glioma patients. The images represent a mix of pre-and post-therapy brain scans, with two volumes showing resections. They were acquired at four different centers (Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital) over the course of several years, using MR scanners from different vendors and with different field strengths (1.5T and 3T) and implementations of the imaging sequences (e.g., 2D or 3D). \n\n-Dataset Name: Clinical Image Data \n  -Size: 65 multi-contrast MR scans\n  -Data type: multi-contrast MR scans of glioma patients (14 low-grade, 51 high-grade)\n  -Demographic balance: 14 low-grade and 51 high-grade glioma patients\n  -Originating institution: Acquired at four different centers (Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital)\n  -Annotation: Not specified\n  -Availability: Not specified\n  -Preprocessing: co-registered each subject's image volumes rigidly to the T1c MRI, resampled all images to 1 mm isotropic resolution in a standardized axial orientation with a linear interpolator. skull stripped to guarantee anomymization of the patients.\n  -Image contrasts: T1, T1c, T2, FLAIR\n\nThe synthetic data of the BRATS 2012 challenge consisted of simulated images for 35 high-grade and low-grade gliomas that exhibit comparable tissue contrast properties and segmentation challenges as the clinical dataset. The same image modalities as for the real data were simulated, with similar 1 mm resolution. The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema.\n\n-Dataset Name: Synthetic Image Data\n  -Size: 35 high-grade and low-grade gliomas\n  -Data type: simulated images \n  -Demographic balance: 35 high-grade and low-grade gliomas\n  -Originating institution: Not specified\n  -Annotation: Not specified\n  -Availability: Not specified\n  -Preprocessing: The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema.\n  -Image contrasts: T1, T1c, T2, FLAIR\n  -Image resolution: 1 mm\n  -Simulation tool: TumorSim\n--- END OF TEXT": "| Dataset Name | Size | Data Type | Demographic Balance | Originating Institution | Annotation | Availability | Preprocessing | Image Contrasts | Image Resolution | Simulation Tool |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) | 130 scans (65 multi-contrast MR scans, 65 comparable scans generated using tumor image simulation software) | multi-contrast MR scans of low-and high-grade glioma patients | not specified | organized in conjunction with the MICCAI 2012 and conferences | manual annotation by up to four raters | publicly available through an online evaluation system as an ongoing benchmarking resource | N/A | N/A | N/A | N/A |\n| Clinical Image Data | 65 multi-contrast MR scans | multi-contrast MR scans of glioma patients (14 low-grade, 51 high-grade) | 14 low-grade and 51 high-grade glioma patients | Acquired at four different centers (Bern University, Debrecen University, Heidelberg University, and Massachusetts General Hospital) | Not specified | Not specified | co-registered each subject's image volumes rigidly to the T1c MRI, resampled all images to 1 mm isotropic resolution in a standardized axial orientation with a linear interpolator. skull stripped to guarantee anomymization of the patients. | T1, T1c, T2, FLAIR | N/A | N/A |\n| Synthetic Image Data | 35 high-grade and low-grade gliomas | simulated images | 35 high-grade and low-grade gliomas | Not specified | Not specified | Not specified | The images were generated using the TumorSim software, a cross-platform simulation tool that combines physical and statistical models to generate synthetic ground truth and synthesized MR images with tumor and edema. | T1, T1c, T2, FLAIR | 1 mm | TumorSim |\n", "Extract the datasets and their characteristics in a nested list manner from the following text: The emergence of deep learning has considerably advanced the state-of-the-art in cardiac magnetic resonance (CMR) segmentation. Many techniques have been proposed over the last few years, bringing the accuracy of automated segmentation close to human performance. However, these models have been all too often trained and validated using cardiac imaging samples from single clinical centres or homogeneous imaging protocols. This has prevented the development and validation of models that are generalizable across different clinical centres, imaging conditions or scanner vendors. To promote further research and scientific benchmarking in the field of generalizable deep learning for cardiac segmentation, this paper presents the results of the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge, which was recently organized as part of the MICCAI 2020 Conference. A total of 14 teams submitted different solutions to the problem, combining various baseline models, data augmentation strategies, and domain adaptation techniques. The obtained results indicate the importance of intensity-driven data augmentation, as well as the need for further research to improve generalizability towards unseen scanner vendors or new imaging protocols. Furthermore, we present a new resource of 375 heterogeneous CMR datasets acquired by using four different scanner vendors in six hospitals and three different countries (Spain, Canada and Germany), which we provide as open-access for the community to enable future research in the field.": "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together.\n\n--- START OF TEXT\nThe emergence of deep learning has considerably advanced the state-of-the-art in cardiac magnetic resonance (CMR) segmentation. Many techniques have been proposed over the last few years, bringing the accuracy of automated segmentation close to human performance. However, these models have been all too often trained and validated using cardiac imaging samples from single clinical centres or homogeneous imaging protocols. This has prevented the development and validation of models that are generalizable across different clinical centres, imaging conditions or scanner vendors. To promote further research and scientific benchmarking in the field of generalizable deep learning for cardiac segmentation, this paper presents the results of the Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge, which was recently organized as part of the MICCAI 2020 Conference. A total of 14 teams submitted different solutions to the problem, combining various baseline models, data augmentation strategies, and domain adaptation techniques. The obtained results indicate the importance of intensity-driven data augmentation, as well as the need for further research to improve generalizability towards unseen scanner vendors or new imaging protocols. Furthermore, we present a new resource of 375 heterogeneous CMR datasets acquired by using four different scanner vendors in six hospitals and three different countries (Spain, Canada and Germany), which we provide as open-access for the community to enable future research in the field.\n--- END OF TEXT\n\n| Dataset Name | Size | Data Type | Demographic Balance | Originating Institution | Scanner Vendor | Availability |\n| --- | --- | --- | --- | --- | --- |", "Extract the datasets and their characteristics in a nested list manner from the following text: A total of six clinical centres from Spain, Canada and Germany (numbered to 6 in this work) contributed to this III  AVERAGE SPECIFICATIONS FOR THE IMAGES ACQUIRED IN THE DIFFERENT CENTRES challenge by providing a different number of CMR studies from different scanner vendors, as detailed in Table I . In total, 375 studies were included in this challenge. The subjects considered for this multi-disease study were selected among groups of various cardiovascular diseases, such as hypertrophic cardiomyopathy, dilated cardiomyopathy, coronary heart disease, abnormal right ventricle, myocarditis and ischemic cardiomyopathy as well as healthy volunteers (see Table II for more details on the distribution of these cases). The specific scanner manufacturers are: 1) Siemens (Siemens Healthineers, Germany), 2) Philips (Philips Healthcare, Netherlands), 3) General Electric (GE, GE Healthcare, USA) and 4) Canon (Canon Inc., Japan). These four manufacturers were coded as A, B, C and D during the challenge, respectively. The CMR images derived from these four vendors are illustrated in Fig. 1 . More specific details on the studies are given in Table III .": "-Dataset Name: Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation (M&Ms) Challenge\n  -Size: 375 CMR studies\n  -Data type: CMR images of various cardiovascular diseases and healthy volunteers\n  -Demographic balance: distribution of various cardiovascular diseases and healthy volunteers as detailed in Table II\n  -Originating institution: Six clinical centres from Spain, Canada and Germany\n  -Annotation: not specified\n  -Availability: new resource provided as open-access for the community\n  -Scanner Vendor: Siemens (Siemens Healthineers, Germany), Philips (Philips Healthcare, Netherlands), General Electric (GE, GE Healthcare, USA) and Canon (Canon Inc., Japan)\n  -Imaging protocols: not specified\n  -Distribution of cases: see Table III for more details on the distribution of cases\n", "Based on the following text, denoted by \"START OF TEXT\" and \"END OF TEXT\", merge the mentioned dataset data characteristics into a markdown table. Please include at least \"size\", \"data type\", \"demographic balance\", \"originating institution\" and others that you might find relevant, just have in mind that it has to be related to the data itself. If there are entries with the same Dataset Name, merge them together. \n--- START OF TEXT\n- Dataset: Not specified \n- Characteristics:\n    - Goal is to advance the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding.\n    - Images of complex everyday scenes containing common objects in their natural context\n    - Objects are labeled using per-instance segmentations to aid in precise object localization\n    - Contains photos of 91 objects types that would be easily recognizable by a 4 year old\n    - 2.5 million labeled instances in 328k images\n    - Involved extensive crowd worker involvement via novel user interfaces for category detection, instance spotting, and instance segmentation\n    - Detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN \n    - Baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.\n\n- Dataset: Microsoft Common Objects in COntext (MS COCO)\n- Characteristics:\n    - Compared to several other popular datasets: ImageNet, PASCAL VOC 2012, and SUN\n    - Each of these datasets varies significantly in size, list of labeled categories and types of images\n    - ImageNet was created to capture a large number of object categories, many of which are fine-grained\n    - SUN focuses on labeling scene types and the objects that commonly occur in them\n    - PASCAL VOC's primary application is object detection in natural images\n    - MS COCO is designed for the detection and segmentation of objects occurring in their natural context.\n\n- Dataset: MS COCO\n- Characteristics:\n    - Number of instances per category for all 91 categories is shown in Fig. 5(a)\n    - Summary of the datasets showing the number of object categories and the number of instances per category is shown in Fig. 5(d)\n    - MS COCO has fewer categories than ImageNet and SUN, but has more instances per category which can be useful for learning complex models capable of precise localization.\n    - In comparison to PASCAL VOC, MS COCO has both more categories and instances.\n\n- Dataset: MS COCO\n- Characteristics:\n    - Strive to find non-iconic images containing objects in their natural context\n    - The amount of contextual information present in an image can be estimated by examining the average number of object categories and instances per image.\n    - For ImageNet we plot the object detection validation set, since the training data only has a single object labeled.\n    - On average MS COCO dataset contains 3.5 categories and 7.7 instances per image\n    - In comparison, ImageNet and PASCAL VOC both have less than 2 categories and 3 instances per image on average\n    - Only 10% of the images in MS COCO have only one category per image, in comparison, over 60% of images contain a single object category in ImageNet and PASCAL VOC.\n    - SUN dataset has the most contextual information since it is scene-based and uses an unrestricted set of categories.\n\n- Dataset: MS COCO and SUN\n- Characteristics:\n    - Analyze the average size of objects in the datasets\n    - Generally smaller objects are harder to recognize and require more contextual reasoning to recognize\n    - As shown in Fig. 5(e), the average sizes of objects is smaller for both MS COCO and SUN\n\n- Dataset: MS COCO\n- Characteristics:\n    - Split into two roughly equal parts:\n        - First half of the dataset was released in 2014\n        - Second half will be released in 2015\n    - 2014 release contains:\n        - 82,783 training images\n        - 40,504 validation images\n        - 40,775 testing images (approximately 1 train, 1 val, and 1 test)\n        - Nearly 270k segmented people \n        - Total of 886k segmented object instances in the 2014 train+val data alone\n    - 2015 release will contain:\n        - 165,482 training images\n        - 81,208 validation images\n        - 81,434 testing images\n    - Care was taken to minimize the chance of near-duplicate images existing across splits by explicitly removing near duplicates (detected with [43]) and grouping images by photographer and date taken.\n\n- Dataset: Not specified\n- Characteristics:\n    - Following established protocol, annotations for train and validation data will be released, but not for test\n    - Evaluation server for automatic evaluation on the test set is being finalized\n    - A full discussion of evaluation metrics will be added once the evaluation server is complete.\n\n- Dataset: MS COCO\n- Characteristics:\n    - 2014 release is limited to a subset of 80 categories\n    - The following 11 categories are not included: hat, shoe, eyeglasses, mirror, window, door, street sign, plate, desk, blender, hair brush \n        - Not collected segmentations for these categories because they are too many instances, ambiguous and difficult to label, due to confusion with other categories, or too few instances\n    - May add segmentations for some of these categories in the cumulative 2015 release.\n--- END OF TEXT": "| Dataset | Size | Data Type | Demographic Balance | Originating Institution |\n| --- | --- | --- | --- | --- |\n| Not specified | 2.5 million labeled instances in 328k images | Images of complex everyday scenes containing common objects in their natural context | N/A | N/A |\n| MS COCO | 82,783 training images, 40,504 validation images, 40,775 testing images | Images of complex everyday scenes containing common objects in their natural context | N/A | Microsoft |\n| MS COCO |  |  | Number of instances per category for all 91 categories, Summary of the datasets showing the number of object categories and the number of instances per category, Strive to find non-iconic images containing objects in their natural context, Average size of objects in the datasets |  |  |"}